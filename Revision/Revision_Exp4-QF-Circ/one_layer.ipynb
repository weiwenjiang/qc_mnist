{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Training procedure for Quantum Computer:\n",
      "\tStart at: 08/12/2020 02:25:39\n",
      "\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\n",
      "\tEnjoy and Good Luck!\n",
      "====================================================================================================\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from lib_model_summary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import functools\n",
    "\n",
    "# from lib_util import *\n",
    "\n",
    "from collections import Counter\n",
    "print = functools.partial(print, flush=True)\n",
    "\n",
    "# For 4*4, 16->4->2: batch_size=32; init_lr=0.01; with_norm=True or False\n",
    "# For 4*4, 16->4->1: batch_size=16; init_lr=0.1; with_norm=True, ang:20; or train\n",
    "\n",
    "# interest_num = [0,1,2,3,4,5,6,7,8,9]\n",
    "# how many samples per batch to load\n",
    "batch_size = 32\n",
    "inference_batch_size = 32\n",
    "num_f1 = 1\n",
    "# num_f2 = len(interest_num)\n",
    "num_f2 = 2\n",
    "num_f3 = 2\n",
    "init_lr = 0.1\n",
    "init_qc_lr = 1\n",
    "with_norm = True\n",
    "\n",
    "# Given_ang to -1 to train the variable \n",
    "given_ang = -1\n",
    "milestones = [6, 10, 14]\n",
    "\n",
    "save_to_file = False\n",
    "if save_to_file:\n",
    "    sys.stdout = open(save_path+\"/log\", 'w')\n",
    "\n",
    "# resume_path = \"././model/ipykernel_launcher.py_2020_05_04-10_08_35/checkpoint_5_0.9401.pth.tar\"\n",
    "resume_path = \"././model/ipykernel_launcher.py_2020_05_04-13_52_53/checkpoint_0_0.9714.pth.tar\"\n",
    "resume_path = \"././model/ipykernel_launcher.py_2020_05_04-14_22_13/checkpoint_0_0.9714.pth.tar\"\n",
    "resume_path = \"././model/ipykernel_launcher.py_2020_05_05-20_06_18/checkpoint_1_1.0.pth.tar\"\n",
    "# resume_path = \"\"\n",
    "save_chkp = 0\n",
    "training = 1\n",
    "print_detail = 0\n",
    "max_epoch = 3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "if save_chkp:\n",
    "    save_path = \"./model/\"+os.path.basename(sys.argv[0])+\"_\"+time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Training procedure for Quantum Computer:\")\n",
    "print(\"\\tStart at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "print(\"\\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\")\n",
    "print(\"\\tEnjoy and Good Luck!\")\n",
    "print(\"=\"*100)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input = []\n",
    "output = []\n",
    "i_dim = 20\n",
    "j_dim = 20\n",
    "ori_out = [1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "\n",
    "# \n",
    "# ori_out = [1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1]\n",
    "\n",
    "# ori_out = [1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "# ori_out = [1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1]\n",
    "\n",
    "\n",
    "# 1 layer 2 neural\n",
    "# ori_out = [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1]\n",
    "# ori_out = torch.tensor(list(torch.empty(20, 20).random_(2).flatten().type(torch.int).tolist()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "golden_data = {}\n",
    "for i in range(i_dim):\n",
    "    for j in range(j_dim):\n",
    "        # if i==0 and j==0:\n",
    "        #     input.append([0.1*i+0.0001,0.1*j+0.0001])\n",
    "        \n",
    "        input.append([(1.0/i_dim)*(i+1),(1.0/j_dim)*(j+1)])\n",
    "        # if i > j or i>j_dim-j:\n",
    "        # # if i > j:\n",
    "        #     output.append([0])\n",
    "        # else:\n",
    "        #     output.append([1])\n",
    "        output.append(ori_out[i*j_dim+j])\n",
    "        golden_data[tuple([(1.0/i_dim)*(i+1),(1.0/j_dim)*(j+1)])] = ori_out[i*j_dim+j]\n",
    "\n",
    "for i in range(i_dim):\n",
    "    for j in range(j_dim):\n",
    "        print(output[i*j_dim+j],end=\" \")\n",
    "    print()\n",
    "\n",
    "img = []\n",
    "\n",
    "for i in range(i_dim):\n",
    "    row = []    \n",
    "    for j in range(j_dim):\n",
    "        # print(input[i*j_https://docs.python.org/3/library/stdtypes.html#listdim+j],end=\" \")\n",
    "        row.append(output[i*j_dim+j])\n",
    "    img.append(row)\n",
    "    # print()\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    # img = img / 2 + 0.5     # unnormalize\n",
    "    img = np.array(img)\n",
    "    image = np.asarray(img * 255, np.uint8)    \n",
    "    im = Image.fromarray(image,mode=\"L\")    \n",
    "    im = im.resize((100,100),Image.NEAREST )    \n",
    "    # im.thumbnail((64, 64), Image.ANTIALIAS)  # \n",
    "    plt.imshow(im,cmap='Greys')\n",
    "    plt.show()\n",
    "    # im.save(\"../results/Figures/example.pdf\")\n",
    "\n",
    "\n",
    "\n",
    "imshow(img)\n",
    "# print labels\n",
    "# print(' '.join('%5s' % filtered_class[labels[j]] for j in range(batch_size)))\n",
    "\n",
    "\n",
    "\n",
    "# sys.exit(0)\n",
    "dataset_ori = []\n",
    "for i in range(i_dim):\n",
    "    for j in range(j_dim):\n",
    "        dataset_ori.append([torch.tensor(input[i*j_dim+j]),torch.tensor(output[i*j_dim+j])])\n",
    "        \n",
    "\n",
    "import random\n",
    "def batch_generator(dataset,batch_size):\n",
    "    batch_dataset = []\n",
    "    for i in range(int(len(dataset)/batch_size)):\n",
    "        batch = torch.zeros((batch_size,2))        \n",
    "        batch_target = torch.zeros((batch_size,1),dtype=torch.long)\n",
    "        for j in range(batch_size):\n",
    "            item = dataset[i*batch_size+j]            \n",
    "            batch[j] = item[0]\n",
    "            batch_target[j] = item[1]\n",
    "            \n",
    "        batch_dataset.append((batch,batch_target))\n",
    "        \n",
    "    return batch_dataset\n",
    "\n",
    "dataset = batch_generator(dataset_ori,batch_size)\n",
    "# dataset = torch.tensor(dataset)\n",
    "        \n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    filename = os.path.join(save_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        bestname = os.path.join(save_path, 'model_best.tar')\n",
    "        shutil.copyfile(filename, bestname)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "========== Model Info ==========\n",
      "Net(\n",
      "  (fc1): BinaryLinear(in_features=2, out_features=1, bias=False)\n",
      "  (fc2): BinaryLinear(in_features=1, out_features=2, bias=False)\n",
      "  (qc1): QC_Norm_try3()\n",
      "  (qc2): QC_Norm_try3()\n",
      "  (qc1a): QC_Norm_Correction_try2()\n",
      "  (qc2a): QC_Norm_Correction_try2()\n",
      ")\n",
      "==================== 0 epoch ====================\n",
      "Epoch Start at: 08/12/2020 02:25:41\n",
      "-------------------- learning rates --------------------\n",
      "0.1,1.0,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 08/12/2020 02:25:41\n",
      "Train Epoch: 0 [0/12 (0%)]\tLoss: nan\tAccuracy: 18/32 (56.25%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: nan\n",
      "Trainign End at: 08/12/2020 02:25:41\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 08/12/2020 02:25:41\n",
      "Test set: Average loss: nan, Accuracy: 141/384 (36.72%)\n",
      "Testing End at: 08/12/2020 02:25:41\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.3671875; Current accuracy 0.3671875. Checkpointing\n",
      "Epoch End at: 08/12/2020 02:25:41\n",
      "============================================================\n",
      "\n",
      "==================== 1 epoch ====================\n",
      "Epoch Start at: 08/12/2020 02:25:41\n",
      "-------------------- learning rates --------------------\n",
      "0.1,1.0,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 08/12/2020 02:25:41\n",
      "Train Epoch: 1 [0/12 (0%)]\tLoss: nan\tAccuracy: 18/32 (56.25%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: nan\n",
      "Trainign End at: 08/12/2020 02:25:41\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 08/12/2020 02:25:41\n",
      "Test set: Average loss: nan, Accuracy: 141/384 (36.72%)\n",
      "Testing End at: 08/12/2020 02:25:42\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.3671875; Current accuracy 0.3671875. Checkpointing\n",
      "Epoch End at: 08/12/2020 02:25:42\n",
      "============================================================\n",
      "\n",
      "==================== 2 epoch ====================\n",
      "Epoch Start at: 08/12/2020 02:25:42\n",
      "-------------------- learning rates --------------------\n",
      "0.1,1.0,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 08/12/2020 02:25:42\n",
      "Train Epoch: 2 [0/12 (0%)]\tLoss: nan\tAccuracy: 18/32 (56.25%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: nan\n",
      "Trainign End at: 08/12/2020 02:25:42\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 08/12/2020 02:25:42\n",
      "Test set: Average loss: nan, Accuracy: 141/384 (36.72%)\n",
      "Testing End at: 08/12/2020 02:25:42\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.3671875; Current accuracy 0.3671875. Checkpointing\n",
      "Epoch End at: 08/12/2020 02:25:42\n",
      "============================================================\n",
      "\n",
      "==================== 3 epoch ====================\n",
      "Epoch Start at: 08/12/2020 02:25:42\n",
      "-------------------- learning rates --------------------\n",
      "0.1,1.0,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 08/12/2020 02:25:42\n",
      "Train Epoch: 3 [0/12 (0%)]\tLoss: nan\tAccuracy: 18/32 (56.25%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: nan\n",
      "Trainign End at: 08/12/2020 02:25:42\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 08/12/2020 02:25:42\n",
      "Test set: Average loss: nan, Accuracy: 141/384 (36.72%)\n",
      "Testing End at: 08/12/2020 02:25:42\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.3671875; Current accuracy 0.3671875. Checkpointing\n",
      "Epoch End at: 08/12/2020 02:25:42\n",
      "============================================================\n",
      "\n",
      "====================================================================================================\n",
      "fc1.weight Parameter containing:\n",
      "tensor([[nan, nan]], requires_grad=True)\n",
      "fc2.weight Parameter containing:\n",
      "tensor([[-0.3500],\n",
      "        [ 0.4033]], requires_grad=True)\n",
      "qc1.x_running_rot Parameter containing:\n",
      "tensor([nan], requires_grad=True)\n",
      "qc1.ang_inc Parameter containing:\n",
      "tensor([nan], requires_grad=True)\n",
      "qc2.x_running_rot Parameter containing:\n",
      "tensor([0., 0.])\n",
      "qc2.ang_inc Parameter containing:\n",
      "tensor([1., 1.], requires_grad=True)\n",
      "qc1a.x_running_rot Parameter containing:\n",
      "tensor([8.3575e-12], requires_grad=True)\n",
      "qc1a.x_l_0_5 Parameter containing:\n",
      "tensor([1.])\n",
      "qc1a.x_g_0_5 Parameter containing:\n",
      "tensor([0.])\n",
      "qc2a.x_running_rot Parameter containing:\n",
      "tensor([nan, nan], requires_grad=True)\n",
      "qc2a.x_l_0_5 Parameter containing:\n",
      "tensor([0., 0.])\n",
      "qc2a.x_g_0_5 Parameter containing:\n",
      "tensor([0., 0.])\n",
      "====================================================================================================\n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ],
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "from torch.nn.parameter import Parameter\n",
    " \n",
    "\n",
    "class BinarizeF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(cxt, input):\n",
    "        output = input.new(input.size())\n",
    "        output[input >= 0] = 1\n",
    "        output[input < 0] = -1\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(cxt, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input\n",
    "# aliases\n",
    "binarize = BinarizeF.apply\n",
    "\n",
    "\n",
    "class ClipF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = input.clone().detach()\n",
    "        # output = input.new(input.size())\n",
    "        output[input >= 1] = 1\n",
    "        output[input <= 0] = 0\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input >= 1] = 0\n",
    "        grad_input[input <= 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# aliases\n",
    "clipfunc = ClipF.apply\n",
    "\n",
    "\n",
    "class BinaryLinear(nn.Linear):\n",
    "\n",
    "\n",
    "    def do_slp_via_th(self,input_ori,w_ori):\n",
    "        p = input_ori\n",
    "        d = 4*p*(1-p)\n",
    "        e = (2*p-1)\n",
    "        # e_sq = torch.tensor(1)\n",
    "        w = w_ori\n",
    "\n",
    "        sum_of_sq = (d+e.pow(2)).sum(-1)\n",
    "        sum_of_sq = sum_of_sq.unsqueeze(-1)        \n",
    "        sum_of_sq = sum_of_sq.expand(p.shape[0], w.shape[0])\n",
    "\n",
    "        diag_p = torch.diag_embed(e)        \n",
    "\n",
    "        p_w = torch.matmul(w,diag_p)\n",
    "\n",
    "        z_p_w = torch.zeros_like(p_w)        \n",
    "        shft_p_w = torch.cat((p_w, z_p_w), -1)\n",
    "\n",
    "        sum_of_cross = torch.zeros_like(p_w)\n",
    "        length = p.shape[1]    \n",
    "\n",
    "        for shft in range(1,length):    \n",
    "            sum_of_cross += shft_p_w[:,:,0:length]*shft_p_w[:,:,shft:length+shft]\n",
    "\n",
    "        sum_of_cross = sum_of_cross.sum(-1)\n",
    "\n",
    "        return (sum_of_sq+2*sum_of_cross)/(length**2) \n",
    "\n",
    "    def forward(self, input):        \n",
    "        binary_weight = binarize(self.weight)        \n",
    "        if self.bias is None:\n",
    "            return self.do_slp_via_th(input,binary_weight)\n",
    "\n",
    "        else:   \n",
    "\n",
    "            bias_one  = torch.ones(input.shape[0],1)            \n",
    "            new_input = torch.cat((input, bias_one), -1)            \n",
    "            bias = clipfunc(self.bias).unsqueeze(1)            \n",
    "            new_weight = binary_weight            \n",
    "            new_weight = torch.cat((new_weight,bias),-1)                        \n",
    "            return self.do_slp_via_th(new_input,new_weight)\n",
    "\n",
    "\n",
    "            torch.set_printoptions(edgeitems=64)\n",
    "            # binary_bias = binarize(self.bias)/float(len(input[0].flatten())+1)\n",
    "            binary_bias = binarize(self.bias)/float(len(input[0].flatten())+1)\n",
    "            res = F.linear(input, binary_weight/float(len(input[0].flatten())+1), binary_bias)\n",
    "            return res\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Glorot initialization\n",
    "        in_features, out_features = self.weight.size()\n",
    "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "        self.weight.lr_scale = 1. / stdv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm(nn.Module):\n",
    "    def __init__(self, num_features, init_ang_inc = 10, momentum=0.1):        \n",
    "        super(QC_Norm, self).__init__()\n",
    "        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)        \n",
    "        self.ang_inc = Parameter(torch.ones(1)*init_ang_inc)\n",
    "        \n",
    "        self.momentum = momentum\n",
    "                \n",
    "        self.printed = False\n",
    "        self.x_mean_ancle=0\n",
    "        self.x_mean_rote = 0\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                print(\"self.ang_inc\",self.ang_inc)\n",
    "                self.printed = True\n",
    "                    \n",
    "            x = x.transpose(0,1)\n",
    "  \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)\n",
    "            x_1 = (x_final.cos()+1)/2\n",
    "                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            # print(x_1)\n",
    "        else:\n",
    "            self.printed = False\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "            x_lack_sum = x_sum - x    \n",
    "            x_mean = x_lack_sum/x.shape[-1]\n",
    "            \n",
    "            # print(x)\n",
    "            # print(x_sum)\n",
    "            # print(x_lack_sum)\n",
    "            # print(x_mean)\n",
    "            # \n",
    "                    \n",
    "            x_mean_ancle = (x_mean*2-1).acos()  \n",
    "            \n",
    "            ang_inc = self.ang_inc.unsqueeze(-1).expand(x_mean_ancle.shape) \n",
    "            # ang_inc = np.pi/2/(x.max(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) -x.min(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) )\n",
    "            \n",
    "            if given_ang!=-1:\n",
    "                x_mean_rote = (np.pi/2 - x_mean_ancle)*given_ang\n",
    "            else:\n",
    "                x_mean_rote = (np.pi/2 - x_mean_ancle)*ang_inc\n",
    "            \n",
    "            x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot\n",
    "                                                \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+x_mean_rote  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "      \n",
    "        return x_1\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        self.ang_inc.data.zeros_()\n",
    "        \n",
    "def print_degree(x,name=\"x\"):\n",
    "    print(name,x/np.pi*180)\n",
    "    \n",
    "    \n",
    "class QC_Norm_Real(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Real, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.x_max = 0\n",
    "        self.x_min = 0\n",
    "        # print(\"Using Normal without real\")\n",
    "        \n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            # x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_final = ((x_ancle-self.x_min)/(self.x_max-self.x_min))*np.pi\n",
    "            \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            x = x.transpose(0,1)        \n",
    "            x_ancle = (x*2-1).acos()     \n",
    "            x_rectify_ancle = (x_ancle.max(-1)[0]-x_ancle.min(-1)[0]).unsqueeze(-1).expand(x.shape)                                                                         \n",
    "            x_final = ((x_ancle-x_ancle.min(-1)[0].unsqueeze(-1))/(x_rectify_ancle))*np.pi\n",
    "            \n",
    "            x_moving_rot = x_final - x_ancle\n",
    "            \n",
    "            x_moving_rot_mean = x_moving_rot.sum(-1)/x.shape[-1] \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot_mean      \n",
    "            \n",
    "            self.x_max = self.momentum * x_ancle.max(-1)[0].unsqueeze(-1) + \\\n",
    "                                    (1 - self.momentum) * self.x_max\n",
    "            self.x_min = self.momentum * x_ancle.min(-1)[0].unsqueeze(-1) + \\\n",
    "                                    (1 - self.momentum) * self.x_min\n",
    "            \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "            \n",
    "        return x_1\n",
    "\n",
    "\n",
    "class QC_Norm_Real_Correction(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Real_Correction, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:            \n",
    "            \n",
    "            x = x.transpose(0,1)                    \n",
    "            x_ancle = (x*2-1).acos()                        \n",
    "            x_moving_rot = -1*(x_ancle.min(-1)[0])\n",
    "            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot                                                    \n",
    "            x_final = x_ancle+x_moving_rot.unsqueeze(-1)                                    \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        return x_1\n",
    "\n",
    "class QC_Norm_Correction(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Correction, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)                \n",
    "            x_mean = x_sum/x.shape[-1]\n",
    "                                \n",
    "            x_mean_ancle = (x_mean*2-1).acos()    \n",
    "            x_mean_rote = (np.pi/2 - x_mean_ancle) \n",
    "            \n",
    "            x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])\n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot                                        \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+x_mean_rote  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "        \n",
    "        return x_1\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm_True(nn.Module):\n",
    "    def __init__(self,num_features):        \n",
    "        super(QC_Norm_True, self).__init__()\n",
    "        \n",
    "        x_r_init =  torch.zeros(num_features)\n",
    "        x_r_init += 0\n",
    "        \n",
    "        self.x_qft = Parameter(torch.zeros(num_features),requires_grad=True)\n",
    "        self.x_hzh = Parameter(torch.zeros(num_features),requires_grad=True)\n",
    "        \n",
    "        self.x_running_rot = Parameter(x_r_init,requires_grad=True)\n",
    "        \n",
    "        self.x_running_means = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.x_running_min = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.x_running_max = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        \n",
    "        self.printed = False\n",
    "        self.momentum = 0.1\n",
    "    def forward(self,x,training=True):\n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                # print(self.x_qft)\n",
    "                # print(self.x_hzh)\n",
    "                print(self.x_running_rot)\n",
    "\n",
    "                # print(((binarize(self.x_qft)+1)/2)*0.5)\n",
    "                # print(((binarize(self.x_qft)+1)/2)*0.5*x)\n",
    "                # print(((1-x)-x)*self.x_running_rot + x)\n",
    "                self.printed = True\n",
    "            x_min_rt = self.x_running_min.detach()\n",
    "            x_max_rt = self.x_running_max.detach()\n",
    "        else:\n",
    "            self.printed = False           \n",
    "            # x_running_rot = self.x_running_rot    \n",
    "            x = x.transpose(0,1)\n",
    "            # x_sum = x.sum(-1)\n",
    "            # x_mean = x_sum/x.shape[-1]\n",
    "            x_min = x.min(1)[0]\n",
    "            x_max = x.max(1)[0]            \n",
    "            x = x.transpose(0,1)\n",
    "            self.x_running_min[:] = self.momentum * self.x_running_min + \\\n",
    "                                (1 - self.momentum) * x_min \n",
    "            self.x_running_max[:] = self.momentum * self.x_running_max + \\\n",
    "                                (1 - self.momentum) * x_max\n",
    "            x_min_rt = x_min\n",
    "            x_max_rt = x_max\n",
    "        \n",
    "        x = (x_min_rt*(1-2*x)+x)\n",
    "            \n",
    "        # x = x+x-2*x*x\n",
    "        \n",
    "        # ry\n",
    "        # init_rot = (x<0.5).float()*np.pi        \n",
    "        # final_rot = init_rot + self.x_running_rot                 \n",
    "        # r = (2*x-1).abs()        \n",
    "        # x = (r*final_rot.cos()+1)/2 \n",
    "        \n",
    "        return x\n",
    "        # QFT\n",
    "        # \n",
    "        # qft_g_10 = (binarize(self.x_qft-10)+1)/2\n",
    "        # qft_g_5 = (binarize(self.x_qft-5)+1)/2 - qft_g_10\n",
    "        # qft_g_0 = (binarize(self.x_qft-0)+1)/2 - qft_g_5 - qft_g_10\n",
    "        # x = qft_g_10*0.038*x + (1-qft_g_10)*x\n",
    "        # x = qft_g_5*0.146*x + (1-qft_g_5)*x\n",
    "        # x = qft_g_0*0.5*x + (1-qft_g_0)*x\n",
    "        \n",
    "        # x = ((binarize(self.x_qft)+1)/2*0.5)*x\n",
    "        # x = (((binarize(self.x_qft-5)+1)/2 - (binarize(self.x_qft-10)+1)/2)*0.146)*x\n",
    "        # x = (((binarize(self.x_qft-0)+1)/2 - (binarize(self.x_qft-5)+1)/2 - (binarize(self.x_qft-10)+1)/2)*0.5)*x\n",
    "        \n",
    "        \n",
    "        # x = (x+(binarize(self.x_hzh)+1)/2*(1-2*x))\n",
    "        # \n",
    "        # x = (((binarize(self.x_qft)+1)/2)*0.5)*x + (((1-(binarize(self.x_qft)+1)/2))*1)*x \n",
    "        # x = (((binarize(self.x_qft-10)+1)/2)*0.1)*x + (((1-(binarize(self.x_qft-10)+1)/2))*1)*x\n",
    "        # \n",
    "        # # hzh <- y\n",
    "        # x = ((1-x)-x)*clipfunc(self.x_hzh) + x        \n",
    "        # \n",
    "        # \n",
    "        # # ry\n",
    "        # init_rot = (x<0.5).float()*np.pi        \n",
    "        # final_rot = init_rot + self.x_running_rot                 \n",
    "        # r = (2*x-1).abs()        \n",
    "        # x1 = (r*final_rot.cos()+1)/2   \n",
    "\n",
    "\n",
    "            \n",
    "        # if not training:\n",
    "        #     \n",
    "        #     # init_rot = (x<0.5).float()*np.pi        \n",
    "        #     # final_rot = init_rot + self.x_running_rot                 \n",
    "        #     # r = (2*x-1).abs()        \n",
    "        #     # x = (r*final_rot.cos()+1)/2   \n",
    "        #     # \n",
    "        #     # x = 1-(x+self.x_running_means-2*x*self.x_running_means)\n",
    "        #     # x = (1-x)/2\n",
    "        #     # x = self.x_running_means*(1-2*x)+x\n",
    "        #     # \n",
    "        #     # if not self.printed:\n",
    "        #     #     print(\"self.x_running_means\",self.x_running_means)\n",
    "        #     #     self.printed = True\n",
    "        #     #     # print(x)\n",
    "        #     return x\n",
    "        # \n",
    "        # else:\n",
    "        #     self.printed = False\n",
    "            \n",
    "            # # ry\n",
    "            # init_rot = (x<0.5).float()*np.pi        \n",
    "            # final_rot = init_rot + self.x_running_rot                 \n",
    "            # r = (2*x-1).abs()        \n",
    "            # x = (r*final_rot.cos()+1)/2   \n",
    "\n",
    "            \n",
    "            \n",
    "            # x = x.transpose(0,1)\n",
    "            # x_sum = x.sum(-1)\n",
    "            # \n",
    "            # # self.x_running_rot = self.x_running_rot.clamp(0,1) \n",
    "            # \n",
    "            # x_mean = x_sum/x.shape[-1]\n",
    "            # \n",
    "            #          \n",
    "            # #          *self.x_hzh+self.x_running_rot/10\n",
    "            # # x_mean = x_mean.clamp(0,1)        \n",
    "            # x = x.transpose(0,1)\n",
    "            #     \n",
    "            \n",
    "            \n",
    "        #     return x\n",
    "        #     \n",
    "        #     \n",
    "        #     sys.exit(0)\n",
    "        #     x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "        #     x_lack_sum = x_sum - x    \n",
    "        #     x_mean = x_lack_sum/x.shape[-1]\n",
    "        #     \n",
    "        #     \n",
    "        #             \n",
    "        #     x_mean_ancle = (x_mean*2-1).acos()  \n",
    "        #     \n",
    "        #     ang_inc = self.ang_inc.unsqueeze(-1).expand(x_mean_ancle.shape) \n",
    "        #     # ang_inc = np.pi/2/(x.max(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) -x.min(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) )\n",
    "        #     \n",
    "        #     if given_ang!=-1:\n",
    "        #         x_mean_rote = (np.pi/2 - x_mean_ancle)*given_ang\n",
    "        #     else:\n",
    "        #         x_mean_rote = (np.pi/2 - x_mean_ancle)*ang_inc\n",
    "        #     \n",
    "        #     x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])            \n",
    "        #     self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "        #                           (1 - self.momentum) * x_moving_rot\n",
    "        #                                         \n",
    "        #     x_ancle = (x*2-1).acos()\n",
    "        #     x_final = x_ancle+x_mean_rote  \n",
    "        #     x_1 = (x_final.cos()+1)/2                                \n",
    "        #     x_1 = x_1.transpose(0,1)\n",
    "        # return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm_try2(nn.Module):\n",
    "    def __init__(self, num_features, batch_size=32, init_ang_inc = 0, momentum=0.1):        \n",
    "        super(QC_Norm_try2, self).__init__()\n",
    "        \n",
    "        self.x_running_rot = Parameter(torch.zeros((num_features,batch_size)),requires_grad=False)        \n",
    "        self.ang_inc = Parameter(torch.ones(1)*init_ang_inc)\n",
    "        \n",
    "        self.momentum = momentum\n",
    "                \n",
    "        self.printed = False\n",
    "        self.x_mean_ancle=0\n",
    "        self.x_mean_rote = 0\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "        \n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                print(\"self.x_running_rot\",self.x_running_rot)\n",
    "                self.printed = True\n",
    "                    \n",
    "            x = x.transpose(0,1)\n",
    "            x_1  = (self.x_running_rot*(1-x)+x)  \n",
    "            x_1 = x_1.transpose(0,1) \n",
    "            # x_ancle = (x*2-1).acos()\n",
    "            # x_final = x_ancle+self.x_running_rot.unsqueeze(-1)\n",
    "            # x_1 = (x_final.cos()+1)/2\n",
    "            #                     \n",
    "            # x_1 = x_1.transpose(0,1)\n",
    "            # print(x_1)\n",
    "        else:\n",
    "            self.printed = False\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "            x_lack_sum = x_sum - x    \n",
    "            x_mean = x_lack_sum/x.shape[-1]\n",
    "            \n",
    "            \n",
    "                                  \n",
    "            # ang_inc = self.ang_inc.unsqueeze(-1).expand(x.shape)    \n",
    "            \n",
    "            y  = ((0.5-x_mean)/(1-x_mean))\n",
    "            # y = clipfunc(y)\n",
    "            \n",
    "            # print(y)\n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * y\n",
    "\n",
    "            # self.x_running_rot[:] = y\n",
    "            \n",
    "            x_1  = (y*(1-x)+x)\n",
    "            x_1 = x_1.transpose(0,1) \n",
    "            # x_1  = (y*(1-x)+x)\n",
    "            # sys.exit(0)\n",
    "            # \n",
    "            # \n",
    "            # x_mean_ancle = (x_mean*2-1).acos()\n",
    "            # ang_inc = self.ang_inc.unsqueeze(-1).expand(x_mean_ancle.shape)             \n",
    "            # \n",
    "            # if given_ang!=-1:\n",
    "            #     x_mean_rote = (np.pi/2 - x_mean_ancle)*given_ang\n",
    "            # else:\n",
    "            #     x_mean_rote = (np.pi/2 - x_mean_ancle)*ang_inc\n",
    "            # \n",
    "            # x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])            \n",
    "            # self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "            #                       (1 - self.momentum) * x_moving_rot\n",
    "            #                                     \n",
    "            # x_ancle = (x*2-1).acos()\n",
    "            # x_final = x_ancle+x_mean_rote  \n",
    "            # x_1 = (x_final.cos()+1)/2                                \n",
    "            # x_1 = x_1.transpose(0,1)\n",
    "      \n",
    "        return x_1\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        self.ang_inc.data.zeros_()\n",
    "\n",
    "\n",
    "class QC_Norm_try3(nn.Module):\n",
    "    def __init__(self, num_features, batch_size=32, init_ang_inc = 1, momentum=0.1):        \n",
    "        super(QC_Norm_try3, self).__init__()\n",
    "        \n",
    "        self.x_running_rot = Parameter(torch.zeros((num_features)),requires_grad=False)        \n",
    "        # self.ang_inc = Parameter(torch.ones((num_features)),requires_grad=True)\n",
    "        self.ang_inc = Parameter(torch.tensor(init_ang_inc,dtype=torch.float32),requires_grad=True)        \n",
    "        self.momentum = momentum\n",
    "                \n",
    "        self.printed = False\n",
    "        self.x_mean_ancle=0\n",
    "        self.x_mean_rote = 0\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "        \n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                # print(\"self.ang_inc\",self.ang_inc)\n",
    "                self.printed = True                            \n",
    "            x_1  = (self.x_running_rot*x)               \n",
    "     \n",
    "        else:\n",
    "            self.printed = False\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "            x_lack_sum = x_sum + x    \n",
    "            x_mean = x_lack_sum/x.shape[-1]                    \n",
    "            \n",
    "            \n",
    "            \n",
    "            ang_inc = (self.ang_inc>0).float()*self.ang_inc+1\n",
    "            # ang_inc = self.ang_inc.abs()+1\n",
    "            # ang_inc = 2\n",
    "            y  = 0.5/x_mean\n",
    "            y = y.transpose(0,1)\n",
    "            y = y/ang_inc\n",
    "            y = y.transpose(0,1)\n",
    "    \n",
    "            x_moving_rot = (y.sum(-1)/x.shape[-1]) \n",
    "            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot\n",
    "            \n",
    "            x_1  = y*x\n",
    "            x_1 = x_1.transpose(0,1) \n",
    "            \n",
    "        return x_1\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        self.ang_inc.data.zeros_()\n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm_Correction_try2(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Correction_try2, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        self.x_l_0_5 = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.x_g_0_5 = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:            \n",
    "            x_1  = self.x_l_0_5*(self.x_running_rot*(1-x)+x)\n",
    "            x_1 += self.x_g_0_5*(self.x_running_rot*x)            \n",
    "        else:\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1)            \n",
    "            x_mean = x_sum/x.shape[-1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.x_l_0_5[:] = ((x_mean<=0.5).float())\n",
    "            self.x_g_0_5[:] = ((x_mean>0.5).float())\n",
    "            \n",
    "            y  = self.x_l_0_5*((0.5-x_mean)/(1-x_mean))\n",
    "            y += self.x_g_0_5*(0.5/x_mean)\n",
    "            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * y\n",
    "            \n",
    "            \n",
    "            x = x.transpose(0,1)  \n",
    "            x_1  = self.x_l_0_5*(y*(1-x)+x)\n",
    "            x_1 += self.x_g_0_5*(y*x)\n",
    "            \n",
    "        return x_1\n",
    "    \n",
    "\n",
    "class BinaryLinearClassic(nn.Linear):\n",
    "\n",
    "    def forward(self, input):\n",
    "        binary_weight = binarize(self.weight)\n",
    "        if self.bias is None:\n",
    "            output = F.linear(input, binary_weight)\n",
    "            output = torch.div(output, input.shape[-1])\n",
    "            output = torch.pow(output, 2)\n",
    "\n",
    "            return output\n",
    "        else:\n",
    "            print(\"Not Implement\")\n",
    "            sys.exit(0)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Glorot initialization\n",
    "        in_features, out_features = self.weight.size()\n",
    "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "        self.weight.lr_scale = 1. / stdv\n",
    "\n",
    "\n",
    "\n",
    "## Define the NN architecture\n",
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc1 = BinaryLinear(2,num_f1,bias=False)\n",
    "        self.fc2 = BinaryLinear(num_f1,num_f2,bias=False)\n",
    "        # self.fc3 = BinaryLinear(num_f2,num_f3,bias=False)\n",
    "        # # # \n",
    "        # \n",
    "        # # if not with_norm or True:\n",
    "        # self.clc_fc1 = BinaryLinearClassic(in_features=2, out_features=num_f1, bias=False)            \n",
    "        # self.clc_fc2 = BinaryLinearClassic(in_features=num_f1, out_features=num_f2, bias=False)\n",
    "        # self.clc_fc3 = BinaryLinearClassic(in_features=num_f2, out_features=num_f3, bias=False)\n",
    "        # \n",
    "        # \n",
    "        # self.clc_bn1 = nn.BatchNorm1d(num_features=num_f1)\n",
    "        # self.clc_bn2 = nn.BatchNorm1d(num_features=num_f2)\n",
    "        if with_norm:\n",
    "            self.qc1 = QC_Norm_try3(num_features=num_f1,init_ang_inc=[1])\n",
    "            self.qc2 = QC_Norm_try3(num_features=num_f2,init_ang_inc=[1,1])\n",
    "            # self.qc3 = QC_Norm(num_features=num_f3)\n",
    "    \n",
    "            # self.qc1a = QC_Norm_True(num_features=num_f1)\n",
    "            # self.qc2a = QC_Norm_True(num_features=num_f2)\n",
    "            # self.qc3a = QC_Norm_Correction(num_features=num_f3)\n",
    "            \n",
    "            self.qc1a = QC_Norm_Correction_try2(num_features=num_f1)\n",
    "            self.qc2a = QC_Norm_Correction_try2(num_features=num_f2)\n",
    "            \n",
    "            # \n",
    "            # self.qc1 = QC_Norm_Real(num_features=num_f1)\n",
    "            # self.qc2 = QC_Norm_Real(num_features=num_f2)\n",
    "            # self.qc3 = QC_Norm_Real(num_features=num_f3)\n",
    "\n",
    "\n",
    "        # self.qc1a = QC_Norm_Real_Correction(num_features=num_f1)\n",
    "        # self.qc2a = QC_Norm_Real_Correction(num_features=num_f2)\n",
    "        # self.qc3a = QC_Norm_Real_Correction(num_features=num_f3)\n",
    "        # \n",
    "    def forward(self, x, training=1):        \n",
    "        x = x.view(-1, 2)\n",
    "        \n",
    "        if training == 1:\n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            # \n",
    "            \n",
    "\n",
    "            if with_norm:\n",
    "                # x = self.qc1a(self.fc1(x))                \n",
    "                # x = self.qc2a(self.fc2(x))\n",
    "                                                \n",
    "                \n",
    "                x = self.qc1(self.qc1a(self.fc1(x)))\n",
    "                x = (self.qc2a(self.fc2(x)))\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                # print(x)\n",
    "                # x = self.fc1(x)\n",
    "                # print(x)\n",
    "                # x = self.qc1a(x)\n",
    "                # print(x)\n",
    "                # x = self.qc1(x)\n",
    "                # print(x)\n",
    "                # \n",
    "                # print(x)\n",
    "                # x = self.fc2(x)\n",
    "                # print(x)\n",
    "                # x = self.qc2a(x)\n",
    "                # print(x)\n",
    "                # x = self.qc2(x)\n",
    "                # print(x)\n",
    "                \n",
    "                # print(x)\n",
    "                # sys.exit(0)\n",
    "            else:\n",
    "                x = self.clc_fc1(x)        \n",
    "                x = self.clc_fc2(x)\n",
    "                # x = self.clc_fc3(x)  \n",
    "\n",
    "            # x = self.qc3(self.qc3a(self.fc3(x)))\n",
    "            # \n",
    "            # x = self.qc1((self.fc1(x)))        \n",
    "            # x = self.qc2((self.fc2(x)))                           \n",
    "            # x = self.qc3((self.fc3(x)))\n",
    "            # \n",
    "        elif training == 2:\n",
    "            \n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            \n",
    "            print(\"=\"*10,\"layer 1\",\"=\"*10)\n",
    "            print(x)\n",
    "            torch.set_printoptions(profile=\"full\")            \n",
    "            print(binarize(self.fc1.weight))                                            \n",
    "            torch.set_printoptions(profile=\"default\")\n",
    "            x = self.fc1(x)            \n",
    "            \n",
    "            print(\"=\"*10,\"layer 2\",\"=\"*10)\n",
    "            print(x)\n",
    "            torch.set_printoptions(profile=\"full\")            \n",
    "            print(binarize(self.fc2.weight))                                            \n",
    "            torch.set_printoptions(profile=\"default\")\n",
    "            x = self.fc2(x)\n",
    "            \n",
    "            print(\"=\"*10,\"results\",\"=\"*10)\n",
    "            print(x)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # x = self.qc1(self.fc1(x),training=False)\n",
    "            #     x = self.qc2(self.fc2(x),training=False)\n",
    "            # x = self.qc3(self.fc3(x),training=False)\n",
    "            # \n",
    "            \n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            # \n",
    "            # x = self.qc3(self.qc3a(self.fc3(x),training=False),training=False)\n",
    "            \n",
    "            if with_norm:                \n",
    "                if not print_detail:\n",
    "                    x = self.qc1(self.qc1a(self.fc1(x),training=False),training=False)                \n",
    "                    x = (self.qc2a(self.fc2(x),training=False))\n",
    "                else:\n",
    "                    print(x)\n",
    "                    x = self.fc1(x)\n",
    "                    print(x)\n",
    "                    x = self.qc1a(x,training=False)\n",
    "                    print(x)\n",
    "                    x = self.qc1(x,training=False)\n",
    "                    print(x)\n",
    "    \n",
    "                    print(x)\n",
    "                    x = self.fc2(x)\n",
    "                    print(x)\n",
    "                    x = self.qc2a(x,training=False)\n",
    "                    print(x)\n",
    "                    # x = self.qc2(x,training=False)\n",
    "                    print(x)\n",
    "                    # \n",
    "                    # x = self.qc1a(self.fc1(x),training=False)                \n",
    "                    # x = self.qc2a(self.fc2(x),training=False)                                        \n",
    "            else:\n",
    "                x = self.clc_fc1(x)                    \n",
    "                x = self.clc_fc2(x)\n",
    "                # x = self.clc_fc3(x)  \n",
    "            \n",
    "        if num_f2==1 or num_f1==1:            \n",
    "            x = torch.cat((x,1-x),-1)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    epoch_loss = []\n",
    "    batch_idx = 0\n",
    "    for (data, target) in dataset:\n",
    "        \n",
    "        \n",
    "        target = target.view(batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # target,new_target = modify_target(target)\n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        # \n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data,True)\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()    \n",
    "        \n",
    "        # print(output.shape,target.shape)\n",
    "        \n",
    "        # sys.exit(0)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "                \n",
    "        if batch_idx % 20 == 0:        \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}/{} ({:.2f}%)'.format(\n",
    "                epoch, batch_idx * len(data), len(dataset),\n",
    "                100. * batch_idx / len(dataset), loss, correct, (batch_idx+1) * len(data),\n",
    "                100. * float(correct) / float(((batch_idx+1) * len(data)) )))                \n",
    "        batch_idx += 1\n",
    "    print(\"-\"*20,\"training done, loss\",\"-\"*20)\n",
    "    print(\"Training Set: Average loss: {}\".format(round(sum(epoch_loss)/len(epoch_loss),6)))\n",
    "    \n",
    "accur=[]\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in dataset:\n",
    "        # target,new_target = modify_target(target)\n",
    "        target = target.view(batch_size)\n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        \n",
    "        # print(\"=\"*100)        \n",
    "        # print(data)\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # print(\"Debug\")\n",
    "        # output = model(data,2)\n",
    "        # \n",
    "        \n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data,False)\n",
    "        \n",
    "        # \n",
    "        test_loss += criterion(output, target) # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get t\n",
    "        # print(pred.view(batch_size))# he index of the max log-probability\n",
    "        # print(target,output)\n",
    "        # sys.exit(0)\n",
    "        # print(pred.eq(target.data.view_as(pred)).cpu().sum())\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    a=100.*correct / len(dataset)*batch_size\n",
    "    accur.append(a)  \n",
    "    test_loss /= len(dataset)*batch_size\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(dataset)*batch_size,\n",
    "        100. * float(correct) / float(len(dataset)*batch_size)))\n",
    "    \n",
    "    return float(correct) / (len(dataset)*batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Training\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "print(\"=\"*10,\"Model Info\",\"=\"*10)\n",
    "print(model)\n",
    "# summary(model,(1,img_size,img_size))\n",
    "\n",
    "# \n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "if with_norm and given_ang==-1:\n",
    "    optimizer = torch.optim.Adam([\n",
    "                    {'params': model.fc1.parameters()},\n",
    "                    # {'params': model.fc2.parameters()},\n",
    "                    # {'params': model.fc3.parameters()},\n",
    "                    {'params': model.qc1.parameters(), 'lr': init_qc_lr},\n",
    "                    # {'params': model.qc2.parameters(), 'lr': init_qc_lr},\n",
    "                    # {'params': model.qc3.parameters(), 'lr': 1},\n",
    "                ], lr=init_lr)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# optimizer = torch.optim.SGD([\n",
    "#                 {'params': model.fc1.parameters()},\n",
    "#                 {'params': model.fc2.parameters()},\n",
    "#                 {'params': model.fc3.parameters()},\n",
    "#                 {'params': model.qc1.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc2.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc3.parameters(), 'lr': 1},\n",
    "#             ], lr=0.1, momentum=0.9)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \\\n",
    "#         base_lr=[1e-1,1e-1,1e-1,1,1,1], \\\n",
    "#         max_lr=[1e-3,1e-3,1e-3,1e-2,1e-2,1e-2], \\\n",
    "#         step_size_up=100\n",
    "#         )\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)\n",
    "\n",
    "# \n",
    "# \n",
    "# test()\n",
    "# \n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile(resume_path):\n",
    "    print(\"=> loading checkpoint from '{}'<=\".format(resume_path))\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    epoch_init,acc = checkpoint[\"epoch\"],checkpoint[\"acc\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])    \n",
    "    scheduler.milestones = Counter(milestones)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "else:\n",
    "    epoch_init,acc = 0,0\n",
    "\n",
    "# for name,para in model.named_parameters():\n",
    "#     print(name,para)\n",
    "\n",
    "# sys.exit(0)\n",
    "import copy\n",
    "best_model = -1\n",
    "acc= 0\n",
    "if training:\n",
    "    for epoch in range(epoch_init, max_epoch + 1):\n",
    "        print(\"=\"*20,epoch,\"epoch\",\"=\"*20)  \n",
    "        print(\"Epoch Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))        \n",
    "\n",
    "        print(\"-\"*20,\"learning rates\",\"-\"*20)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(param_group['lr'],end=\",\")\n",
    "        print()    \n",
    "        \n",
    "        print(\"-\"*20,\"training\",\"-\"*20)\n",
    "        print(\"Trainign Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        train(epoch)\n",
    "        print(\"Trainign End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        \n",
    "        # \n",
    "        # for name,para in model.named_parameters():\n",
    "        #     print(name,para)\n",
    "        # \n",
    "        print(\"-\"*20,\"testing\",\"-\"*20)\n",
    "        print(\"Testing Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))        \n",
    "        cur_acc = test()\n",
    "        print(\"Testing End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"-\"*60)\n",
    "        print()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        is_best = False\n",
    "        if cur_acc > acc:\n",
    "            is_best = True\n",
    "            acc=cur_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "            \n",
    "            \n",
    "        \n",
    "        print(\"Best accuracy: {}; Current accuracy {}. Checkpointing\".format(acc,cur_acc))\n",
    "        if save_chkp:\n",
    "            save_checkpoint({\n",
    "              'epoch': epoch + 1,\n",
    "              'acc': acc, \n",
    "              'state_dict': model.state_dict(),      \n",
    "              'optimizer' : optimizer.state_dict(),\n",
    "               'scheduler': scheduler.state_dict(),\n",
    "            }, is_best, save_path, 'checkpoint_{}_{}.pth.tar'.format(epoch,round(cur_acc,4)))\n",
    "        print(\"Epoch End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"=\"*60)\n",
    "        print()        \n",
    "else:    \n",
    "    # print(\"=\"*20,epoch,\"Testing\",\"=\"*20)  \n",
    "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "    #     num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "    # test()\n",
    "\n",
    "\n",
    "\n",
    "    print(\"=\"*100)\n",
    "    for name,para in model.named_parameters():\n",
    "        print(name,para)\n",
    "    print(\"=\"*100)\n",
    "\n",
    "\n",
    "    # output = model(torch.tensor([0.2,0.6]),False)\n",
    "    # print(output[0][0],output[0][1])\n",
    "    \n",
    "\n",
    "    # \n",
    "    # # # \n",
    "    # # model(torch.tensor([0.95,0.95]),False)\n",
    "    # sys.exit(0)\n",
    "    # \n",
    "    gap = []\n",
    "    pair = {}\n",
    "    for i in range(i_dim):\n",
    "        for j in range(j_dim):\n",
    "            input = ([(1.0/i_dim)*(i+1),(1.0/j_dim)*(j+1)])            \n",
    "            output = model(torch.tensor(input),False)\n",
    "            gap.append((output[0][0]-output[0][1]).data)\n",
    "            pair[tuple(input)] = [output[0][0],output[0][1]]\n",
    "            # print(input,output.data,(output[0][0]-output[0][1]).data)\n",
    "    \n",
    "    \n",
    "    # for k,v in pair.items():        \n",
    "    #     # if abs(v[0]-v[1]) in torch.tensor(gap).abs().topk(70)[0]: #and golden_data[k]==1:\n",
    "    #     print((v[0].data-v[1].data).abs(),v[0].data,v[1].data,golden_data[k],k)\n",
    "    # sys.exit(0)\n",
    "\n",
    "\n",
    "if training:\n",
    "    print(\"=\"*100)\n",
    "    for name,para in best_model.named_parameters():\n",
    "        print(name,para)\n",
    "    print(\"=\"*100) \n",
    "    for i in range(i_dim):\n",
    "        for j in range(j_dim):\n",
    "            data = dataset_ori[i*j_dim+j][0]\n",
    "            output = model(data,False)\n",
    "    \n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            print(pred.data.item(),end = \" \")\n",
    "        print()\n",
    "    \n",
    "    for i in range(i_dim):\n",
    "        for j in range(j_dim):\n",
    "            data = dataset_ori[i*j_dim+j][0]\n",
    "            output = model(data,False)\n",
    "    \n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            print(pred.data.item(),end = \",\")\n",
    "    print()     \n",
    "\n",
    "sys.exit(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"=\"*30)\n",
    "x = 0.95\n",
    "y = 0.65\n",
    "print(torch.tensor([((x*(1-y)+y*(1-x))*2)-1]).acos())\n",
    "# print(torch.tensor([(0.3*2)-1]).acos())\n",
    "# print(torch.tensor([(0.1*2)-1]).acos())\n",
    "print(\"=\"*30)\n",
    "print(torch.tensor([1-(0.1393*2)]).acos())\n",
    "print(torch.tensor([(0.9699*2)-1]).acos())\n",
    "# print(torch.tensor([(0.8607*0.9699*2)-1]).acos())\n",
    "# print(torch.tensor([(0.0733*2)-1]).acos())\n",
    "\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(torch.tensor([1-(0.8*2)]).acos())\n",
    "print(torch.tensor([1-(0.6*2)]).acos())\n",
    "print(torch.tensor([1-(0.1*2)]).acos())\n",
    "print(torch.tensor([1-(0.3*2)]).acos())\n",
    "\n",
    "x = torch.tensor([0.8,0.8,0.8])\n",
    "y = torch.tensor([0.6,0.1,0.3])\n",
    "print((1-((1-x)*y+(1-y)*x)*2).acos())\n",
    "# \n",
    "# print(torch.tensor([1-((0.8*0.4+0.2*0.6)*2)]).acos())\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../interfae\")\n",
    "from qiskit_simulator import *\n",
    "\n",
    "\n",
    "\n",
    "accur=[]\n",
    "def test_debug():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target,new_target = modify_target(target)\n",
    "        \n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # print(\"Debug\",data,target)\n",
    "        # output = model(data,2)\n",
    "        # \n",
    "        # run_simulator(model,data)\n",
    "        \n",
    "        output = model(data,False)\n",
    "        # print(data)\n",
    "        # print(output)\n",
    "        # print(target)\n",
    "        # sys.exit(0)\n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "        # output = model(data,False)\n",
    "        test_loss += criterion(output, target) # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    a=100.*correct / len(test_loader.dataset)\n",
    "    accur.append(a)  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / float(len(test_loader.dataset))))\n",
    "    \n",
    "    return float(correct) / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile(resume_path):\n",
    "    print(\"=> loading checkpoint from '{}'<=\".format(resume_path))\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    epoch_init,acc = checkpoint[\"epoch\"],checkpoint[\"acc\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])    \n",
    "    scheduler.milestones = Counter(milestones)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "else:\n",
    "    epoch_init,acc = 0,0\n",
    "\n",
    "print(\"HERE\")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "        num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "test_debug()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for name, para in model.named_parameters():\n",
    "# #     print(name,para)\n",
    "# print(\"HERE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8213722",
   "language": "python",
   "display_name": "PyCharm (qiskit_practice)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}