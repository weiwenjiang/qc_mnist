{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import functools\n",
    "print = functools.partial(print, flush=True)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "interest_num = [3,6]\n",
    "img_size = 4\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 1\n",
    "inference_batch_size = 1\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Demo 2 on MNIST. This script is QF-Map for Quantum Circuit from Weights.\")\n",
    "print(\"\\tStart at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "print(\"\\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\")\n",
    "print(\"\\tEnjoy and Good Luck!\")\n",
    "print(\"=\"*100)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def modify_target(target):\n",
    "    for j in range(len(target)):\n",
    "        for idx in range(len(interest_num)):\n",
    "            if target[j] == interest_num[idx]:\n",
    "                target[j] = idx\n",
    "                break\n",
    "    \n",
    "    new_target = torch.zeros(target.shape[0],2)\n",
    "        \n",
    "    for i in range(target.shape[0]):        \n",
    "        if target[i].item() == 0:            \n",
    "            new_target[i] = torch.tensor([1,0]).clone()     \n",
    "        else:\n",
    "            new_target[i] = torch.tensor([0,1]).clone()\n",
    "               \n",
    "    return target,new_target\n",
    "\n",
    "def select_num(dataset,interest_num):\n",
    "    labels = dataset.targets #get labels\n",
    "    labels = labels.numpy()\n",
    "    idx = {}\n",
    "    for num in interest_num:\n",
    "        idx[num] = np.where(labels == num)\n",
    "        \n",
    "    fin_idx = idx[interest_num[0]]\n",
    "    for i in range(1,len(interest_num)):           \n",
    "        \n",
    "        fin_idx = (np.concatenate((fin_idx[0],idx[interest_num[i]][0])),)\n",
    "    \n",
    "    fin_idx = fin_idx[0]    \n",
    "    \n",
    "    dataset.targets = labels[fin_idx]\n",
    "    dataset.data = dataset.data[fin_idx]\n",
    "    \n",
    "    # print(dataset.targets.shape)\n",
    "    \n",
    "    dataset.targets,_ = modify_target(dataset.targets)\n",
    "    # print(dataset.targets.shape)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def qc_input_trans(dataset):\n",
    "    dataset.data = dataset.data\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class ToQuantumData(object):\n",
    "    def __call__(self, tensor):        \n",
    "        data = tensor                \n",
    "        input_vec = data.view(-1)\n",
    "        vec_len = input_vec.size()[0]\n",
    "        input_matrix = torch.zeros(vec_len,vec_len)\n",
    "        input_matrix[0] = input_vec\n",
    "        input_matrix = input_matrix.transpose(0,1)        \n",
    "        u,s,v = np.linalg.svd(input_matrix)    \n",
    "        output_matrix = torch.tensor(np.dot(u,v))            \n",
    "        output_data = output_matrix[:,0].view(1,img_size,img_size)    \n",
    "        return output_data\n",
    "    \n",
    "\n",
    "class ToQuantumMatrix(object):\n",
    "    def __call__(self, tensor):        \n",
    "        data = tensor                \n",
    "        input_vec = data.view(-1)\n",
    "        vec_len = input_vec.size()[0]\n",
    "        input_matrix = torch.zeros(vec_len,vec_len)\n",
    "        input_matrix[0] = input_vec\n",
    "        input_matrix = np.float64(input_matrix.transpose(0,1))        \n",
    "        u,s,v = np.linalg.svd(input_matrix)    \n",
    "        output_matrix = torch.tensor(np.dot(u,v))                        \n",
    "        return output_matrix\n",
    "                \n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                transforms.ToTensor(),ToQuantumData()])\n",
    "# transform = transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "#                                 transforms.ToTensor(),ToQuantumData()])\n",
    "# transform = transforms.Compose([transforms.Resize((img_size,img_size)),transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# choose the training and test datasets\n",
    "\n",
    "# Path to MNIST Dataset\n",
    "train_data = datasets.MNIST(root='../../pytorch/data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='../../pytorch/data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "train_data = select_num(train_data,interest_num)\n",
    "test_data =  select_num(test_data,interest_num)\n",
    "\n",
    "# train_data = qc_input_trans(train_data)\n",
    "\n",
    "# imshow(torchvision.utils.make_grid(train_data[0][0]))\n",
    "# \n",
    "# sys.exit(0)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "    num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    filename = os.path.join(save_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        bestname = os.path.join(save_path, 'model_best.tar')\n",
    "        shutil.copyfile(filename, bestname)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# functions to show an image\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))    \n",
    "    plt.show()\n",
    "    \n",
    "    # image = np.asarray(npimg[0] * 255, np.uint8)    \n",
    "    # \n",
    "    # im = Image.fromarray(image,mode=\"L\")\n",
    "    # im.save(\"32*32.jpg\",cmap=\"gray\") \n",
    "    # im = im.resize((4,4),Image.BILINEAR)    \n",
    "    # \n",
    "    # plt.imshow(im,cmap='gray',)\n",
    "    # \n",
    "    # trans_to_tensor = transforms.ToTensor()\n",
    "    # trans_to_matrix = ToQuantumMatrix()\n",
    "    # plt.show()\n",
    "    # im.save(\"4*4.jpg\",cmap=\"gray\") \n",
    "    \n",
    "    # print(trans_to_tensor(im))\n",
    "    # for row in trans_to_matrix(trans_to_tensor(im)).tolist():\n",
    "    #     for num in row:\n",
    "    #         print(num,end=\",\")\n",
    "    #     print()\n",
    "\n",
    "#     \n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#     torch.set_printoptions(threshold=sys.maxsize)\n",
    "#     imshow(torchvision.utils.make_grid(data))\n",
    "#     break\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from lib_qc import * \n",
    "from lib_util import *\n",
    "from lib_net import *\n",
    "\n",
    "# Network Architecture: 2 layers and each layer contains 2 neurons\n",
    "\n",
    "img_size = 4\n",
    "device = torch.device(\"cpu\")\n",
    "layers = [2, 2]\n",
    "\n",
    "model = Net(img_size,layers,True,[[1,1,1,1],[1,1]],True,False,False,False,True).to(device)\n",
    "\n",
    "resume_path=\"checkpoint_8_0.9573.pth.tar\"\n",
    "print(\"=> loading checkpoint from '{}'<=\".format(resume_path))\n",
    "checkpoint = torch.load(resume_path, map_location=device)\n",
    "epoch_init, acc = checkpoint[\"epoch\"], checkpoint[\"acc\"]\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "\n",
    "for name, para in model.named_parameters():\n",
    "    if \"fc\" in name:\n",
    "        print(name,binarize(para))\n",
    "    else:\n",
    "        print(name, para)\n",
    "\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    torch.set_printoptions(threshold=sys.maxsize)\n",
    "    imshow(torchvision.utils.make_grid(data))\n",
    "    # print(data)\n",
    "    \n",
    "    Q_InputMatrix = ToQuantumMatrix()(data)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Visualization of Model\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "text": [
      "tensor([[[[0.0000, 0.3088, 0.0755, 0.0000],\n",
      "          [0.0000, 0.4186, 0.2059, 0.0274],\n",
      "          [0.0000, 0.3843, 0.6999, 0.1098],\n",
      "          [0.0000, 0.0618, 0.1647, 0.0069]]]]) tensor([1])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from qiskit import QuantumRegister, QuantumCircuit\n",
    "from qiskit.extensions import XGate, UnitaryGate\n",
    "import qiskit\n",
    "from math import sqrt \n",
    "import math\n",
    "import copy\n",
    "\n",
    "def get_index_list(input,target):\n",
    "    index_list = []\n",
    "    try:\n",
    "        beg_pos = 0\n",
    "        while True:\n",
    "            find_pos = input.index(target,beg_pos)\n",
    "            index_list.append(find_pos)\n",
    "            beg_pos = find_pos+1\n",
    "    except Exception as exception:        \n",
    "        pass    \n",
    "    return index_list\n",
    "\n",
    "def change_sign(sign,bin):\n",
    "    affect_num = [bin]\n",
    "    one_positions = []\n",
    "    \n",
    "    try:\n",
    "        beg_pos = 0\n",
    "        while True:\n",
    "            find_pos = bin.index(\"1\",beg_pos)\n",
    "            one_positions.append(find_pos)\n",
    "            beg_pos = find_pos+1\n",
    "    except Exception as exception:\n",
    "        # print(\"Not Found\")\n",
    "        pass\n",
    "    \n",
    "    for k,v in sign.items():\n",
    "        change = True\n",
    "        for pos in one_positions:\n",
    "            if k[pos]==\"0\":                \n",
    "                change = False\n",
    "                break\n",
    "        if change:\n",
    "            sign[k] = -1*v\n",
    "    \n",
    "\n",
    "def find_start(affect_count_table,target_num):\n",
    "    for k in list(affect_count_table.keys())[::-1]:\n",
    "        if target_num<=k:\n",
    "            return k\n",
    "\n",
    "\n",
    "def recursive_change(direction,start_point,target_num,sign,affect_count_table,quantum_gates):\n",
    "    \n",
    "    if start_point == target_num:\n",
    "        # print(\"recursive_change: STOP\")\n",
    "        return\n",
    "    \n",
    "    gap = int(math.fabs(start_point-target_num))    \n",
    "    step = find_start(affect_count_table,gap)\n",
    "    change_sign(sign,affect_count_table[step])\n",
    "    quantum_gates.append(affect_count_table[step])\n",
    "    \n",
    "    if direction==\"r\": \n",
    "        # print(\"recursive_change: From\",start_point,\"Right(-):\",step)\n",
    "        start_point = start_point - step\n",
    "        direction = \"l\"\n",
    "        recursive_change(direction,start_point,target_num,sign,affect_count_table,quantum_gates)\n",
    "        \n",
    "    else:        \n",
    "        # print(\"recursive_change: From\",start_point,\"Left(+):\",step)\n",
    "        start_point = start_point + step\n",
    "        direction = \"r\"\n",
    "        recursive_change(direction,start_point,target_num,sign,affect_count_table,quantum_gates)\n",
    "        \n",
    "    \n",
    "\n",
    "def guarntee_upper_bound_algorithm(sign,target_num,total_len,digits):        \n",
    "    flag = \"0\"+str(digits)+\"b\"\n",
    "    pre_num = 0\n",
    "    affect_count_table = {}\n",
    "    quantum_gates = []\n",
    "    for i in range(digits):\n",
    "        cur_num = pre_num + pow(2,i)\n",
    "        pre_num = cur_num\n",
    "        binstr_cur_num = format(cur_num,flag) \n",
    "        affect_count_table[int(pow(2,binstr_cur_num.count(\"0\")))] = binstr_cur_num   \n",
    "        \n",
    "    if target_num in affect_count_table.keys():\n",
    "        quantum_gates.append(affect_count_table[target_num])\n",
    "        change_sign(sign,affect_count_table[target_num])    \n",
    "    else:\n",
    "        direction = \"r\"\n",
    "        start_point = find_start(affect_count_table,target_num)\n",
    "        quantum_gates.append(affect_count_table[start_point])\n",
    "        change_sign(sign,affect_count_table[start_point])\n",
    "        recursive_change(direction,start_point,target_num,sign,affect_count_table,quantum_gates)\n",
    "    \n",
    "    return quantum_gates\n",
    "\n",
    "def qf_map_extract_from_weight(weights):    \n",
    "    # Find Z control gates according to weights\n",
    "    w = (weights.detach().cpu().numpy())\n",
    "    total_len = len(w)\n",
    "    target_num = np.count_nonzero(w == -1)\n",
    "    if target_num > total_len/2:\n",
    "        w = w*-1\n",
    "    target_num = np.count_nonzero(w == -1)    \n",
    "    digits = int(math.log(total_len,2))\n",
    "    flag = \"0\"+str(digits)+\"b\"\n",
    "    max_num = int(math.pow(2,digits))\n",
    "    sign = {}\n",
    "    for i in range(max_num):        \n",
    "        sign[format(i,flag)] = +1\n",
    "    quantum_gates = guarntee_upper_bound_algorithm(sign,target_num,total_len,digits)\n",
    "    \n",
    "    # for k,v in sign.items():\n",
    "    #     print(k,v)\n",
    "    # print(w)\n",
    "        \n",
    "    # Build the mapping from weight to final negative num \n",
    "    fin_sign = list(sign.values())\n",
    "    fin_weig = [int(x) for x in list(w)]\n",
    "    sign_neg_index = []    \n",
    "    try:\n",
    "        beg_pos = 0\n",
    "        while True:\n",
    "            find_pos = fin_sign.index(-1,beg_pos)\n",
    "            sign_neg_index.append(find_pos)\n",
    "            beg_pos = find_pos+1\n",
    "    except Exception as exception:        \n",
    "        pass    \n",
    "    weight_neg_index = []\n",
    "    try:\n",
    "        beg_pos = 0\n",
    "        while True:\n",
    "            find_pos = fin_weig.index(-1,beg_pos)\n",
    "            weight_neg_index.append(find_pos)\n",
    "            beg_pos = find_pos+1\n",
    "    except Exception as exception:        \n",
    "        pass    \n",
    "    map = {}\n",
    "    for i in range(len(sign_neg_index)):\n",
    "        map[sign_neg_index[i]] = weight_neg_index[i]\n",
    "            \n",
    "    # print(map)\n",
    "    \n",
    "    ret_index = list(range(len(fin_weig)))\n",
    "    \n",
    "    for k,v in map.items():\n",
    "        old_val = ret_index[k] \n",
    "        ret_index[k] = v\n",
    "        ret_index[v] = old_val\n",
    "    \n",
    "\n",
    "    return quantum_gates,ret_index\n",
    "    \n",
    "    \n",
    "def extract_model(model):\n",
    "    layer_prop = {}\n",
    "    batch_adj_prop = {}\n",
    "    indiv_adj_prop = {}\n",
    "    for name, para in model.named_parameters():\n",
    "        if \"fc\" in name:\n",
    "            layer_id = int(name.split(\".\")[0].split(\"c\")[1])\n",
    "            layer_prop[layer_id] = [para.shape[1],para.shape[0],binarize(para)]            \n",
    "        elif \"qca\" in name:            \n",
    "            if \"l_0_5\" in name or \"running_rot\" in name:\n",
    "                layer_id = int(name.split(\".\")[0].split(\"a\")[1])\n",
    "                layer_fun = name.split(\".\")[1]\n",
    "                if layer_id not in batch_adj_prop.keys():\n",
    "                    batch_adj_prop[layer_id] = {}\n",
    "                batch_adj_prop[layer_id][layer_fun] = para\n",
    "        else:            \n",
    "            print(name, para)\n",
    "    \n",
    "    # print(layer_prop)\n",
    "    # print(batch_adj_prop)\n",
    "\n",
    "    # First layer\n",
    "    first_layer_num = layer_prop[0][1]\n",
    "    \n",
    "    first_layer_input_q = int(math.log(layer_prop[0][0],2))\n",
    "    first_layer_addition_q = max(first_layer_input_q-2,0)        \n",
    "    first_layer_batch_q = 0    \n",
    "    if 0 in batch_adj_prop.keys():\n",
    "        first_layer_batch_q = 2\n",
    "    \n",
    "    first_layer_q = first_layer_num*(first_layer_input_q+first_layer_batch_q)\n",
    "    \n",
    "    # print(first_layer_q)\n",
    "    \n",
    "    # Second layer\n",
    "    second_layer_num = layer_prop[1][1]\n",
    "    second_layer_input_q = int(math.log(layer_prop[1][0],2))\n",
    "    second_layer_addition_q = max(first_layer_input_q-2,0)        \n",
    "    second_layer_batch_q = 0\n",
    "    if 1 in batch_adj_prop.keys():\n",
    "        second_layer_batch_q = 2\n",
    "\n",
    "    second_layer_q = second_layer_num*(second_layer_input_q+second_layer_batch_q)\n",
    "    # print(second_layer_q)\n",
    "    \n",
    "    return first_layer_q,second_layer_q,first_layer_input_q,layer_prop,batch_adj_prop,max(first_layer_addition_q,second_layer_addition_q)\n",
    "    \n",
    "# Main part\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    torch.set_printoptions(threshold=sys.maxsize)\n",
    "    imshow(torchvision.utils.make_grid(data))\n",
    "    print(data,target)\n",
    "    Q_InputMatrix = ToQuantumMatrix()(data)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% QF-Map\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "             ┌──────────┐                                       \n",
      "fLayer_0: |0>┤0         ├─────■─────────────────■───────────────\n",
      "             │          │     │                 │               \n",
      "fLayer_1: |0>┤1         ├─────■─────────────────■───────────────\n",
      "             │  unitary │     │                 │               \n",
      "fLayer_2: |0>┤2         ├─■───┼────■───────■────┼───────────────\n",
      "             │          │ │   │    │       │    │               \n",
      "fLayer_3: |0>┤3         ├─■───┼────┼───■───┼────┼───────────────\n",
      "             ├──────────┤     │    │   │   │    │               \n",
      "fLayer_4: |0>┤0         ├─────┼────┼───┼───┼────┼───────────────\n",
      "             │          │     │    │   │   │    │               \n",
      "fLayer_5: |0>┤1         ├─────┼────┼───┼───┼────┼────■───────■──\n",
      "             │  unitary │     │    │   │   │    │    │       │  \n",
      "fLayer_6: |0>┤2         ├─────┼────┼───┼───┼────┼────■───────■──\n",
      "             │          │     │    │   │   │    │    │       │  \n",
      "fLayer_7: |0>┤3         ├─────┼────┼───┼───┼────┼────┼───■───┼──\n",
      "             └──────────┘     │    │   │   │    │    │   │   │  \n",
      "sLayer_0: |0>─────────────────┼────┼───┼───┼────┼────┼───┼───┼──\n",
      "                              │    │   │   │    │    │   │   │  \n",
      "sLayer_1: |0>─────────────────┼────┼───┼───┼────┼────┼───┼───┼──\n",
      "                              │    │   │   │    │    │   │   │  \n",
      "sLayer_2: |0>─────────────────┼────┼───┼───┼────┼────┼───┼───┼──\n",
      "                              │    │   │   │    │    │   │   │  \n",
      "sLayer_3: |0>─────────────────┼────┼───┼───┼────┼────┼───┼───┼──\n",
      "                              │    │   │   │    │    │   │   │  \n",
      "sLayer_4: |0>─────────────────┼────┼───┼───┼────┼────┼───┼───┼──\n",
      "                              │    │   │   │    │    │   │   │  \n",
      "sLayer_5: |0>─────────────────┼────┼───┼───┼────┼────┼───┼───┼──\n",
      "                            ┌─┴─┐  │   │   │  ┌─┴─┐┌─┴─┐ │ ┌─┴─┐\n",
      "   aux_0: |0>───────────────┤ X ├──■───┼───■──┤ X ├┤ X ├─■─┤ X ├\n",
      "                            └───┘┌─┴─┐ │ ┌─┴─┐└───┘└───┘   └───┘\n",
      "   aux_1: |0>────────────────────┤ X ├─■─┤ X ├──────────────────\n",
      "                                 └───┘   └───┘                  \n",
      "tensor([ 0.0000e+00, -3.0879e-01, -7.5482e-02,  0.0000e+00,  0.0000e+00,\n",
      "        -4.1858e-01, -2.0586e-01, -2.7448e-02,  0.0000e+00, -3.8427e-01,\n",
      "        -6.9993e-01, -1.0979e-01,  0.0000e+00, -6.1758e-02, -1.6469e-01,\n",
      "        -6.8620e-03,  3.0879e-01,  9.0465e-01, -2.3308e-02,  0.0000e+00,\n",
      "         0.0000e+00, -1.2926e-01, -6.3568e-02, -8.4758e-03,  0.0000e+00,\n",
      "        -1.1866e-01, -2.1613e-01, -3.3903e-02,  0.0000e+00, -1.9070e-02,\n",
      "        -5.0855e-02, -2.1189e-03,  7.5482e-02, -2.3308e-02,  9.9430e-01,\n",
      "         0.0000e+00,  0.0000e+00, -3.1596e-02, -1.5539e-02, -2.0719e-03,\n",
      "         0.0000e+00, -2.9006e-02, -5.2832e-02, -8.2874e-03,  0.0000e+00,\n",
      "        -4.6617e-03, -1.2431e-02, -5.1796e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         4.1858e-01, -1.2926e-01, -3.1596e-02,  0.0000e+00,  0.0000e+00,\n",
      "         8.2479e-01, -8.6170e-02, -1.1489e-02,  0.0000e+00, -1.6085e-01,\n",
      "        -2.9298e-01, -4.5958e-02,  0.0000e+00, -2.5851e-02, -6.8936e-02,\n",
      "        -2.8723e-03,  2.0586e-01, -6.3568e-02, -1.5539e-02,  0.0000e+00,\n",
      "         0.0000e+00, -8.6170e-02,  9.5762e-01, -5.6505e-03,  0.0000e+00,\n",
      "        -7.9107e-02, -1.4409e-01, -2.2602e-02,  0.0000e+00, -1.2714e-02,\n",
      "        -3.3903e-02, -1.4126e-03,  2.7448e-02, -8.4758e-03, -2.0719e-03,\n",
      "         0.0000e+00,  0.0000e+00, -1.1489e-02, -5.6505e-03,  9.9925e-01,\n",
      "         0.0000e+00, -1.0548e-02, -1.9212e-02, -3.0136e-03,  0.0000e+00,\n",
      "        -1.6952e-03, -4.5204e-03, -1.8835e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.8427e-01,\n",
      "        -1.1866e-01, -2.9006e-02,  0.0000e+00,  0.0000e+00, -1.6085e-01,\n",
      "        -7.9107e-02, -1.0548e-02,  0.0000e+00,  8.5233e-01, -2.6896e-01,\n",
      "        -4.2191e-02,  0.0000e+00, -2.3732e-02, -6.3286e-02, -2.6369e-03,\n",
      "         6.9993e-01, -2.1613e-01, -5.2832e-02,  0.0000e+00,  0.0000e+00,\n",
      "        -2.9298e-01, -1.4409e-01, -1.9212e-02,  0.0000e+00, -2.6896e-01,\n",
      "         5.1010e-01, -7.6847e-02,  0.0000e+00, -4.3226e-02, -1.1527e-01,\n",
      "        -4.8029e-03,  1.0979e-01, -3.3903e-02, -8.2874e-03,  0.0000e+00,\n",
      "         0.0000e+00, -4.5958e-02, -2.2602e-02, -3.0136e-03,  0.0000e+00,\n",
      "        -4.2191e-02, -7.6847e-02,  9.8795e-01,  0.0000e+00, -6.7806e-03,\n",
      "        -1.8082e-02, -7.5340e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  6.1758e-02, -1.9070e-02,\n",
      "        -4.6617e-03,  0.0000e+00,  0.0000e+00, -2.5851e-02, -1.2714e-02,\n",
      "        -1.6952e-03,  0.0000e+00, -2.3732e-02, -4.3226e-02, -6.7806e-03,\n",
      "         0.0000e+00,  9.9619e-01, -1.0171e-02, -4.2379e-04,  1.6469e-01,\n",
      "        -5.0855e-02, -1.2431e-02,  0.0000e+00,  0.0000e+00, -6.8936e-02,\n",
      "        -3.3903e-02, -4.5204e-03,  0.0000e+00, -6.3286e-02, -1.1527e-01,\n",
      "        -1.8082e-02,  0.0000e+00, -1.0171e-02,  9.7288e-01, -1.1301e-03,\n",
      "         6.8620e-03, -2.1189e-03, -5.1796e-04,  0.0000e+00,  0.0000e+00,\n",
      "        -2.8723e-03, -1.4126e-03, -1.8835e-04,  0.0000e+00, -2.6369e-03,\n",
      "        -4.8029e-03, -7.5340e-04,  0.0000e+00, -4.2379e-04, -1.1301e-03,\n",
      "         9.9995e-01], dtype=torch.float64)\n",
      "====================\n",
      "tensor([ 0.0000e+00, -3.0879e-01, -7.5482e-02,  0.0000e+00,  0.0000e+00,\n",
      "        -4.1858e-01, -2.0586e-01, -2.7448e-02,  0.0000e+00, -3.8427e-01,\n",
      "        -6.9993e-01, -1.0979e-01,  0.0000e+00, -6.1758e-02, -1.6469e-01,\n",
      "        -6.8620e-03,  3.0879e-01,  9.0465e-01, -2.3308e-02,  0.0000e+00,\n",
      "         0.0000e+00, -1.2926e-01, -6.3568e-02, -8.4758e-03,  0.0000e+00,\n",
      "        -1.1866e-01, -2.1613e-01, -3.3903e-02,  0.0000e+00, -1.9070e-02,\n",
      "        -5.0855e-02, -2.1189e-03,  7.5482e-02, -2.3308e-02,  9.9430e-01,\n",
      "         0.0000e+00,  0.0000e+00, -3.1596e-02, -1.5539e-02, -2.0719e-03,\n",
      "         0.0000e+00, -2.9006e-02, -5.2832e-02, -8.2874e-03,  0.0000e+00,\n",
      "        -4.6617e-03, -1.2431e-02, -5.1796e-04,  2.7448e-02, -8.4758e-03,\n",
      "        -2.0719e-03,  0.0000e+00,  0.0000e+00, -1.1489e-02, -5.6505e-03,\n",
      "         9.9925e-01,  0.0000e+00, -1.0548e-02, -1.9212e-02, -3.0136e-03,\n",
      "         0.0000e+00, -1.6952e-03, -4.5204e-03, -1.8835e-04,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         4.1858e-01, -1.2926e-01, -3.1596e-02,  0.0000e+00,  0.0000e+00,\n",
      "         8.2479e-01, -8.6170e-02, -1.1489e-02,  0.0000e+00, -1.6085e-01,\n",
      "        -2.9298e-01, -4.5958e-02,  0.0000e+00, -2.5851e-02, -6.8936e-02,\n",
      "        -2.8723e-03,  2.0586e-01, -6.3568e-02, -1.5539e-02,  0.0000e+00,\n",
      "         0.0000e+00, -8.6170e-02,  9.5762e-01, -5.6505e-03,  0.0000e+00,\n",
      "        -7.9107e-02, -1.4409e-01, -2.2602e-02,  0.0000e+00, -1.2714e-02,\n",
      "        -3.3903e-02, -1.4126e-03,  3.8427e-01, -1.1866e-01, -2.9006e-02,\n",
      "         0.0000e+00,  0.0000e+00, -1.6085e-01, -7.9107e-02, -1.0548e-02,\n",
      "         0.0000e+00,  8.5233e-01, -2.6896e-01, -4.2191e-02,  0.0000e+00,\n",
      "        -2.3732e-02, -6.3286e-02, -2.6369e-03,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         6.9993e-01, -2.1613e-01, -5.2832e-02,  0.0000e+00,  0.0000e+00,\n",
      "        -2.9298e-01, -1.4409e-01, -1.9212e-02,  0.0000e+00, -2.6896e-01,\n",
      "         5.1010e-01, -7.6847e-02,  0.0000e+00, -4.3226e-02, -1.1527e-01,\n",
      "        -4.8029e-03,  1.0979e-01, -3.3903e-02, -8.2874e-03,  0.0000e+00,\n",
      "         0.0000e+00, -4.5958e-02, -2.2602e-02, -3.0136e-03,  0.0000e+00,\n",
      "        -4.2191e-02, -7.6847e-02,  9.8795e-01,  0.0000e+00, -6.7806e-03,\n",
      "        -1.8082e-02, -7.5340e-04,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  6.1758e-02, -1.9070e-02,\n",
      "        -4.6617e-03,  0.0000e+00,  0.0000e+00, -2.5851e-02, -1.2714e-02,\n",
      "        -1.6952e-03,  0.0000e+00, -2.3732e-02, -4.3226e-02, -6.7806e-03,\n",
      "         0.0000e+00,  9.9619e-01, -1.0171e-02, -4.2379e-04,  1.6469e-01,\n",
      "        -5.0855e-02, -1.2431e-02,  0.0000e+00,  0.0000e+00, -6.8936e-02,\n",
      "        -3.3903e-02, -4.5204e-03,  0.0000e+00, -6.3286e-02, -1.1527e-01,\n",
      "        -1.8082e-02,  0.0000e+00, -1.0171e-02,  9.7288e-01, -1.1301e-03,\n",
      "         6.8620e-03, -2.1189e-03, -5.1796e-04,  0.0000e+00,  0.0000e+00,\n",
      "        -2.8723e-03, -1.4126e-03, -1.8835e-04,  0.0000e+00, -2.6369e-03,\n",
      "        -4.8029e-03, -7.5340e-04,  0.0000e+00, -4.2379e-04, -1.1301e-03,\n",
      "         9.9995e-01], dtype=torch.float64)\n",
      "====================\n",
      "tensor([ 0.0000e+00, -3.0879e-01, -7.5482e-02,  0.0000e+00,  0.0000e+00,\n",
      "        -4.1858e-01, -2.0586e-01, -2.7448e-02,  0.0000e+00, -3.8427e-01,\n",
      "        -6.9993e-01, -1.0979e-01,  0.0000e+00, -6.1758e-02, -1.6469e-01,\n",
      "        -6.8620e-03,  3.0879e-01,  9.0465e-01, -2.3308e-02,  0.0000e+00,\n",
      "         0.0000e+00, -1.2926e-01, -6.3568e-02, -8.4758e-03,  0.0000e+00,\n",
      "        -1.1866e-01, -2.1613e-01, -3.3903e-02,  0.0000e+00, -1.9070e-02,\n",
      "        -5.0855e-02, -2.1189e-03,  7.5482e-02, -2.3308e-02,  9.9430e-01,\n",
      "         0.0000e+00,  0.0000e+00, -3.1596e-02, -1.5539e-02, -2.0719e-03,\n",
      "         0.0000e+00, -2.9006e-02, -5.2832e-02, -8.2874e-03,  0.0000e+00,\n",
      "        -4.6617e-03, -1.2431e-02, -5.1796e-04,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         4.1858e-01, -1.2926e-01, -3.1596e-02,  0.0000e+00,  0.0000e+00,\n",
      "         8.2479e-01, -8.6170e-02, -1.1489e-02,  0.0000e+00, -1.6085e-01,\n",
      "        -2.9298e-01, -4.5958e-02,  0.0000e+00, -2.5851e-02, -6.8936e-02,\n",
      "        -2.8723e-03,  2.0586e-01, -6.3568e-02, -1.5539e-02,  0.0000e+00,\n",
      "         0.0000e+00, -8.6170e-02,  9.5762e-01, -5.6505e-03,  0.0000e+00,\n",
      "        -7.9107e-02, -1.4409e-01, -2.2602e-02,  0.0000e+00, -1.2714e-02,\n",
      "        -3.3903e-02, -1.4126e-03,  3.8427e-01, -1.1866e-01, -2.9006e-02,\n",
      "         0.0000e+00,  0.0000e+00, -1.6085e-01, -7.9107e-02, -1.0548e-02,\n",
      "         0.0000e+00,  8.5233e-01, -2.6896e-01, -4.2191e-02,  0.0000e+00,\n",
      "        -2.3732e-02, -6.3286e-02, -2.6369e-03,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  1.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7448e-02,\n",
      "        -8.4758e-03, -2.0719e-03,  0.0000e+00,  0.0000e+00, -1.1489e-02,\n",
      "        -5.6505e-03,  9.9925e-01,  0.0000e+00, -1.0548e-02, -1.9212e-02,\n",
      "        -3.0136e-03,  0.0000e+00, -1.6952e-03, -4.5204e-03, -1.8835e-04,\n",
      "         6.9993e-01, -2.1613e-01, -5.2832e-02,  0.0000e+00,  0.0000e+00,\n",
      "        -2.9298e-01, -1.4409e-01, -1.9212e-02,  0.0000e+00, -2.6896e-01,\n",
      "         5.1010e-01, -7.6847e-02,  0.0000e+00, -4.3226e-02, -1.1527e-01,\n",
      "        -4.8029e-03,  6.8620e-03, -2.1189e-03, -5.1796e-04,  0.0000e+00,\n",
      "         0.0000e+00, -2.8723e-03, -1.4126e-03, -1.8835e-04,  0.0000e+00,\n",
      "        -2.6369e-03, -4.8029e-03, -7.5340e-04,  0.0000e+00, -4.2379e-04,\n",
      "        -1.1301e-03,  9.9995e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  6.1758e-02, -1.9070e-02,\n",
      "        -4.6617e-03,  0.0000e+00,  0.0000e+00, -2.5851e-02, -1.2714e-02,\n",
      "        -1.6952e-03,  0.0000e+00, -2.3732e-02, -4.3226e-02, -6.7806e-03,\n",
      "         0.0000e+00,  9.9619e-01, -1.0171e-02, -4.2379e-04,  1.6469e-01,\n",
      "        -5.0855e-02, -1.2431e-02,  0.0000e+00,  0.0000e+00, -6.8936e-02,\n",
      "        -3.3903e-02, -4.5204e-03,  0.0000e+00, -6.3286e-02, -1.1527e-01,\n",
      "        -1.8082e-02,  0.0000e+00, -1.0171e-02,  9.7288e-01, -1.1301e-03,\n",
      "         1.0979e-01, -3.3903e-02, -8.2874e-03,  0.0000e+00,  0.0000e+00,\n",
      "        -4.5958e-02, -2.2602e-02, -3.0136e-03,  0.0000e+00, -4.2191e-02,\n",
      "        -7.6847e-02,  9.8795e-01,  0.0000e+00, -6.7806e-03, -1.8082e-02,\n",
      "        -7.5340e-04], dtype=torch.float64)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import qiskit_library\n",
    "\n",
    "f,s,iq,nn_prop,bn_prop,aux = extract_model(model)\n",
    "\n",
    "f_qr = QuantumRegister(f,\"fLayer\")\n",
    "s_qr = QuantumRegister(s,\"sLayer\")\n",
    "aux_qr = QuantumRegister(aux,\"aux\")\n",
    "\n",
    "circ = QuantumCircuit(f_qr,s_qr,aux_qr)\n",
    "\n",
    "# circ.append(UnitaryGate(Q_InputMatrix, label=\"iswap\"), f_qr[iq:iq*2])\n",
    "\n",
    "quantum_gates,ret_index = qf_map_extract_from_weight(nn_prop[0][2][0])\n",
    "index = torch.LongTensor(ret_index)\n",
    "Input0 = copy.deepcopy(Q_InputMatrix)\n",
    "Input0 = Input0[index]\n",
    "circ.append(UnitaryGate(Input0, label=\"Input0\"), f_qr[0:iq])\n",
    "qbits = f_qr[0:iq]\n",
    "for gate in quantum_gates:\n",
    "    z_count = gate.count(\"1\")\n",
    "    z_pos = get_index_list(gate,\"1\")\n",
    "    if z_count==1:\n",
    "        circ.z(qbits[z_pos[0]])\n",
    "    elif z_count==2:\n",
    "        circ.cz(qbits[z_pos[0]],qbits[z_pos[1]])\n",
    "    elif z_count==3:\n",
    "        qiskit_library.ccz(circ,qbits[z_pos[0]],qbits[z_pos[1]],qbits[z_pos[2]],aux_qr[0])\n",
    "    elif z_count==4:\n",
    "        qiskit_library.cccz(circ,qbits[z_pos[0]],qbits[z_pos[1]],qbits[z_pos[2]],qbits[z_pos[3]],aux_qr[0],aux_qr[1])\n",
    "        \n",
    "\n",
    "quantum_gates,ret_index = qf_map_extract_from_weight(nn_prop[0][2][1])\n",
    "index = torch.LongTensor(ret_index)\n",
    "Input1 = copy.deepcopy(Q_InputMatrix)\n",
    "Input1 = Input1[index]\n",
    "circ.append(UnitaryGate(Input1, label=\"Input1\"), f_qr[iq:iq*2])\n",
    "qbits = f_qr[iq:iq*2]\n",
    "for gate in quantum_gates:\n",
    "    z_count = gate.count(\"1\")\n",
    "    z_pos = get_index_list(gate,\"1\")\n",
    "    if z_count==1:\n",
    "        circ.z(qbits[z_pos[0]])\n",
    "    elif z_count==2:\n",
    "        circ.cz(qbits[z_pos[0]],qbits[z_pos[1]])\n",
    "    elif z_count==3:\n",
    "        qiskit_library.ccz(circ,qbits[z_pos[0]],qbits[z_pos[1]],qbits[z_pos[2]],aux_qr[0])\n",
    "    elif z_count==4:\n",
    "        qiskit_library.cccz(circ,qbits[z_pos[0]],qbits[z_pos[1]],qbits[z_pos[2]],qbits[z_pos[3]],aux_qr[0],aux_qr[1])\n",
    "        \n",
    "\n",
    "\n",
    "print(circ)\n",
    "\n",
    "print(Q_InputMatrix.flatten())\n",
    "print(\"==\"*10)\n",
    "print(Input0.flatten())\n",
    "print(\"==\"*10)\n",
    "print(Input1.flatten())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8213722",
   "language": "python",
   "display_name": "PyCharm (qiskit_practice)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}