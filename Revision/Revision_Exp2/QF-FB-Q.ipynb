{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/home/weiwen/.local/lib/python3.6/site-packages/qiskit/providers/ibmq/credentials/configrc.py:130: UserWarning: Credentials already present. Set overwrite=True to overwrite.\n",
      "  warnings.warn('Credentials already present. '\n",
      "/home/weiwen/.local/lib/python3.6/site-packages/qiskit/providers/ibmq/ibmqfactory.py:181: UserWarning: Credentials are already in use. The existing account in the session will be replaced.\n",
      "  warnings.warn('Credentials are already in use. The existing '\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Demo 3 on MNIST. This script is for batch of data generation.\n",
      "\tStart at: 08/10/2020 22:48:49\n",
      "\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\n",
      "\tEnjoy and Good Luck!\n",
      "====================================================================================================\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import functools\n",
    "print = functools.partial(print, flush=True)\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "from qiskit_library import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "import qiskit as qk\n",
    "from qiskit import Aer\n",
    "from qiskit import execute\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from qiskit import IBMQ\n",
    "# IBMQ.delete_accounts()\n",
    "IBMQ.save_account('62d0e14364f490e45b5b5e0f6eebdbc083270ffffb660c7054219b15c7ce99ab4aa3b321309c0a9d0c3fc20086baece1376297dcdb67c7b715f9de1e4fa79efb')\n",
    "IBMQ.load_account()\n",
    "\n",
    "\n",
    "\n",
    "interest_num = [3,6]\n",
    "img_size = 4\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 2\n",
    "inference_batch_size = 1\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Demo 3 on MNIST. This script is for batch of data generation.\")\n",
    "print(\"\\tStart at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "print(\"\\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\")\n",
    "print(\"\\tEnjoy and Good Luck!\")\n",
    "print(\"=\"*100)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def modify_target(target):\n",
    "    for j in range(len(target)):\n",
    "        for idx in range(len(interest_num)):\n",
    "            if target[j] == interest_num[idx]:\n",
    "                target[j] = idx\n",
    "                break\n",
    "    \n",
    "    new_target = torch.zeros(target.shape[0],2)\n",
    "        \n",
    "    for i in range(target.shape[0]):        \n",
    "        if target[i].item() == 0:            \n",
    "            new_target[i] = torch.tensor([1,0]).clone()     \n",
    "        else:\n",
    "            new_target[i] = torch.tensor([0,1]).clone()\n",
    "               \n",
    "    return target,new_target\n",
    "\n",
    "def select_num(dataset,interest_num):\n",
    "    labels = dataset.targets #get labels\n",
    "    labels = labels.numpy()\n",
    "    idx = {}\n",
    "    for num in interest_num:\n",
    "        idx[num] = np.where(labels == num)\n",
    "        \n",
    "    fin_idx = idx[interest_num[0]]\n",
    "    for i in range(1,len(interest_num)):           \n",
    "        \n",
    "        fin_idx = (np.concatenate((fin_idx[0],idx[interest_num[i]][0])),)\n",
    "    \n",
    "    fin_idx = fin_idx[0]    \n",
    "    \n",
    "    dataset.targets = labels[fin_idx]\n",
    "    dataset.data = dataset.data[fin_idx]\n",
    "    \n",
    "    # print(dataset.targets.shape)\n",
    "    \n",
    "    dataset.targets,_ = modify_target(dataset.targets)\n",
    "    # print(dataset.targets.shape)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def qc_input_trans(dataset):\n",
    "    dataset.data = dataset.data\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class ToQuantumData(object):\n",
    "    def __call__(self, tensor):        \n",
    "        data = tensor                \n",
    "        input_vec = data.view(-1)\n",
    "        vec_len = input_vec.size()[0]\n",
    "        input_matrix = torch.zeros(vec_len,vec_len)\n",
    "        input_matrix[0] = input_vec\n",
    "        input_matrix = input_matrix.transpose(0,1)        \n",
    "        u,s,v = np.linalg.svd(input_matrix)    \n",
    "        output_matrix = torch.tensor(np.dot(u,v))            \n",
    "        output_data = output_matrix[:,0].view(1,img_size,img_size)    \n",
    "        return output_data\n",
    "    \n",
    "\n",
    "class ToQuantumMatrix(object):\n",
    "    def __call__(self, tensor):        \n",
    "        data = tensor                \n",
    "        input_vec = data.view(-1)\n",
    "        vec_len = input_vec.size()[0]\n",
    "        input_matrix = torch.zeros(vec_len,vec_len)\n",
    "        input_matrix[0] = input_vec\n",
    "        input_matrix = np.float64(input_matrix.transpose(0,1))        \n",
    "        u,s,v = np.linalg.svd(input_matrix)    \n",
    "        output_matrix = torch.tensor(np.dot(u,v))                        \n",
    "        return output_matrix\n",
    "\n",
    "\n",
    "class ToQuantumData_Batch(object):\n",
    "    def __call__(self, tensor):\n",
    "        data = tensor\n",
    "        input_vec = data.view(-1)\n",
    "        vec_len = input_vec.size()[0]\n",
    "        input_matrix = torch.zeros(vec_len,vec_len)\n",
    "        input_matrix[0] = input_vec\n",
    "        input_matrix = input_matrix.transpose(0,1)\n",
    "        u,s,v = np.linalg.svd(input_matrix)\n",
    "        output_matrix = torch.tensor(np.dot(u,v))\n",
    "        output_data = output_matrix[:,0].view(data.shape)\n",
    "        return output_data\n",
    "\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "                                transforms.ToTensor()])\n",
    "# transform = transforms.Compose([transforms.Resize((img_size,img_size)),\n",
    "#                                 transforms.ToTensor(),ToQuantumData()])\n",
    "# transform = transforms.Compose([transforms.Resize((img_size,img_size)),transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# choose the training and test datasets\n",
    "\n",
    "# Path to MNIST Dataset\n",
    "train_data = datasets.MNIST(root='../../pytorch/data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='../../pytorch/data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "train_data = select_num(train_data,interest_num)\n",
    "test_data =  select_num(test_data,interest_num)\n",
    "\n",
    "# train_data = qc_input_trans(train_data)\n",
    "\n",
    "# imshow(torchvision.utils.make_grid(train_data[0][0]))\n",
    "# \n",
    "# sys.exit(0)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "    num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    filename = os.path.join(save_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        bestname = os.path.join(save_path, 'model_best.tar')\n",
    "        shutil.copyfile(filename, bestname)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "torch.Size([1, 1, 4, 4])\n",
      "tensor([[[[0.0000, 0.0157, 0.1373, 0.0157],\n",
      "          [0.0000, 0.1255, 0.1725, 0.0039],\n",
      "          [0.0000, 0.2745, 0.2941, 0.0078],\n",
      "          [0.0000, 0.1020, 0.0471, 0.0000]]]]) tensor([1])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# functions to show an image\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))    \n",
    "    plt.show()\n",
    "    \n",
    "    image = np.asarray(npimg[0] * 255, np.uint8)    \n",
    "    \n",
    "    im = Image.fromarray(image,mode=\"L\")\n",
    "    im.save(\"32*32.jpg\",cmap=\"gray\") \n",
    "    im = im.resize((4,4),Image.BILINEAR)    \n",
    "    \n",
    "    plt.imshow(im,cmap='gray',)\n",
    "    \n",
    "    trans_to_tensor = transforms.ToTensor()\n",
    "    trans_to_matrix = ToQuantumMatrix()\n",
    "    plt.show()\n",
    "    im.save(\"4*4.jpg\",cmap=\"gray\") \n",
    "    \n",
    "    # print(trans_to_tensor(im))\n",
    "    for row in trans_to_matrix(trans_to_tensor(im)).tolist():\n",
    "        for num in row:\n",
    "            print(num,end=\",\")\n",
    "        print()\n",
    "\n",
    "    \n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    torch.set_printoptions(threshold=sys.maxsize)\n",
    "    # imshow(torchvision.utils.make_grid(data))\n",
    "    print(data.shape)\n",
    "    print(data,target)\n",
    "    inputs = data.flatten()\n",
    "    \n",
    "    \n",
    "    trans_to_tensor = transforms.ToTensor()\n",
    "    trans_to_matrix = ToQuantumMatrix()\n",
    "    \n",
    "    \n",
    "    # for row in trans_to_matrix((inputs)).tolist():\n",
    "    #     for num in row:\n",
    "    #         print(num,end=\",\")\n",
    "    #     print()\n",
    "    # \n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# functions to show an image\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))    \n",
    "    plt.show()\n",
    "    \n",
    "    # image = np.asarray(npimg[0] * 255, np.uint8)    \n",
    "    # \n",
    "    # im = Image.fromarray(image,mode=\"L\")\n",
    "    # im.save(\"32*32.jpg\",cmap=\"gray\") \n",
    "    # im = im.resize((4,4),Image.BILINEAR)    \n",
    "    # \n",
    "    # plt.imshow(im,cmap='gray',)\n",
    "    # \n",
    "    # trans_to_tensor = transforms.ToTensor()\n",
    "    # trans_to_matrix = ToQuantumMatrix()\n",
    "    # plt.show()\n",
    "    # im.save(\"4*4.jpg\",cmap=\"gray\") \n",
    "    \n",
    "    # print(trans_to_tensor(im))\n",
    "    # for row in trans_to_matrix(trans_to_tensor(im)).tolist():\n",
    "    #     for num in row:\n",
    "    #         print(num,end=\",\")\n",
    "    #     print()\n",
    "\n",
    "#     \n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#     torch.set_printoptions(threshold=sys.maxsize)\n",
    "#     imshow(torchvision.utils.make_grid(data))\n",
    "#     break\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "=> loading checkpoint from 'checkpoint_13_0.9746.pth.tar'<=\n",
      "fc0.weight tensor([[-1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,\n",
      "          1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,\n",
      "          1.,  1.],\n",
      "        [ 1., -1., -1., -1., -1., -1.,  1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         -1.,  1.],\n",
      "        [-1., -1., -1., -1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.,\n",
      "          1.,  1.]], grad_fn=<BinarizeFBackward>)\n",
      "fc1.weight tensor([[-1.,  1., -1., -1.],\n",
      "        [ 1.,  1., -1.,  1.]], grad_fn=<BinarizeFBackward>)\n",
      "qca1.x_running_rot Parameter containing:\n",
      "tensor([0.2991, 0.3488])\n",
      "qca1.x_l_0_5 Parameter containing:\n",
      "tensor([1., 1.])\n",
      "qca1.x_g_0_5 Parameter containing:\n",
      "tensor([0., 0.])\n",
      "\t tensor([[0.0034, 0.1940, 0.1294, 0.0000, 0.0579, 0.4017, 0.1872, 0.0306, 0.0647,\n",
      "         0.5208, 0.6400, 0.1430, 0.0068, 0.1294, 0.1498, 0.0170]])\n",
      "\t tensor([[0.2820, 0.1669, 0.2470, 0.1739]], grad_fn=<PowBackward0>)\n",
      "\t tensor([[0.2820, 0.1669, 0.2470, 0.1739]], grad_fn=<PowBackward0>)\n",
      "\t tensor([[0.2216, 0.2652]], grad_fn=<DivBackward0>)\n",
      "\t tensor([[0.4544, 0.5215]], grad_fn=<AddBackward0>)\n",
      "tensor([[0.4544, 0.5215]], grad_fn=<AddBackward0>)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from lib_qc import * \n",
    "from lib_util import *\n",
    "from lib_net import *\n",
    "\n",
    "# Network Architecture: 2 layers and each layer contains 2 neurons\n",
    "\n",
    "img_size = 4\n",
    "device = torch.device(\"cpu\")\n",
    "layers = [4, 2]\n",
    "\n",
    "model = Net(img_size,layers,True,[[1,1,1,1],[1,1]],True,False,False,False,True).to(device)\n",
    "\n",
    "resume_path=\"mnist36_0.9746.pth.tar\"\n",
    "print(\"=> loading checkpoint from '{}'<=\".format(resume_path))\n",
    "checkpoint = torch.load(resume_path, map_location=device)\n",
    "epoch_init, acc = checkpoint[\"epoch\"], checkpoint[\"acc\"]\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "\n",
    "for name, para in model.named_parameters():\n",
    "    if \"fc\" in name:\n",
    "        print(name,binarize(para))\n",
    "    else:\n",
    "        print(name, para)\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    torch.set_printoptions(threshold=sys.maxsize)\n",
    "    # imshow(torchvision.utils.make_grid(data))\n",
    "    # print(data,target)\n",
    "    Q_InputMatrix = ToQuantumMatrix()(data.flatten())\n",
    "    break\n",
    "\n",
    "\n",
    "model.eval()\n",
    "quantum_data = ToQuantumData_Batch()(data)\n",
    "output = model(quantum_data, False)\n",
    "print(output)\n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#     torch.set_printoptions(threshold=sys.maxsize)\n",
    "#     # imshow(torchvision.utils.make_grid(data))\n",
    "#     # print(data)\n",
    "#     \n",
    "#     Q_InputMatrix = ToQuantumMatrix()(data.flatten())\n",
    "#     break\n",
    "# \n",
    "# print(\"=\"*10)\n",
    "# print(Q_InputMatrix.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Visualization of Model\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from qiskit import QuantumRegister, QuantumCircuit, ClassicalRegister\n",
    "from qiskit.extensions import XGate, UnitaryGate\n",
    "import qiskit\n",
    "from math import sqrt \n",
    "import math\n",
    "import copy\n",
    "\n",
    "def get_index_list(input,target):\n",
    "    index_list = []\n",
    "    try:\n",
    "        beg_pos = 0\n",
    "        while True:\n",
    "            find_pos = input.index(target,beg_pos)\n",
    "            index_list.append(find_pos)\n",
    "            beg_pos = find_pos+1\n",
    "    except Exception as exception:        \n",
    "        pass    \n",
    "    return index_list\n",
    "\n",
    "def change_sign(sign,bin):\n",
    "    affect_num = [bin]\n",
    "    one_positions = []\n",
    "    \n",
    "    try:\n",
    "        beg_pos = 0\n",
    "        while True:\n",
    "            find_pos = bin.index(\"1\",beg_pos)\n",
    "            one_positions.append(find_pos)\n",
    "            beg_pos = find_pos+1\n",
    "    except Exception as exception:\n",
    "        # print(\"Not Found\")\n",
    "        pass\n",
    "    \n",
    "    for k,v in sign.items():\n",
    "        change = True\n",
    "        for pos in one_positions:\n",
    "            if k[pos]==\"0\":                \n",
    "                change = False\n",
    "                break\n",
    "        if change:\n",
    "            sign[k] = -1*v\n",
    "    \n",
    "\n",
    "def find_start(affect_count_table,target_num):\n",
    "    for k in list(affect_count_table.keys())[::-1]:\n",
    "        if target_num<=k:\n",
    "            return k\n",
    "\n",
    "\n",
    "def recursive_change(direction,start_point,target_num,sign,affect_count_table,quantum_gates):\n",
    "    \n",
    "    if start_point == target_num:\n",
    "        # print(\"recursive_change: STOP\")\n",
    "        return\n",
    "    \n",
    "    gap = int(math.fabs(start_point-target_num))    \n",
    "    step = find_start(affect_count_table,gap)\n",
    "    change_sign(sign,affect_count_table[step])\n",
    "    quantum_gates.append(affect_count_table[step])\n",
    "    \n",
    "    if direction==\"r\": \n",
    "        # print(\"recursive_change: From\",start_point,\"Right(-):\",step)\n",
    "        start_point = start_point - step\n",
    "        direction = \"l\"\n",
    "        recursive_change(direction,start_point,target_num,sign,affect_count_table,quantum_gates)\n",
    "        \n",
    "    else:        \n",
    "        # print(\"recursive_change: From\",start_point,\"Left(+):\",step)\n",
    "        start_point = start_point + step\n",
    "        direction = \"r\"\n",
    "        recursive_change(direction,start_point,target_num,sign,affect_count_table,quantum_gates)\n",
    "        \n",
    "    \n",
    "\n",
    "def guarntee_upper_bound_algorithm(sign,target_num,total_len,digits):        \n",
    "    flag = \"0\"+str(digits)+\"b\"\n",
    "    pre_num = 0\n",
    "    affect_count_table = {}\n",
    "    quantum_gates = []\n",
    "    for i in range(digits):\n",
    "        cur_num = pre_num + pow(2,i)\n",
    "        pre_num = cur_num\n",
    "        binstr_cur_num = format(cur_num,flag) \n",
    "        affect_count_table[int(pow(2,binstr_cur_num.count(\"0\")))] = binstr_cur_num   \n",
    "        \n",
    "    if target_num in affect_count_table.keys():\n",
    "        quantum_gates.append(affect_count_table[target_num])\n",
    "        change_sign(sign,affect_count_table[target_num])    \n",
    "    else:\n",
    "        direction = \"r\"\n",
    "        start_point = find_start(affect_count_table,target_num)\n",
    "        quantum_gates.append(affect_count_table[start_point])\n",
    "        change_sign(sign,affect_count_table[start_point])\n",
    "        recursive_change(direction,start_point,target_num,sign,affect_count_table,quantum_gates)\n",
    "    \n",
    "    return quantum_gates\n",
    "\n",
    "def qf_map_extract_from_weight(weights):    \n",
    "    # Find Z control gates according to weights\n",
    "    w = (weights.detach().cpu().numpy())\n",
    "    total_len = len(w)\n",
    "    target_num = np.count_nonzero(w == -1)\n",
    "    if target_num > total_len/2:\n",
    "        w = w*-1\n",
    "    target_num = np.count_nonzero(w == -1)    \n",
    "    digits = int(math.log(total_len,2))\n",
    "    flag = \"0\"+str(digits)+\"b\"\n",
    "    max_num = int(math.pow(2,digits))\n",
    "    sign = {}\n",
    "    for i in range(max_num):        \n",
    "        sign[format(i,flag)] = +1\n",
    "    quantum_gates = guarntee_upper_bound_algorithm(sign,target_num,total_len,digits)\n",
    "    \n",
    "    # for k,v in sign.items():\n",
    "    #     print(k,v)\n",
    "    # print(w)\n",
    "        \n",
    "    # Build the mapping from weight to final negative num \n",
    "    fin_sign = list(sign.values())\n",
    "    fin_weig = [int(x) for x in list(w)]\n",
    "    \n",
    "    # print(fin_sign)\n",
    "    # print(fin_weig)\n",
    "    sign_neg_index = []    \n",
    "    try:\n",
    "        beg_pos = 0\n",
    "        while True:\n",
    "            find_pos = fin_sign.index(-1,beg_pos)            \n",
    "            # qiskit_position = int(format(find_pos,flag)[::-1],2)                            \n",
    "            sign_neg_index.append(find_pos)\n",
    "            beg_pos = find_pos+1\n",
    "    except Exception as exception:        \n",
    "        pass    \n",
    "    \n",
    "\n",
    "    weight_neg_index = []\n",
    "    try:\n",
    "        beg_pos = 0\n",
    "        while True:\n",
    "            find_pos = fin_weig.index(-1,beg_pos)\n",
    "            weight_neg_index.append(find_pos)\n",
    "            beg_pos = find_pos+1\n",
    "    except Exception as exception:        \n",
    "        pass    \n",
    "    map = {}\n",
    "    for i in range(len(sign_neg_index)):\n",
    "        map[sign_neg_index[i]] = weight_neg_index[i]\n",
    "            \n",
    "    # print(map)\n",
    "    \n",
    "    ret_index = list(range(len(fin_weig)))\n",
    "    \n",
    "    for k,v in map.items():\n",
    "        old_val = ret_index[k] \n",
    "        ret_index[k] = v\n",
    "        ret_index[v] = old_val\n",
    "    \n",
    "\n",
    "    return quantum_gates,ret_index\n",
    "    \n",
    "    \n",
    "def extract_model(model):\n",
    "    layer_prop = {}\n",
    "    batch_adj_prop = {}\n",
    "    indiv_adj_prop = {}\n",
    "    for name, para in model.named_parameters():\n",
    "        if \"fc\" in name:\n",
    "            layer_id = int(name.split(\".\")[0].split(\"c\")[1])\n",
    "            layer_prop[layer_id] = [para.shape[1],para.shape[0],binarize(para)]            \n",
    "        elif \"qca\" in name:            \n",
    "            if \"l_0_5\" in name or \"running_rot\" in name:\n",
    "                layer_id = int(name.split(\".\")[0].split(\"a\")[1])\n",
    "                layer_fun = name.split(\".\")[1]\n",
    "                if layer_id not in batch_adj_prop.keys():\n",
    "                    batch_adj_prop[layer_id] = {}\n",
    "                batch_adj_prop[layer_id][layer_fun] = para\n",
    "        else:            \n",
    "            print(name, para)\n",
    "    \n",
    "    # print(layer_prop)\n",
    "    # print(batch_adj_prop)\n",
    "\n",
    "    # First layer\n",
    "    first_layer_num = layer_prop[0][1]\n",
    "    \n",
    "    first_layer_input_q = int(math.log(layer_prop[0][0],2))\n",
    "    first_layer_addition_q = max(first_layer_input_q-2,0)        \n",
    "    first_layer_batch_q = 0    \n",
    "    if 0 in batch_adj_prop.keys():\n",
    "        first_layer_batch_q = 2\n",
    "    \n",
    "    first_layer_q = first_layer_num*(first_layer_input_q+first_layer_batch_q)\n",
    "    \n",
    "    # print(first_layer_q)\n",
    "    \n",
    "    # Second layer    \n",
    "    second_layer_num = layer_prop[1][1]\n",
    "    if layer_prop[1][0]==0:\n",
    "        second_layer_input_q=int(math.log(layer_prop[1][0],2))\n",
    "        second_layer_encode_q = 0\n",
    "        second_layer_output_q = 0\n",
    "    else:\n",
    "        second_layer_input_q = layer_prop[1][0]\n",
    "        second_layer_encode_q = int(math.log(second_layer_input_q,2))    \n",
    "        second_layer_output_q = int(math.log(second_layer_encode_q,2))\n",
    "    \n",
    "    second_layer_addition_q = max(first_layer_input_q-2,0)        \n",
    "    \n",
    "    second_layer_batch_q = 0\n",
    "    if 1 in batch_adj_prop.keys():\n",
    "        second_layer_batch_q = 2\n",
    "\n",
    "    second_layer_q = second_layer_num*(second_layer_input_q+second_layer_output_q+second_layer_batch_q)\n",
    "    # print(second_layer_q)\n",
    "    sec_list = [second_layer_input_q, second_layer_num*second_layer_output_q,\n",
    "                second_layer_num*second_layer_encode_q, second_layer_num*second_layer_batch_q]\n",
    "    return first_layer_q,sec_list,first_layer_input_q,layer_prop,batch_adj_prop,max(first_layer_addition_q,second_layer_addition_q)\n",
    "    \n",
    "# Main part\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def q_map_neural_compute_body(circ,inputs,iq,inference_batch_size,log_batch_size,weights):\n",
    "    \n",
    "    quantum_gates,ret_index = qf_map_extract_from_weight(weights)\n",
    "    # print(ret_index)\n",
    "    # print(quantum_gates)\n",
    "    # \n",
    "    expand_for_batch_index = copy.deepcopy(ret_index)\n",
    "    for b in range(inference_batch_size-1):\n",
    "        start = len(ret_index)*(b+1)\n",
    "        new_batch_index = [x+start for x in ret_index]\n",
    "        expand_for_batch_index += new_batch_index\n",
    "    index = torch.LongTensor(expand_for_batch_index)    \n",
    "    Input0 = copy.deepcopy(Q_InputMatrix)\n",
    "    Input0 = Input0[index]\n",
    "    # print(\"for debug comparison\")\n",
    "    # print(Q_InputMatrix[:,0])\n",
    "    \n",
    "    \n",
    "    # print(Input0[:,0])\n",
    "    circ.append(UnitaryGate(Input0, label=\"Input0\"), inputs[0:iq])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # print(circ)\n",
    "    # backend = Aer.get_backend('unitary_simulator')\n",
    "    # job = execute(circ, backend)\n",
    "    # result = job.result()\n",
    "    # torch.set_printoptions(threshold=sys.maxsize)\n",
    "    # np.set_printoptions(threshold=sys.maxsize)\n",
    "    # state = result.get_unitary(circ, decimals=4)\n",
    "    # print(state[:,0])\n",
    "    # \n",
    "    \n",
    "    qbits = inputs[0:iq-log_batch_size]\n",
    "    for gate in quantum_gates:\n",
    "        z_count = gate.count(\"1\")\n",
    "        # z_pos = get_index_list(gate,\"1\")\n",
    "        z_pos = get_index_list(gate[::-1],\"1\")\n",
    "        # print(z_pos)\n",
    "        if z_count==1:\n",
    "            circ.z(qbits[z_pos[0]])\n",
    "        elif z_count==2:\n",
    "            circ.cz(qbits[z_pos[0]],qbits[z_pos[1]])\n",
    "        elif z_count==3:\n",
    "            qiskit_library.ccz(circ,qbits[z_pos[0]],qbits[z_pos[1]],qbits[z_pos[2]],aux_qr[0])\n",
    "        elif z_count==4:\n",
    "            qiskit_library.cccz(circ,qbits[z_pos[0]],qbits[z_pos[1]],qbits[z_pos[2]],qbits[z_pos[3]],aux_qr[0],aux_qr[1])\n",
    "    \n",
    "    # \n",
    "    # \n",
    "    # print(circ)\n",
    "    # backend = Aer.get_backend('unitary_simulator')\n",
    "    # job = execute(circ, backend)\n",
    "    # result = job.result()\n",
    "    # \n",
    "    # torch.set_printoptions(threshold=sys.maxsize)\n",
    "    # np.set_printoptions(threshold=sys.maxsize)\n",
    "    # state = result.get_unitary(circ, decimals=4)\n",
    "    # print(state[:,0])\n",
    "    # sys.exit(0)\n",
    "\n",
    "\n",
    "def q_map_neural_compute_extract(circ,inputs,iq,outputs,log_batch_size):        \n",
    "    qbits = inputs[0:iq-log_batch_size]\n",
    "    # qbits = inputs[log_batch_size:iq]\n",
    "    for q in qbits:\n",
    "        circ.h(q)\n",
    "    \n",
    "    circ.barrier()\n",
    "    \n",
    "    for q in qbits:\n",
    "        circ.x(q)\n",
    "    \n",
    "    \n",
    "    digits = log_batch_size\n",
    "    flag = \"0\"+str(digits)+\"b\"\n",
    "    # qbits = inputs[0:log_batch_size]\n",
    "    qbits = inputs[iq-log_batch_size:iq]\n",
    "    if log_batch_size!=0:\n",
    "        for i in range(int(math.pow(2,log_batch_size))):        \n",
    "            binstr_cur_num = format(i,flag)[::-1]\n",
    "            \n",
    "            for pos in range(len(binstr_cur_num)):            \n",
    "                if binstr_cur_num[pos]==\"0\":                \n",
    "                    circ.x(qbits[pos])\n",
    "                    \n",
    "            if digits==1:                            \n",
    "                qiskit_library.cccccx(circ, inputs[0:iq], outputs[i],aux_qr)            \n",
    "            elif digits==2:\n",
    "                qiskit_library.ccccccx(circ, inputs[0:iq], outputs[i],aux_qr)\n",
    "            elif digits==3:\n",
    "                qiskit_library.cccccccx(circ, inputs[0:iq], outputs[i],aux_qr)\n",
    "            \n",
    "            \n",
    "            for pos in range(len(binstr_cur_num)):                \n",
    "                if binstr_cur_num[pos]==\"0\":                \n",
    "                    circ.x(qbits[pos])\n",
    "            circ.barrier()\n",
    "            \n",
    "    else:\n",
    "        qiskit_library.ccccx(circ, inputs[0], inputs[1], inputs[2], inputs[3], outputs[0], aux_qr[0], aux_qr[1])\n",
    "    \n",
    "    circ.barrier()\n",
    "    qbits = inputs[log_batch_size:iq]\n",
    "    for q in qbits:    \n",
    "        circ.x(q)\n",
    "    \n",
    "\n",
    "def q_map_q_ori_net(circ, s_qr_in, s_qr_enc, aux_qr, weights):\n",
    "    if len(weights)==4:\n",
    "        for i in range(len(weights)):\n",
    "            if weights[i]==-1:\n",
    "                circ.x(s_qr_in[i])\n",
    "        for i in range(2):\n",
    "            circ.h(s_qr_enc[i])\n",
    "        circ.barrier()\n",
    "        circ.x(s_qr_enc[0])\n",
    "        circ.x(s_qr_enc[1])\n",
    "        ccz(circ,s_qr_in[3],s_qr_enc[0],s_qr_enc[1],aux_qr[0])\n",
    "        circ.x(s_qr_enc[0])\n",
    "        circ.x(s_qr_enc[1])\n",
    "                \n",
    "        circ.x(s_qr_enc[1])\n",
    "        ccz(circ,s_qr_in[2],s_qr_enc[0],s_qr_enc[1],aux_qr[0])\n",
    "        circ.x(s_qr_enc[1])\n",
    "        \n",
    "        circ.x(s_qr_enc[0])\n",
    "        ccz(circ,s_qr_in[1],s_qr_enc[0],s_qr_enc[1],aux_qr[0])\n",
    "        circ.x(s_qr_enc[0])\n",
    "        \n",
    "        \n",
    "        ccz(circ,s_qr_in[0],s_qr_enc[0],s_qr_enc[1],aux_qr[0])\n",
    "        \n",
    "        circ.barrier()\n",
    "        for i in range(len(weights)):\n",
    "            if weights[i]==-1:\n",
    "                circ.x(s_qr_in[i])\n",
    "                \n",
    "    elif len(weights)==8:\n",
    "        print(\"Size\", len(weights),\"in 2nd layer is now not supportted\")\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        print(\"Size\", len(weights),\"in 2nd layer is now not supportted\")\n",
    "        sys.exit(0)\n",
    "\n",
    "\n",
    "def q_map_bn(circ, s_qr_enc, s_qr_bat, output, aux_qr, enc_len, type, val):\n",
    "    if enc_len!=2:\n",
    "        print(\"Encoder size of \", enc_len,\"is now not supportted\")\n",
    "        sys.exit(0)\n",
    "    else:        \n",
    "        for i in range(enc_len):\n",
    "            circ.h(s_qr_enc[i])\n",
    "            circ.x(s_qr_enc[i])\n",
    "        circ.barrier()\n",
    "        circ.ccx(s_qr_enc[0],s_qr_enc[1],s_qr_bat[0])        \n",
    "        qc_ang = 2*torch.tensor(math.sqrt(val)).asin().item()        \n",
    "        circ.ry(qc_ang,s_qr_bat[1])\n",
    "        if type==1:\n",
    "            circ.ccx(s_qr_bat[0],s_qr_bat[1],output)\n",
    "            \n",
    "            \n",
    "        \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% QF-Map\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import qiskit_library\n",
    "f,sec_list,iq,nn_prop,bn_prop,aux = extract_model(model)\n",
    "log_batch_size = int(math.log(inference_batch_size,2))\n",
    "\n",
    "f = f+(log_batch_size)*nn_prop[0][1]\n",
    "iq = iq+log_batch_size\n",
    "aux = aux+inference_batch_size-1\n",
    "# s = (inference_batch_size)*s\n",
    "# s = inference_batch_size*2\n",
    "\n",
    "if sec_list[0]!=0:\n",
    "    s_qr_in = QuantumRegister(sec_list[0]*inference_batch_size,\"s_in\")\n",
    "if sec_list[1]!=0:\n",
    "    s_qr_out = QuantumRegister(sec_list[1]*inference_batch_size,\"s_out\")\n",
    "if sec_list[2]!=0:\n",
    "    s_qr_enc = QuantumRegister(sec_list[2]*inference_batch_size,\"s_encoder\")\n",
    "if sec_list[3]!=0:\n",
    "    s_qr_bat = QuantumRegister(sec_list[3]*inference_batch_size,\"s_BN\")\n",
    "# f=5\n",
    "# aux=1\n",
    "\n",
    "if sec_list[1]==0:\n",
    "    print(\"Did not implement!\")\n",
    "    sys.exit(0)\n",
    "\n",
    "f_qr = QuantumRegister(f,\"fLayer\")\n",
    "# s_qr = QuantumRegister(s,\"sLayer\")\n",
    "aux_qr = QuantumRegister(aux,\"aux\")\n",
    "c = ClassicalRegister(sec_list[1]*inference_batch_size,\"reg\")\n",
    "\n",
    "circ = QuantumCircuit(f_qr,s_qr_in,s_qr_enc,s_qr_bat,s_qr_out,aux_qr,c)\n",
    "# circ = QuantumCircuit(f_qr,aux_qr)\n",
    "# Build Inputs and Computation\n",
    "for i in range(nn_prop[0][1]):\n",
    "    q_map_neural_compute_body(circ,f_qr[iq*i:iq*(i+1)],iq,inference_batch_size,log_batch_size,nn_prop[0][2][i])\n",
    "    circ.barrier()\n",
    "    # q_map_neural_compute_body(circ,f_qr[iq:iq*2],iq,inference_batch_size,log_batch_size)\n",
    "    # circ.barrier()\n",
    "\n",
    "# Build the extraction of results\n",
    "for i in range(nn_prop[0][1]):\n",
    "    q_map_neural_compute_extract(circ,f_qr[iq*i:iq*(i+1)],iq,s_qr_in[inference_batch_size*i:inference_batch_size*(i+1)],log_batch_size)\n",
    "    circ.barrier()\n",
    "\n",
    "# Build the computation of original neural comp\n",
    "enc_len = int(math.log(nn_prop[1][0],2))\n",
    "for i in range(nn_prop[1][1]):\n",
    "    q_map_q_ori_net(circ, s_qr_in, s_qr_enc[enc_len*i:enc_len*(i+1)], aux_qr,nn_prop[1][2][i])\n",
    "    circ.barrier()\n",
    "circ.barrier()\n",
    "\n",
    "batchnorm_len = 2\n",
    "for i in range(nn_prop[1][1]):    \n",
    "    q_map_bn(circ, s_qr_enc[enc_len*i:enc_len*(i+1)], s_qr_bat[batchnorm_len*i:batchnorm_len*(i+1)], s_qr_out[i], aux_qr, enc_len, 1, bn_prop[1]['x_running_rot'][i])\n",
    "    \n",
    "    circ.barrier()\n",
    "circ.barrier()\n",
    "\n",
    "for i in range(sec_list[1]):\n",
    "    circ.measure(s_qr_out[i],c[i])\n",
    "    \n",
    "# print(circ)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "num_c_reg=2\n",
    "\n",
    "def analyze(counts):\n",
    "    mycount = {}\n",
    "    for i in range(num_c_reg):\n",
    "        mycount[i] = 0\n",
    "    for k,v in counts.items():\n",
    "        bits = len(k) \n",
    "        for i in range(bits):            \n",
    "            if k[bits-1-i] == \"1\":\n",
    "                if i in mycount.keys():\n",
    "                    mycount[i] += v\n",
    "                else:\n",
    "                    mycount[i] = v\n",
    "    return mycount,bits\n",
    "\n",
    "def fire_ibmq(circuit,shots,iter,Simulation = False, printable=True,backend_name='ibmq_essex'):\n",
    "    if printable:\n",
    "        print(circuit)\n",
    "    \n",
    "    count_set = []\n",
    "    start = time.time()\n",
    "    for it in range(iter):\n",
    "        if not Simulation:\n",
    "            provider = IBMQ.get_provider('ibm-q-academic')\n",
    "            # ibm-q-academic backends: \n",
    "            #  5 qubits: ibmq_valencia\n",
    "            # 20 qubits: ibmq_poughkeepsie, ibmq_johannesburg,ibmq_boeblingen, ibmq_20_tokyo\n",
    "            # 53 qubits: ibmq_rochester\n",
    "            \n",
    "            # To get a specific qubit backend: \n",
    "            backend = provider.get_backend(backend_name)\n",
    "        else:\n",
    "            backend = Aer.get_backend('qasm_simulator')\n",
    "        job_ibm_q = execute(circuit, backend, shots=shots)\n",
    "        job_monitor(job_ibm_q)\n",
    "        result_ibm_q = job_ibm_q.result()\n",
    "        counts = result_ibm_q.get_counts()\n",
    "        count_set.append(counts)\n",
    "    end = time.time()\n",
    "    # print(\"Simulation time:\", end - start)\n",
    "\n",
    "    return count_set\n",
    "\n",
    "# \n",
    "# print(\"=\"*50)\n",
    "# print(\"Start simulation:\")\n",
    "# start = time.time()        \n",
    "# iters = 1\n",
    "# qc_shots=81920\n",
    "# counts = fire_ibmq(circ,qc_shots,iters,True,False)\n",
    "# end = time.time()\n",
    "# qc_time = end - start\n",
    "# \n",
    "# (mycount,bits) = analyze(counts[0])\n",
    "# for b in range(bits):\n",
    "#     print (b,float(mycount[b])/qc_shots)\n",
    "#     \n",
    "# print(\"From QC:\",counts)\n",
    "# print(\"Simulation elasped time:\",qc_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Job Status: job has successfully run\n",
      "Job Status: job has successfully run\n",
      "Job Status: job has successfully run\n",
      "Job Status: job has successfully run\n",
      "[0.2822265625, 0.1697998046875, 0.250244140625, 0.1705322265625]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "f_qr = QuantumRegister(4,\"fLayer\")\n",
    "aux_qr = QuantumRegister(2,\"aux\")\n",
    "s_qr_in = QuantumRegister(1,\"s_in\")\n",
    "c = ClassicalRegister(1,\"reg\")\n",
    "\n",
    "FLayer_Res=[]\n",
    "for i in range(nn_prop[0][1]):\n",
    "    circ = QuantumCircuit(f_qr,s_qr_in,aux_qr,c)\n",
    "    q_map_neural_compute_body(circ,f_qr[0:iq],iq,inference_batch_size,log_batch_size,nn_prop[0][2][i])\n",
    "    circ.barrier()\n",
    "    q_map_neural_compute_extract(circ,f_qr[0:iq],iq,s_qr_in[0:inference_batch_size],log_batch_size)\n",
    "    circ.barrier()    \n",
    "    circ.measure(s_qr_in,c)\n",
    "    \n",
    "    iters = 1\n",
    "    qc_shots=8192\n",
    "    counts = fire_ibmq(circ,qc_shots,iters,True,False)    \n",
    "    (mycount,bits) = analyze(counts[0])\n",
    "    for b in range(bits):        \n",
    "        FLayer_Res.append(float(mycount[b])/qc_shots)\n",
    "print(FLayer_Res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Job Status: job has successfully run\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "s_qr_in = QuantumRegister(4,\"s_in\")\n",
    "s_qr_enc = QuantumRegister(4,\"s_encoder\")\n",
    "s_qr_out = QuantumRegister(2,\"s_out\")\n",
    "aux_qr = QuantumRegister(1,\"aux\")\n",
    "c = ClassicalRegister(2,\"reg\")\n",
    "\n",
    "circ = QuantumCircuit(s_qr_in,s_qr_enc,s_qr_out,aux_qr,c)\n",
    "for i in range(len(FLayer_Res)):\n",
    "    ang = 2*torch.tensor(math.sqrt(FLayer_Res[i])).asin().item()\n",
    "    circ.ry(ang,s_qr_in[i])\n",
    "\n",
    "# Build the computation of original neural comp\n",
    "enc_len = int(math.log(nn_prop[1][0],2))\n",
    "for i in range(nn_prop[1][1]):\n",
    "    q_map_q_ori_net(circ, s_qr_in, s_qr_enc[enc_len*i:enc_len*(i+1)], aux_qr,nn_prop[1][2][i])\n",
    "    circ.barrier()\n",
    "circ.barrier()\n",
    "\n",
    "circ.h(s_qr_enc[0])\n",
    "circ.x(s_qr_enc[0])\n",
    "circ.h(s_qr_enc[1])\n",
    "circ.x(s_qr_enc[1])\n",
    "circ.ccx(s_qr_enc[0],s_qr_enc[1],s_qr_out[0])        \n",
    "\n",
    "circ.barrier()\n",
    "circ.h(s_qr_enc[2])\n",
    "circ.x(s_qr_enc[2])\n",
    "circ.h(s_qr_enc[3])\n",
    "circ.x(s_qr_enc[3])\n",
    "circ.ccx(s_qr_enc[2],s_qr_enc[3],s_qr_out[1])\n",
    "circ.barrier()\n",
    "\n",
    "circ.measure(s_qr_out[0],c[0])\n",
    "circ.measure(s_qr_out[1],c[1])\n",
    "\n",
    "\n",
    "SLayer_Res = []\n",
    "iters = 1\n",
    "qc_shots=8192\n",
    "counts = fire_ibmq(circ,qc_shots,iters,True,False)    \n",
    "(mycount,bits) = analyze(counts[0])\n",
    "for b in range(bits):        \n",
    "    SLayer_Res.append(float(mycount[b])/qc_shots)\n",
    "\n",
    "print(SLayer_Res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[0.219482421875, 0.2667236328125]\n",
      "Job Status: job has successfully run\n",
      "[0.447509765625, 0.51904296875]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "s_qr_in = QuantumRegister(2,\"s_in\")\n",
    "s_qr_bat = QuantumRegister(2,\"s_BN\")\n",
    "s_qr_out = QuantumRegister(2,\"s_out\")\n",
    "c = ClassicalRegister(2,\"reg\")\n",
    "\n",
    "circ = QuantumCircuit(s_qr_in,s_qr_bat,s_qr_out,c)\n",
    "\n",
    "for i in range(len(SLayer_Res)):\n",
    "    ang = 2*torch.tensor(math.sqrt(SLayer_Res[i])).asin().item()\n",
    "    circ.ry(ang,s_qr_in[i])\n",
    "\n",
    "if bn_prop[1]['x_l_0_5'][0]==0:\n",
    "    # upward\n",
    "    val = bn_prop[1]['x_running_rot'][0]\n",
    "    qc_ang = 2*torch.tensor(math.sqrt(val)).asin().item() \n",
    "    circ.ry(qc_ang,s_qr_bat[0])\n",
    "    circ.ccx(s_qr_in[0],s_qr_bat[0],s_qr_out[0])\n",
    "else:\n",
    "    # downward\n",
    "    val = bn_prop[1]['x_running_rot'][0]\n",
    "    qc_ang = 2*torch.tensor(math.sqrt(val)).asin().item()     \n",
    "    circ.ry(qc_ang,s_qr_bat[0])    \n",
    "    circ.cx(s_qr_in[0],s_qr_out[0])\n",
    "    circ.x(s_qr_in[0])\n",
    "    circ.ccx(s_qr_in[0],s_qr_bat[0],s_qr_out[0])\n",
    "\n",
    "circ.barrier()\n",
    "if bn_prop[1]['x_l_0_5'][1]==0:\n",
    "    val = bn_prop[1]['x_running_rot'][1]\n",
    "    qc_ang = 2*torch.tensor(math.sqrt(val)).asin().item() \n",
    "    circ.ry(qc_ang,s_qr_bat[1])\n",
    "    circ.ccx(s_qr_in[1],s_qr_bat[1],s_qr_out[1])\n",
    "else:\n",
    "    # downward\n",
    "    val = bn_prop[1]['x_running_rot'][1]\n",
    "    qc_ang = 2*torch.tensor(math.sqrt(val)).asin().item()     \n",
    "    circ.ry(qc_ang,s_qr_bat[1])    \n",
    "    circ.cx(s_qr_in[1],s_qr_out[1])\n",
    "    circ.x(s_qr_in[1])\n",
    "    circ.ccx(s_qr_in[1],s_qr_bat[1],s_qr_out[1])\n",
    "\n",
    "\n",
    "circ.measure(s_qr_out[0],c[0])\n",
    "circ.measure(s_qr_out[1],c[1])\n",
    "\n",
    "Final_Res = []\n",
    "\n",
    "iters = 1\n",
    "qc_shots=8192\n",
    "counts = fire_ibmq(circ,qc_shots,iters,True,False)    \n",
    "(mycount,bits) = analyze(counts[0])\n",
    "for b in range(bits):        \n",
    "    Final_Res.append(float(mycount[b])/qc_shots)\n",
    "    \n",
    "print(Final_Res)\n",
    "\n",
    "\n",
    "\n",
    "#     \n",
    "#     circ.barrier()\n",
    "# circ.barrier()\n",
    "# \n",
    "# for i in range(sec_list[1]):\n",
    "#     circ.measure(s_qr_out[i],c[i])\n",
    "#     \n",
    "#     \n",
    "# qc_ang = 2*torch.tensor(math.sqrt(val)).asin().item()        \n",
    "#         circ.ry(qc_ang,s_qr_bat[1])\n",
    "#         if type==1:\n",
    "#             circ.ccx(s_qr_bat[0],s_qr_bat[1],output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# quantum_data = ToQuantumData_Batch()(data)\n",
    "# output = model(Q_InputMatrix, False)\n",
    "# \n",
    "# print(output)\n",
    "\n",
    "# w = torch.tensor([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1., 1.,  1.])\n",
    "# quantum_data=quantum_data.view(2,-1)\n",
    "# print(\"Input\")\n",
    "# print(quantum_data)\n",
    "# weighted = (quantum_data*w)\n",
    "# print((weighted[0].sum()/4).pow(2))\n",
    "# print((weighted[1].sum()/4).pow(2))\n",
    "\n",
    "# tensor([0.0108, 0.1053, 0.1674, 0.0432, 0.0162, 0.1809, 0.3646, 0.1053, 0.0729,\n",
    "#         0.0702, 0.2674, 0.0216, 0.1242, 0.2107, 0.1809, 0.0648, 0.0000, 0.0297,\n",
    "#         0.2053, 0.0243, 0.0135, 0.2620, 0.2296, 0.4321, 0.0486, 0.0189, 0.4699,\n",
    "#         0.0108, 0.0054, 0.0891, 0.0999, 0.0972], dtype=torch.float64)\n",
    "# tensor([0.0108, 0.1053, 0.1674, 0.0432, 0.0162, 0.1809, 0.3646, 0.1053, 0.0729,\n",
    "#         0.0702, 0.2674, 0.0216, 0.1242, 0.2107, 0.1809, 0.0648, 0.0000, 0.0297,\n",
    "#         0.2053, 0.0243, 0.0135, 0.2620, 0.2296, 0.4321, 0.0486, 0.0189, 0.4699,\n",
    "#         0.0108, 0.0054, 0.0891, 0.0999, 0.0972], dtype=torch.float64)\n",
    "\n",
    "# Q_InputMatrix = ToQuantumMatrix()(data.flatten())\n",
    "\n",
    "# \n",
    "# print(Q_InputMatrix[:,0])\n",
    "# Q_InputMatrix = ToQuantumMatrix()(data[0].flatten())\n",
    "# print(Q_InputMatrix[:,0])\n",
    "# Q_InputMatrix = ToQuantumMatrix()(data[1].flatten())\n",
    "# print(Q_InputMatrix[:,0])\n",
    "# \n",
    "# mydata = [0.0000, 0.2344, 0.0732, 0.0000, 0.0146, 0.4345, 0.3027, 0.0439, 0.0146,\n",
    "#         0.5273, 0.5908, 0.1123, 0.0000, 0.0586, 0.1172, 0.0098, 0.0000, 0.0185, 0.3193, 0.0787, 0.0046, 0.2823, 0.4026, 0.0278, 0.0972,\n",
    "#         0.6154, 0.4720, 0.0370, 0.0324, 0.1805, 0.0555, 0.0000]\n",
    "# Q_InputMatrix = ToQuantumMatrix()(torch.tensor(mydata))\n",
    "# print(Q_InputMatrix[:,0])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# circ.append(UnitaryGate(Q_InputMatrix, label=\"iswap\"), f_qr[iq:iq*2])\n",
    "\n",
    "# quantum_gates,ret_index = qf_map_extract_from_weight(nn_prop[0][2][0])\n",
    "# expand_for_batch_index = copy.deepcopy(ret_index)\n",
    "# for b in range(inference_batch_size-1):\n",
    "#     start = len(ret_index)*(b+1)\n",
    "#     new_batch_index = [x+start for x in ret_index]\n",
    "#     expand_for_batch_index += new_batch_index\n",
    "# index = torch.LongTensor(expand_for_batch_index)\n",
    "# Input0 = copy.deepcopy(Q_InputMatrix)\n",
    "# Input0 = Input0[index]\n",
    "# circ.append(UnitaryGate(Input0, label=\"Input0\"), f_qr[0:iq])\n",
    "# qbits = f_qr[inference_batch_size-1:iq]\n",
    "# \n",
    "# \n",
    "# for gate in quantum_gates:\n",
    "#     z_count = gate.count(\"1\")    \n",
    "#     z_pos = get_index_list(gate[::-1],\"1\")    \n",
    "#     if z_count==1:\n",
    "#         circ.z(qbits[z_pos[0]])\n",
    "#     elif z_count==2:\n",
    "#         circ.cz(qbits[z_pos[0]],qbits[z_pos[1]])\n",
    "#     elif z_count==3:\n",
    "#         qiskit_library.ccz(circ,qbits[z_pos[0]],qbits[z_pos[1]],qbits[z_pos[2]],aux_qr[0])\n",
    "#     elif z_count==4:\n",
    "#         qiskit_library.cccz(circ,qbits[z_pos[0]],qbits[z_pos[1]],qbits[z_pos[2]],qbits[z_pos[3]],aux_qr[0],aux_qr[1])\n",
    "    \n",
    "\n",
    "# quantum_gates,ret_index = qf_map_extract_from_weight(nn_prop[0][2][1])\n",
    "# expand_for_batch_index = copy.deepcopy(ret_index)\n",
    "# for b in range(inference_batch_size-1):\n",
    "#     start = len(ret_index)*(b+1)\n",
    "#     new_batch_index = [x+start for x in ret_index]\n",
    "#     expand_for_batch_index += new_batch_index\n",
    "# index = torch.LongTensor(expand_for_batch_index)\n",
    "# Input1 = copy.deepcopy(Q_InputMatrix)\n",
    "# Input1 = Input1[index]\n",
    "# circ.append(UnitaryGate(Input1, label=\"Input1\"), f_qr[iq:iq*2])\n",
    "# qbits = f_qr[iq+inference_batch_size-1:iq*2]\n",
    "# for gate in quantum_gates:\n",
    "#     z_count = gate.count(\"1\")\n",
    "#     z_pos = get_index_list(gate[::-1],\"1\")\n",
    "#     if z_count==1:\n",
    "#         circ.z(qbits[z_pos[0]])\n",
    "#     elif z_count==2:\n",
    "#         circ.cz(qbits[z_pos[0]],qbits[z_pos[1]])\n",
    "#     elif z_count==3:\n",
    "#         qiskit_library.ccz(circ,qbits[z_pos[0]],qbits[z_pos[1]],qbits[z_pos[2]],aux_qr[0])\n",
    "#     elif z_count==4:\n",
    "#         qiskit_library.cccz(circ,qbits[z_pos[0]],qbits[z_pos[1]],qbits[z_pos[2]],qbits[z_pos[3]],aux_qr[0],aux_qr[1])\n",
    "# \n",
    "# circ.barrier()\n",
    "\n",
    "# \n",
    "# qbits = f_qr[inference_batch_size-1:iq]\n",
    "# for q in qbits:\n",
    "#     circ.h(q)\n",
    "# \n",
    "# circ.barrier()\n",
    "# \n",
    "# for q in qbits:\n",
    "#     circ.x(q)\n",
    "# \n",
    "# \n",
    "# digits = int(math.log(inference_batch_size,2))\n",
    "# flag = \"0\"+str(digits)+\"b\"\n",
    "# qbits = f_qr[0:inference_batch_size-1]\n",
    "# if inference_batch_size!=1:\n",
    "#     for i in range(inference_batch_size):        \n",
    "#         binstr_cur_num = format(i,flag)\n",
    "#         for pos in range(len(binstr_cur_num)):            \n",
    "#             if binstr_cur_num[pos]==\"0\":                \n",
    "#                 circ.x(qbits[pos])\n",
    "#             if digits==1:                            \n",
    "#                 qiskit_library.cccccx(circ, f_qr[0:iq], s_qr[i*2],aux_qr)\n",
    "#             if binstr_cur_num[pos]==\"0\":                \n",
    "#                 circ.x(qbits[pos])            \n",
    "# else:\n",
    "#     qiskit_library.ccccx(circ, f_qr[0], f_qr[1], f_qr[2], f_qr[3], s_qr[0], aux_qr[0], aux_qr[1])\n",
    "#         \n",
    "# # qbits = f_qr[inference_batch_size-1:iq]\n",
    "# # for q in qbits:    \n",
    "# #     circ.x(q)\n",
    "# \n",
    "# print(circ)\n",
    "# circ.barrier()\n",
    "# print(circ)\n",
    "# sys.exit(0)\n",
    "# \n",
    "# \n",
    "# \n",
    "# \n",
    "# qbits = f_qr[iq+inference_batch_size-1:iq*2]\n",
    "# for q in qbits:\n",
    "#     circ.h(q)\n",
    "#     circ.x(q)\n",
    "# qiskit_library.ccccx(circ, f_qr[4], f_qr[5], f_qr[6], f_qr[7], s_qr[1], aux_qr[0], aux_qr[1])\n",
    "# qbits = f_qr[iq+inference_batch_size-1:iq*2]\n",
    "# for q in qbits:\n",
    "#     circ.x(q)\n",
    "# circ.barrier()\n",
    "# \n",
    "# \n",
    "# circ.measure(s_qr[0],c[0])\n",
    "# circ.measure(s_qr[1],c[1])\n",
    "# print(circ)\n",
    "# \n",
    "# sys.exit(0)\n",
    "\n",
    "# \n",
    "# print(Q_InputMatrix.flatten())\n",
    "# print(\"==\"*10)\n",
    "# print(Input0.flatten())\n",
    "# print(\"==\"*10)\n",
    "# print(Input1.flatten())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8213722",
   "language": "python",
   "display_name": "PyCharm (qiskit_practice)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}