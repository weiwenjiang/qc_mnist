{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Training procedure for Quantum Computer:\n",
      "\tStart at: 05/05/2020 00:29:59\n",
      "\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\n",
      "\tEnjoy and Good Luck!\n",
      "====================================================================================================\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from lib_model_summary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import functools\n",
    "\n",
    "# from lib_util import *\n",
    "\n",
    "from collections import Counter\n",
    "print = functools.partial(print, flush=True)\n",
    "\n",
    "# For 4*4, 16->4->2: batch_size=32; init_lr=0.01; with_norm=True or False\n",
    "# For 4*4, 16->4->1: batch_size=16; init_lr=0.1; with_norm=True, ang:20; or train\n",
    "\n",
    "# interest_num = [0,1,2,3,4,5,6,7,8,9]\n",
    "# how many samples per batch to load\n",
    "batch_size = 32\n",
    "inference_batch_size = 32\n",
    "num_f1 = 2\n",
    "# num_f2 = len(interest_num)\n",
    "num_f2 = 2\n",
    "num_f3 = 2\n",
    "init_lr = 0.1\n",
    "init_qc_lr = 0.01\n",
    "with_norm = True\n",
    "\n",
    "# Given_ang to -1 to train the variable \n",
    "given_ang = -1\n",
    "milestones = [6, 10, 14]\n",
    "\n",
    "save_to_file = False\n",
    "if save_to_file:\n",
    "    sys.stdout = open(save_path+\"/log\", 'w')\n",
    "\n",
    "# resume_path = \"././model/ipykernel_launcher.py_2020_05_04-10_08_35/checkpoint_5_0.9401.pth.tar\"\n",
    "resume_path = \"././model/ipykernel_launcher.py_2020_05_04-13_52_53/checkpoint_0_0.9714.pth.tar\"\n",
    "resume_path = \"././model/ipykernel_launcher.py_2020_05_04-14_22_13/checkpoint_0_0.9714.pth.tar\"\n",
    "# resume_path = \"././model/ipykernel_launcher.py_2020_05_04-16_16_37/checkpoint_3_1.0.pth.tar\"\n",
    "# resume_path = \"\"\n",
    "save_chkp = 0\n",
    "training = 0\n",
    "print_detail = 0\n",
    "max_epoch = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "if save_chkp:\n",
    "    save_path = \"./model/\"+os.path.basename(sys.argv[0])+\"_\"+time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Training procedure for Quantum Computer:\")\n",
    "print(\"\\tStart at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "print(\"\\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\")\n",
    "print(\"\\tEnjoy and Good Luck!\")\n",
    "print(\"=\"*100)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 \n",
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "input = []\n",
    "output = []\n",
    "i_dim = 20\n",
    "j_dim = 20\n",
    "# ori_out = [1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "ori_out = [1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "# ori_out = [1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1]\n",
    "\n",
    "\n",
    "# 1 layer 2 neural\n",
    "# ori_out = [1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1]\n",
    "\n",
    "golden_data = {}\n",
    "for i in range(i_dim):\n",
    "    for j in range(j_dim):\n",
    "        # if i==0 and j==0:\n",
    "        #     input.append([0.1*i+0.0001,0.1*j+0.0001])\n",
    "        \n",
    "        input.append([(1.0/i_dim)*(i+1),(1.0/j_dim)*(j+1)])\n",
    "        # if i > j or i>j_dim-j:\n",
    "        # # if i > j:\n",
    "        #     output.append([0])\n",
    "        # else:\n",
    "        #     output.append([1])\n",
    "        output.append(ori_out[i*j_dim+j])\n",
    "        golden_data[tuple([(1.0/i_dim)*(i+1),(1.0/j_dim)*(j+1)])] = ori_out[i*j_dim+j]\n",
    "\n",
    "for i in range(i_dim):\n",
    "    for j in range(j_dim):\n",
    "        print(output[i*j_dim+j],end=\" \")\n",
    "    print()\n",
    "\n",
    "dataset_ori = []\n",
    "for i in range(i_dim):\n",
    "    for j in range(j_dim):\n",
    "        dataset_ori.append([torch.tensor(input[i*j_dim+j]),torch.tensor(output[i*j_dim+j])])\n",
    "        \n",
    "\n",
    "import random\n",
    "def batch_generator(dataset,batch_size):\n",
    "    batch_dataset = []\n",
    "    for i in range(int(len(dataset)/batch_size)):\n",
    "        batch = torch.zeros((batch_size,2))        \n",
    "        batch_target = torch.zeros((batch_size,1),dtype=torch.long)\n",
    "        for j in range(batch_size):\n",
    "            item = dataset[i*batch_size+j]            \n",
    "            batch[j] = item[0]\n",
    "            batch_target[j] = item[1]\n",
    "            \n",
    "        batch_dataset.append((batch,batch_target))\n",
    "        \n",
    "    return batch_dataset\n",
    "\n",
    "dataset = batch_generator(dataset_ori,batch_size)\n",
    "# dataset = torch.tensor(dataset)\n",
    "        \n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    filename = os.path.join(save_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        bestname = os.path.join(save_path, 'model_best.tar')\n",
    "        shutil.copyfile(filename, bestname)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "========== Model Info ==========\n",
      "Net(\n",
      "  (fc1): BinaryLinear(in_features=2, out_features=2, bias=False)\n",
      "  (fc2): BinaryLinear(in_features=2, out_features=2, bias=False)\n",
      "  (qc1): QC_Norm_try3()\n",
      "  (qc2): QC_Norm_try3()\n",
      "  (qc1a): QC_Norm_Correction_try2()\n",
      "  (qc2a): QC_Norm_Correction_try2()\n",
      ")\n",
      "=> loading checkpoint from '././model/ipykernel_launcher.py_2020_05_04-14_22_13/checkpoint_0_0.9714.pth.tar'<=\n",
      "tensor(0.4216) tensor(0.2889) tensor(0.7105) 1 (0.05, 0.05)\n",
      "tensor(0.3699) tensor(0.3148) tensor(0.6847) 1 (0.05, 0.1)\n",
      "tensor(0.3204) tensor(0.3395) tensor(0.6600) 1 (0.05, 0.15000000000000002)\n",
      "tensor(0.2734) tensor(0.3631) tensor(0.6365) 1 (0.05, 0.2)\n",
      "tensor(0.2286) tensor(0.3855) tensor(0.6141) 1 (0.05, 0.25)\n",
      "tensor(0.1862) tensor(0.4067) tensor(0.5929) 1 (0.05, 0.30000000000000004)\n",
      "tensor(0.1461) tensor(0.4267) tensor(0.5728) 1 (0.05, 0.35000000000000003)\n",
      "tensor(0.1083) tensor(0.4456) tensor(0.5540) 1 (0.05, 0.4)\n",
      "tensor(0.0729) tensor(0.4634) tensor(0.5363) 1 (0.05, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.05, 0.5)\n",
      "tensor(0.0090) tensor(0.4953) tensor(0.5043) 0 (0.05, 0.55)\n",
      "tensor(0.0194) tensor(0.5095) tensor(0.4901) 0 (0.05, 0.6000000000000001)\n",
      "tensor(0.0455) tensor(0.5226) tensor(0.4771) 0 (0.05, 0.65)\n",
      "tensor(0.0693) tensor(0.5345) tensor(0.4652) 0 (0.05, 0.7000000000000001)\n",
      "tensor(0.0908) tensor(0.5452) tensor(0.4545) 0 (0.05, 0.75)\n",
      "tensor(0.1099) tensor(0.5548) tensor(0.4449) 0 (0.05, 0.8)\n",
      "tensor(0.1267) tensor(0.5632) tensor(0.4365) 0 (0.05, 0.8500000000000001)\n",
      "tensor(0.1411) tensor(0.5704) tensor(0.4293) 0 (0.05, 0.9)\n",
      "tensor(0.1533) tensor(0.5765) tensor(0.4232) 0 (0.05, 0.9500000000000001)\n",
      "tensor(0.1631) tensor(0.5814) tensor(0.4183) 0 (0.05, 1.0)\n",
      "tensor(0.3699) tensor(0.3148) tensor(0.6847) 1 (0.1, 0.05)\n",
      "tensor(0.3258) tensor(0.3369) tensor(0.6627) 1 (0.1, 0.1)\n",
      "tensor(0.2836) tensor(0.3580) tensor(0.6416) 1 (0.1, 0.15000000000000002)\n",
      "tensor(0.2433) tensor(0.3781) tensor(0.6214) 1 (0.1, 0.2)\n",
      "tensor(0.2048) tensor(0.3974) tensor(0.6022) 1 (0.1, 0.25)\n",
      "tensor(0.1681) tensor(0.4157) tensor(0.5838) 1 (0.1, 0.30000000000000004)\n",
      "tensor(0.1333) tensor(0.4332) tensor(0.5664) 1 (0.1, 0.35000000000000003)\n",
      "tensor(0.1003) tensor(0.4497) tensor(0.5499) 1 (0.1, 0.4)\n",
      "tensor(0.0691) tensor(0.4653) tensor(0.5344) 1 (0.1, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.1, 0.5)\n",
      "tensor(0.0123) tensor(0.4937) tensor(0.5060) 1 (0.1, 0.55)\n",
      "tensor(0.0133) tensor(0.5065) tensor(0.4932) 0 (0.1, 0.6000000000000001)\n",
      "tensor(0.0371) tensor(0.5184) tensor(0.4813) 0 (0.1, 0.65)\n",
      "tensor(0.0590) tensor(0.5293) tensor(0.4703) 0 (0.1, 0.7000000000000001)\n",
      "tensor(0.0791) tensor(0.5394) tensor(0.4603) 0 (0.1, 0.75)\n",
      "tensor(0.0974) tensor(0.5485) tensor(0.4511) 0 (0.1, 0.8)\n",
      "tensor(0.1138) tensor(0.5568) tensor(0.4429) 0 (0.1, 0.8500000000000001)\n",
      "tensor(0.1284) tensor(0.5640) tensor(0.4356) 0 (0.1, 0.9)\n",
      "tensor(0.1411) tensor(0.5704) tensor(0.4293) 0 (0.1, 0.9500000000000001)\n",
      "tensor(0.1520) tensor(0.5759) tensor(0.4238) 0 (0.1, 1.0)\n",
      "tensor(0.3204) tensor(0.3395) tensor(0.6600) 1 (0.15000000000000002, 0.05)\n",
      "tensor(0.2836) tensor(0.3580) tensor(0.6416) 1 (0.15000000000000002, 0.1)\n",
      "tensor(0.2482) tensor(0.3757) tensor(0.6239) 1 (0.15000000000000002, 0.15000000000000002)\n",
      "tensor(0.2142) tensor(0.3927) tensor(0.6069) 1 (0.15000000000000002, 0.2)\n",
      "tensor(0.1816) tensor(0.4090) tensor(0.5906) 1 (0.15000000000000002, 0.25)\n",
      "tensor(0.1504) tensor(0.4246) tensor(0.5750) 1 (0.15000000000000002, 0.30000000000000004)\n",
      "tensor(0.1207) tensor(0.4395) tensor(0.5601) 1 (0.15000000000000002, 0.35000000000000003)\n",
      "tensor(0.0923) tensor(0.4537) tensor(0.5460) 1 (0.15000000000000002, 0.4)\n",
      "tensor(0.0653) tensor(0.4671) tensor(0.5325) 1 (0.15000000000000002, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.15000000000000002, 0.5)\n",
      "tensor(0.0157) tensor(0.4920) tensor(0.5076) 1 (0.15000000000000002, 0.55)\n",
      "tensor(0.0071) tensor(0.5034) tensor(0.4963) 0 (0.15000000000000002, 0.6000000000000001)\n",
      "tensor(0.0284) tensor(0.5140) tensor(0.4856) 0 (0.15000000000000002, 0.65)\n",
      "tensor(0.0483) tensor(0.5240) tensor(0.4757) 0 (0.15000000000000002, 0.7000000000000001)\n",
      "tensor(0.0668) tensor(0.5332) tensor(0.4664) 0 (0.15000000000000002, 0.75)\n",
      "tensor(0.0839) tensor(0.5418) tensor(0.4579) 0 (0.15000000000000002, 0.8)\n",
      "tensor(0.0996) tensor(0.5496) tensor(0.4501) 0 (0.15000000000000002, 0.8500000000000001)\n",
      "tensor(0.1138) tensor(0.5568) tensor(0.4429) 0 (0.15000000000000002, 0.9)\n",
      "tensor(0.1267) tensor(0.5632) tensor(0.4365) 0 (0.15000000000000002, 0.9500000000000001)\n",
      "tensor(0.1381) tensor(0.5689) tensor(0.4308) 0 (0.15000000000000002, 1.0)\n",
      "tensor(0.2734) tensor(0.3631) tensor(0.6365) 1 (0.2, 0.05)\n",
      "tensor(0.2433) tensor(0.3781) tensor(0.6214) 1 (0.2, 0.1)\n",
      "tensor(0.2142) tensor(0.3927) tensor(0.6069) 1 (0.2, 0.15000000000000002)\n",
      "tensor(0.1862) tensor(0.4067) tensor(0.5929) 1 (0.2, 0.2)\n",
      "tensor(0.1592) tensor(0.4202) tensor(0.5794) 1 (0.2, 0.25)\n",
      "tensor(0.1333) tensor(0.4332) tensor(0.5664) 1 (0.2, 0.30000000000000004)\n",
      "tensor(0.1083) tensor(0.4456) tensor(0.5540) 1 (0.2, 0.35000000000000003)\n",
      "tensor(0.0845) tensor(0.4576) tensor(0.5420) 1 (0.2, 0.4)\n",
      "tensor(0.0616) tensor(0.4690) tensor(0.5306) 1 (0.2, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.2, 0.5)\n",
      "tensor(0.0190) tensor(0.4903) tensor(0.5093) 1 (0.2, 0.55)\n",
      "tensor(0.0007) tensor(0.5002) tensor(0.4995) 0 (0.2, 0.6000000000000001)\n",
      "tensor(0.0194) tensor(0.5095) tensor(0.4901) 0 (0.2, 0.65)\n",
      "tensor(0.0371) tensor(0.5184) tensor(0.4813) 0 (0.2, 0.7000000000000001)\n",
      "tensor(0.0537) tensor(0.5267) tensor(0.4730) 0 (0.2, 0.75)\n",
      "tensor(0.0693) tensor(0.5345) tensor(0.4652) 0 (0.2, 0.8)\n",
      "tensor(0.0839) tensor(0.5418) tensor(0.4579) 0 (0.2, 0.8500000000000001)\n",
      "tensor(0.0974) tensor(0.5485) tensor(0.4511) 0 (0.2, 0.9)\n",
      "tensor(0.1099) tensor(0.5548) tensor(0.4449) 0 (0.2, 0.9500000000000001)\n",
      "tensor(0.1213) tensor(0.5605) tensor(0.4392) 0 (0.2, 1.0)\n",
      "tensor(0.2286) tensor(0.3855) tensor(0.6141) 1 (0.25, 0.05)\n",
      "tensor(0.2048) tensor(0.3974) tensor(0.6022) 1 (0.25, 0.1)\n",
      "tensor(0.1816) tensor(0.4090) tensor(0.5906) 1 (0.25, 0.15000000000000002)\n",
      "tensor(0.1592) tensor(0.4202) tensor(0.5794) 1 (0.25, 0.2)\n",
      "tensor(0.1375) tensor(0.4310) tensor(0.5685) 1 (0.25, 0.25)\n",
      "tensor(0.1165) tensor(0.4415) tensor(0.5581) 1 (0.25, 0.30000000000000004)\n",
      "tensor(0.0963) tensor(0.4517) tensor(0.5479) 1 (0.25, 0.35000000000000003)\n",
      "tensor(0.0767) tensor(0.4614) tensor(0.5382) 1 (0.25, 0.4)\n",
      "tensor(0.0579) tensor(0.4709) tensor(0.5288) 1 (0.25, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.25, 0.5)\n",
      "tensor(0.0224) tensor(0.4886) tensor(0.5110) 1 (0.25, 0.55)\n",
      "tensor(0.0058) tensor(0.4969) tensor(0.5027) 0 (0.25, 0.6000000000000001)\n",
      "tensor(0.0102) tensor(0.5049) tensor(0.4947) 0 (0.25, 0.65)\n",
      "tensor(0.0254) tensor(0.5125) tensor(0.4871) 0 (0.25, 0.7000000000000001)\n",
      "tensor(0.0399) tensor(0.5198) tensor(0.4799) 0 (0.25, 0.75)\n",
      "tensor(0.0537) tensor(0.5267) tensor(0.4730) 0 (0.25, 0.8)\n",
      "tensor(0.0668) tensor(0.5332) tensor(0.4664) 0 (0.25, 0.8500000000000001)\n",
      "tensor(0.0791) tensor(0.5394) tensor(0.4603) 0 (0.25, 0.9)\n",
      "tensor(0.0908) tensor(0.5452) tensor(0.4545) 0 (0.25, 0.9500000000000001)\n",
      "tensor(0.1017) tensor(0.5507) tensor(0.4490) 0 (0.25, 1.0)\n",
      "tensor(0.1862) tensor(0.4067) tensor(0.5929) 1 (0.30000000000000004, 0.05)\n",
      "tensor(0.1681) tensor(0.4157) tensor(0.5838) 1 (0.30000000000000004, 0.1)\n",
      "tensor(0.1504) tensor(0.4246) tensor(0.5750) 1 (0.30000000000000004, 0.15000000000000002)\n",
      "tensor(0.1333) tensor(0.4332) tensor(0.5664) 1 (0.30000000000000004, 0.2)\n",
      "tensor(0.1165) tensor(0.4415) tensor(0.5581) 1 (0.30000000000000004, 0.25)\n",
      "tensor(0.1003) tensor(0.4497) tensor(0.5499) 1 (0.30000000000000004, 0.30000000000000004)\n",
      "tensor(0.0845) tensor(0.4576) tensor(0.5420) 1 (0.30000000000000004, 0.35000000000000003)\n",
      "tensor(0.0691) tensor(0.4653) tensor(0.5344) 1 (0.30000000000000004, 0.4)\n",
      "tensor(0.0542) tensor(0.4727) tensor(0.5269) 1 (0.30000000000000004, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.30000000000000004, 0.5)\n",
      "tensor(0.0258) tensor(0.4869) tensor(0.5127) 1 (0.30000000000000004, 0.55)\n",
      "tensor(0.0123) tensor(0.4937) tensor(0.5060) 1 (0.30000000000000004, 0.6000000000000001)\n",
      "tensor(0.0007) tensor(0.5002) tensor(0.4995) 0 (0.30000000000000004, 0.65)\n",
      "tensor(0.0133) tensor(0.5065) tensor(0.4932) 0 (0.30000000000000004, 0.7000000000000001)\n",
      "tensor(0.0254) tensor(0.5125) tensor(0.4871) 0 (0.30000000000000004, 0.75)\n",
      "tensor(0.0371) tensor(0.5184) tensor(0.4813) 0 (0.30000000000000004, 0.8)\n",
      "tensor(0.0483) tensor(0.5240) tensor(0.4757) 0 (0.30000000000000004, 0.8500000000000001)\n",
      "tensor(0.0590) tensor(0.5293) tensor(0.4703) 0 (0.30000000000000004, 0.9)\n",
      "tensor(0.0693) tensor(0.5345) tensor(0.4652) 0 (0.30000000000000004, 0.9500000000000001)\n",
      "tensor(0.0791) tensor(0.5394) tensor(0.4603) 0 (0.30000000000000004, 1.0)\n",
      "tensor(0.1461) tensor(0.4267) tensor(0.5728) 1 (0.35000000000000003, 0.05)\n",
      "tensor(0.1333) tensor(0.4332) tensor(0.5664) 1 (0.35000000000000003, 0.1)\n",
      "tensor(0.1207) tensor(0.4395) tensor(0.5601) 1 (0.35000000000000003, 0.15000000000000002)\n",
      "tensor(0.1083) tensor(0.4456) tensor(0.5540) 1 (0.35000000000000003, 0.2)\n",
      "tensor(0.0963) tensor(0.4517) tensor(0.5479) 1 (0.35000000000000003, 0.25)\n",
      "tensor(0.0845) tensor(0.4576) tensor(0.5420) 1 (0.35000000000000003, 0.30000000000000004)\n",
      "tensor(0.0729) tensor(0.4634) tensor(0.5363) 1 (0.35000000000000003, 0.35000000000000003)\n",
      "tensor(0.0616) tensor(0.4690) tensor(0.5306) 1 (0.35000000000000003, 0.4)\n",
      "tensor(0.0506) tensor(0.4745) tensor(0.5251) 1 (0.35000000000000003, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.35000000000000003, 0.5)\n",
      "tensor(0.0293) tensor(0.4852) tensor(0.5145) 1 (0.35000000000000003, 0.55)\n",
      "tensor(0.0190) tensor(0.4903) tensor(0.5093) 1 (0.35000000000000003, 0.6000000000000001)\n",
      "tensor(0.0090) tensor(0.4953) tensor(0.5043) 0 (0.35000000000000003, 0.65)\n",
      "tensor(0.0007) tensor(0.5002) tensor(0.4995) 0 (0.35000000000000003, 0.7000000000000001)\n",
      "tensor(0.0102) tensor(0.5049) tensor(0.4947) 0 (0.35000000000000003, 0.75)\n",
      "tensor(0.0194) tensor(0.5095) tensor(0.4901) 0 (0.35000000000000003, 0.8)\n",
      "tensor(0.0284) tensor(0.5140) tensor(0.4856) 0 (0.35000000000000003, 0.8500000000000001)\n",
      "tensor(0.0371) tensor(0.5184) tensor(0.4813) 0 (0.35000000000000003, 0.9)\n",
      "tensor(0.0455) tensor(0.5226) tensor(0.4771) 0 (0.35000000000000003, 0.9500000000000001)\n",
      "tensor(0.0537) tensor(0.5267) tensor(0.4730) 0 (0.35000000000000003, 1.0)\n",
      "tensor(0.1083) tensor(0.4456) tensor(0.5540) 1 (0.4, 0.05)\n",
      "tensor(0.1003) tensor(0.4497) tensor(0.5499) 1 (0.4, 0.1)\n",
      "tensor(0.0923) tensor(0.4537) tensor(0.5460) 1 (0.4, 0.15000000000000002)\n",
      "tensor(0.0845) tensor(0.4576) tensor(0.5420) 1 (0.4, 0.2)\n",
      "tensor(0.0767) tensor(0.4614) tensor(0.5382) 1 (0.4, 0.25)\n",
      "tensor(0.0691) tensor(0.4653) tensor(0.5344) 1 (0.4, 0.30000000000000004)\n",
      "tensor(0.0616) tensor(0.4690) tensor(0.5306) 1 (0.4, 0.35000000000000003)\n",
      "tensor(0.0542) tensor(0.4727) tensor(0.5269) 1 (0.4, 0.4)\n",
      "tensor(0.0470) tensor(0.4763) tensor(0.5233) 1 (0.4, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.4, 0.5)\n",
      "tensor(0.0328) tensor(0.4834) tensor(0.5162) 1 (0.4, 0.55)\n",
      "tensor(0.0258) tensor(0.4869) tensor(0.5127) 1 (0.4, 0.6000000000000001)\n",
      "tensor(0.0190) tensor(0.4903) tensor(0.5093) 1 (0.4, 0.65)\n",
      "tensor(0.0123) tensor(0.4937) tensor(0.5060) 1 (0.4, 0.7000000000000001)\n",
      "tensor(0.0058) tensor(0.4969) tensor(0.5027) 0 (0.4, 0.75)\n",
      "tensor(0.0007) tensor(0.5002) tensor(0.4995) 0 (0.4, 0.8)\n",
      "tensor(0.0071) tensor(0.5034) tensor(0.4963) 0 (0.4, 0.8500000000000001)\n",
      "tensor(0.0133) tensor(0.5065) tensor(0.4932) 0 (0.4, 0.9)\n",
      "tensor(0.0194) tensor(0.5095) tensor(0.4901) 0 (0.4, 0.9500000000000001)\n",
      "tensor(0.0254) tensor(0.5125) tensor(0.4871) 0 (0.4, 1.0)\n",
      "tensor(0.0729) tensor(0.4634) tensor(0.5363) 1 (0.45, 0.05)\n",
      "tensor(0.0691) tensor(0.4653) tensor(0.5344) 1 (0.45, 0.1)\n",
      "tensor(0.0653) tensor(0.4671) tensor(0.5325) 1 (0.45, 0.15000000000000002)\n",
      "tensor(0.0616) tensor(0.4690) tensor(0.5306) 1 (0.45, 0.2)\n",
      "tensor(0.0579) tensor(0.4709) tensor(0.5288) 1 (0.45, 0.25)\n",
      "tensor(0.0542) tensor(0.4727) tensor(0.5269) 1 (0.45, 0.30000000000000004)\n",
      "tensor(0.0506) tensor(0.4745) tensor(0.5251) 1 (0.45, 0.35000000000000003)\n",
      "tensor(0.0470) tensor(0.4763) tensor(0.5233) 1 (0.45, 0.4)\n",
      "tensor(0.0434) tensor(0.4781) tensor(0.5215) 1 (0.45, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.45, 0.5)\n",
      "tensor(0.0363) tensor(0.4817) tensor(0.5179) 1 (0.45, 0.55)\n",
      "tensor(0.0328) tensor(0.4834) tensor(0.5162) 1 (0.45, 0.6000000000000001)\n",
      "tensor(0.0293) tensor(0.4852) tensor(0.5145) 1 (0.45, 0.65)\n",
      "tensor(0.0258) tensor(0.4869) tensor(0.5127) 1 (0.45, 0.7000000000000001)\n",
      "tensor(0.0224) tensor(0.4886) tensor(0.5110) 1 (0.45, 0.75)\n",
      "tensor(0.0190) tensor(0.4903) tensor(0.5093) 1 (0.45, 0.8)\n",
      "tensor(0.0157) tensor(0.4920) tensor(0.5076) 1 (0.45, 0.8500000000000001)\n",
      "tensor(0.0123) tensor(0.4937) tensor(0.5060) 1 (0.45, 0.9)\n",
      "tensor(0.0090) tensor(0.4953) tensor(0.5043) 0 (0.45, 0.9500000000000001)\n",
      "tensor(0.0058) tensor(0.4969) tensor(0.5027) 0 (0.45, 1.0)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.05)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.1)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.15000000000000002)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.2)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.25)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.30000000000000004)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.35000000000000003)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.4)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.5)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.55)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.6000000000000001)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.65)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.7000000000000001)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.75)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.8)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.8500000000000001)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.9)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 0.9500000000000001)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.5, 1.0)\n",
      "tensor(0.0090) tensor(0.4953) tensor(0.5043) 0 (0.55, 0.05)\n",
      "tensor(0.0123) tensor(0.4937) tensor(0.5060) 1 (0.55, 0.1)\n",
      "tensor(0.0157) tensor(0.4920) tensor(0.5076) 1 (0.55, 0.15000000000000002)\n",
      "tensor(0.0190) tensor(0.4903) tensor(0.5093) 1 (0.55, 0.2)\n",
      "tensor(0.0224) tensor(0.4886) tensor(0.5110) 1 (0.55, 0.25)\n",
      "tensor(0.0258) tensor(0.4869) tensor(0.5127) 1 (0.55, 0.30000000000000004)\n",
      "tensor(0.0293) tensor(0.4852) tensor(0.5145) 1 (0.55, 0.35000000000000003)\n",
      "tensor(0.0328) tensor(0.4834) tensor(0.5162) 1 (0.55, 0.4)\n",
      "tensor(0.0363) tensor(0.4817) tensor(0.5179) 1 (0.55, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.55, 0.5)\n",
      "tensor(0.0434) tensor(0.4781) tensor(0.5215) 1 (0.55, 0.55)\n",
      "tensor(0.0470) tensor(0.4763) tensor(0.5233) 1 (0.55, 0.6000000000000001)\n",
      "tensor(0.0506) tensor(0.4745) tensor(0.5251) 1 (0.55, 0.65)\n",
      "tensor(0.0542) tensor(0.4727) tensor(0.5269) 1 (0.55, 0.7000000000000001)\n",
      "tensor(0.0579) tensor(0.4709) tensor(0.5288) 1 (0.55, 0.75)\n",
      "tensor(0.0616) tensor(0.4690) tensor(0.5306) 1 (0.55, 0.8)\n",
      "tensor(0.0653) tensor(0.4671) tensor(0.5325) 1 (0.55, 0.8500000000000001)\n",
      "tensor(0.0691) tensor(0.4653) tensor(0.5344) 1 (0.55, 0.9)\n",
      "tensor(0.0729) tensor(0.4634) tensor(0.5363) 1 (0.55, 0.9500000000000001)\n",
      "tensor(0.0767) tensor(0.4614) tensor(0.5382) 1 (0.55, 1.0)\n",
      "tensor(0.0194) tensor(0.5095) tensor(0.4901) 0 (0.6000000000000001, 0.05)\n",
      "tensor(0.0133) tensor(0.5065) tensor(0.4932) 0 (0.6000000000000001, 0.1)\n",
      "tensor(0.0071) tensor(0.5034) tensor(0.4963) 0 (0.6000000000000001, 0.15000000000000002)\n",
      "tensor(0.0007) tensor(0.5002) tensor(0.4995) 0 (0.6000000000000001, 0.2)\n",
      "tensor(0.0058) tensor(0.4969) tensor(0.5027) 0 (0.6000000000000001, 0.25)\n",
      "tensor(0.0123) tensor(0.4937) tensor(0.5060) 1 (0.6000000000000001, 0.30000000000000004)\n",
      "tensor(0.0190) tensor(0.4903) tensor(0.5093) 1 (0.6000000000000001, 0.35000000000000003)\n",
      "tensor(0.0258) tensor(0.4869) tensor(0.5127) 1 (0.6000000000000001, 0.4)\n",
      "tensor(0.0328) tensor(0.4834) tensor(0.5162) 1 (0.6000000000000001, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.6000000000000001, 0.5)\n",
      "tensor(0.0470) tensor(0.4763) tensor(0.5233) 1 (0.6000000000000001, 0.55)\n",
      "tensor(0.0542) tensor(0.4727) tensor(0.5269) 1 (0.6000000000000001, 0.6000000000000001)\n",
      "tensor(0.0616) tensor(0.4690) tensor(0.5306) 1 (0.6000000000000001, 0.65)\n",
      "tensor(0.0691) tensor(0.4653) tensor(0.5344) 1 (0.6000000000000001, 0.7000000000000001)\n",
      "tensor(0.0767) tensor(0.4614) tensor(0.5382) 1 (0.6000000000000001, 0.75)\n",
      "tensor(0.0845) tensor(0.4576) tensor(0.5420) 1 (0.6000000000000001, 0.8)\n",
      "tensor(0.0923) tensor(0.4537) tensor(0.5460) 1 (0.6000000000000001, 0.8500000000000001)\n",
      "tensor(0.1003) tensor(0.4497) tensor(0.5499) 1 (0.6000000000000001, 0.9)\n",
      "tensor(0.1083) tensor(0.4456) tensor(0.5540) 1 (0.6000000000000001, 0.9500000000000001)\n",
      "tensor(0.1165) tensor(0.4415) tensor(0.5581) 1 (0.6000000000000001, 1.0)\n",
      "tensor(0.0455) tensor(0.5226) tensor(0.4771) 0 (0.65, 0.05)\n",
      "tensor(0.0371) tensor(0.5184) tensor(0.4813) 0 (0.65, 0.1)\n",
      "tensor(0.0284) tensor(0.5140) tensor(0.4856) 0 (0.65, 0.15000000000000002)\n",
      "tensor(0.0194) tensor(0.5095) tensor(0.4901) 0 (0.65, 0.2)\n",
      "tensor(0.0102) tensor(0.5049) tensor(0.4947) 0 (0.65, 0.25)\n",
      "tensor(0.0007) tensor(0.5002) tensor(0.4995) 0 (0.65, 0.30000000000000004)\n",
      "tensor(0.0090) tensor(0.4953) tensor(0.5043) 0 (0.65, 0.35000000000000003)\n",
      "tensor(0.0190) tensor(0.4903) tensor(0.5093) 1 (0.65, 0.4)\n",
      "tensor(0.0293) tensor(0.4852) tensor(0.5145) 1 (0.65, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.65, 0.5)\n",
      "tensor(0.0506) tensor(0.4745) tensor(0.5251) 1 (0.65, 0.55)\n",
      "tensor(0.0616) tensor(0.4690) tensor(0.5306) 1 (0.65, 0.6000000000000001)\n",
      "tensor(0.0729) tensor(0.4634) tensor(0.5363) 1 (0.65, 0.65)\n",
      "tensor(0.0845) tensor(0.4576) tensor(0.5420) 1 (0.65, 0.7000000000000001)\n",
      "tensor(0.0963) tensor(0.4517) tensor(0.5479) 1 (0.65, 0.75)\n",
      "tensor(0.1083) tensor(0.4456) tensor(0.5540) 1 (0.65, 0.8)\n",
      "tensor(0.1207) tensor(0.4395) tensor(0.5601) 1 (0.65, 0.8500000000000001)\n",
      "tensor(0.1333) tensor(0.4332) tensor(0.5664) 1 (0.65, 0.9)\n",
      "tensor(0.1461) tensor(0.4267) tensor(0.5728) 1 (0.65, 0.9500000000000001)\n",
      "tensor(0.1592) tensor(0.4202) tensor(0.5794) 1 (0.65, 1.0)\n",
      "tensor(0.0693) tensor(0.5345) tensor(0.4652) 0 (0.7000000000000001, 0.05)\n",
      "tensor(0.0590) tensor(0.5293) tensor(0.4703) 0 (0.7000000000000001, 0.1)\n",
      "tensor(0.0483) tensor(0.5240) tensor(0.4757) 0 (0.7000000000000001, 0.15000000000000002)\n",
      "tensor(0.0371) tensor(0.5184) tensor(0.4813) 0 (0.7000000000000001, 0.2)\n",
      "tensor(0.0254) tensor(0.5125) tensor(0.4871) 0 (0.7000000000000001, 0.25)\n",
      "tensor(0.0133) tensor(0.5065) tensor(0.4932) 0 (0.7000000000000001, 0.30000000000000004)\n",
      "tensor(0.0007) tensor(0.5002) tensor(0.4995) 0 (0.7000000000000001, 0.35000000000000003)\n",
      "tensor(0.0123) tensor(0.4937) tensor(0.5060) 1 (0.7000000000000001, 0.4)\n",
      "tensor(0.0258) tensor(0.4869) tensor(0.5127) 1 (0.7000000000000001, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.7000000000000001, 0.5)\n",
      "tensor(0.0542) tensor(0.4727) tensor(0.5269) 1 (0.7000000000000001, 0.55)\n",
      "tensor(0.0691) tensor(0.4653) tensor(0.5344) 1 (0.7000000000000001, 0.6000000000000001)\n",
      "tensor(0.0845) tensor(0.4576) tensor(0.5420) 1 (0.7000000000000001, 0.65)\n",
      "tensor(0.1003) tensor(0.4497) tensor(0.5499) 1 (0.7000000000000001, 0.7000000000000001)\n",
      "tensor(0.1165) tensor(0.4415) tensor(0.5581) 1 (0.7000000000000001, 0.75)\n",
      "tensor(0.1333) tensor(0.4332) tensor(0.5664) 1 (0.7000000000000001, 0.8)\n",
      "tensor(0.1504) tensor(0.4246) tensor(0.5750) 1 (0.7000000000000001, 0.8500000000000001)\n",
      "tensor(0.1681) tensor(0.4157) tensor(0.5838) 1 (0.7000000000000001, 0.9)\n",
      "tensor(0.1862) tensor(0.4067) tensor(0.5929) 1 (0.7000000000000001, 0.9500000000000001)\n",
      "tensor(0.2048) tensor(0.3974) tensor(0.6022) 1 (0.7000000000000001, 1.0)\n",
      "tensor(0.0908) tensor(0.5452) tensor(0.4545) 0 (0.75, 0.05)\n",
      "tensor(0.0791) tensor(0.5394) tensor(0.4603) 0 (0.75, 0.1)\n",
      "tensor(0.0668) tensor(0.5332) tensor(0.4664) 0 (0.75, 0.15000000000000002)\n",
      "tensor(0.0537) tensor(0.5267) tensor(0.4730) 0 (0.75, 0.2)\n",
      "tensor(0.0399) tensor(0.5198) tensor(0.4799) 0 (0.75, 0.25)\n",
      "tensor(0.0254) tensor(0.5125) tensor(0.4871) 0 (0.75, 0.30000000000000004)\n",
      "tensor(0.0102) tensor(0.5049) tensor(0.4947) 0 (0.75, 0.35000000000000003)\n",
      "tensor(0.0058) tensor(0.4969) tensor(0.5027) 0 (0.75, 0.4)\n",
      "tensor(0.0224) tensor(0.4886) tensor(0.5110) 1 (0.75, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.75, 0.5)\n",
      "tensor(0.0579) tensor(0.4709) tensor(0.5288) 1 (0.75, 0.55)\n",
      "tensor(0.0767) tensor(0.4614) tensor(0.5382) 1 (0.75, 0.6000000000000001)\n",
      "tensor(0.0963) tensor(0.4517) tensor(0.5479) 1 (0.75, 0.65)\n",
      "tensor(0.1165) tensor(0.4415) tensor(0.5581) 1 (0.75, 0.7000000000000001)\n",
      "tensor(0.1375) tensor(0.4310) tensor(0.5685) 1 (0.75, 0.75)\n",
      "tensor(0.1592) tensor(0.4202) tensor(0.5794) 1 (0.75, 0.8)\n",
      "tensor(0.1816) tensor(0.4090) tensor(0.5906) 1 (0.75, 0.8500000000000001)\n",
      "tensor(0.2048) tensor(0.3974) tensor(0.6022) 1 (0.75, 0.9)\n",
      "tensor(0.2286) tensor(0.3855) tensor(0.6141) 1 (0.75, 0.9500000000000001)\n",
      "tensor(0.2532) tensor(0.3732) tensor(0.6264) 1 (0.75, 1.0)\n",
      "tensor(0.1099) tensor(0.5548) tensor(0.4449) 0 (0.8, 0.05)\n",
      "tensor(0.0974) tensor(0.5485) tensor(0.4511) 0 (0.8, 0.1)\n",
      "tensor(0.0839) tensor(0.5418) tensor(0.4579) 0 (0.8, 0.15000000000000002)\n",
      "tensor(0.0693) tensor(0.5345) tensor(0.4652) 0 (0.8, 0.2)\n",
      "tensor(0.0537) tensor(0.5267) tensor(0.4730) 0 (0.8, 0.25)\n",
      "tensor(0.0371) tensor(0.5184) tensor(0.4813) 0 (0.8, 0.30000000000000004)\n",
      "tensor(0.0194) tensor(0.5095) tensor(0.4901) 0 (0.8, 0.35000000000000003)\n",
      "tensor(0.0007) tensor(0.5002) tensor(0.4995) 0 (0.8, 0.4)\n",
      "tensor(0.0190) tensor(0.4903) tensor(0.5093) 1 (0.8, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.8, 0.5)\n",
      "tensor(0.0616) tensor(0.4690) tensor(0.5306) 1 (0.8, 0.55)\n",
      "tensor(0.0845) tensor(0.4576) tensor(0.5420) 1 (0.8, 0.6000000000000001)\n",
      "tensor(0.1083) tensor(0.4456) tensor(0.5540) 1 (0.8, 0.65)\n",
      "tensor(0.1333) tensor(0.4332) tensor(0.5664) 1 (0.8, 0.7000000000000001)\n",
      "tensor(0.1592) tensor(0.4202) tensor(0.5794) 1 (0.8, 0.75)\n",
      "tensor(0.1862) tensor(0.4067) tensor(0.5929) 1 (0.8, 0.8)\n",
      "tensor(0.2142) tensor(0.3927) tensor(0.6069) 1 (0.8, 0.8500000000000001)\n",
      "tensor(0.2433) tensor(0.3781) tensor(0.6214) 1 (0.8, 0.9)\n",
      "tensor(0.2734) tensor(0.3631) tensor(0.6365) 1 (0.8, 0.9500000000000001)\n",
      "tensor(0.3045) tensor(0.3475) tensor(0.6520) 1 (0.8, 1.0)\n",
      "tensor(0.1267) tensor(0.5632) tensor(0.4365) 0 (0.8500000000000001, 0.05)\n",
      "tensor(0.1138) tensor(0.5568) tensor(0.4429) 0 (0.8500000000000001, 0.1)\n",
      "tensor(0.0996) tensor(0.5496) tensor(0.4501) 0 (0.8500000000000001, 0.15000000000000002)\n",
      "tensor(0.0839) tensor(0.5418) tensor(0.4579) 0 (0.8500000000000001, 0.2)\n",
      "tensor(0.0668) tensor(0.5332) tensor(0.4664) 0 (0.8500000000000001, 0.25)\n",
      "tensor(0.0483) tensor(0.5240) tensor(0.4757) 0 (0.8500000000000001, 0.30000000000000004)\n",
      "tensor(0.0284) tensor(0.5140) tensor(0.4856) 0 (0.8500000000000001, 0.35000000000000003)\n",
      "tensor(0.0071) tensor(0.5034) tensor(0.4963) 0 (0.8500000000000001, 0.4)\n",
      "tensor(0.0157) tensor(0.4920) tensor(0.5076) 1 (0.8500000000000001, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.8500000000000001, 0.5)\n",
      "tensor(0.0653) tensor(0.4671) tensor(0.5325) 1 (0.8500000000000001, 0.55)\n",
      "tensor(0.0923) tensor(0.4537) tensor(0.5460) 1 (0.8500000000000001, 0.6000000000000001)\n",
      "tensor(0.1207) tensor(0.4395) tensor(0.5601) 1 (0.8500000000000001, 0.65)\n",
      "tensor(0.1504) tensor(0.4246) tensor(0.5750) 1 (0.8500000000000001, 0.7000000000000001)\n",
      "tensor(0.1816) tensor(0.4090) tensor(0.5906) 1 (0.8500000000000001, 0.75)\n",
      "tensor(0.2142) tensor(0.3927) tensor(0.6069) 1 (0.8500000000000001, 0.8)\n",
      "tensor(0.2482) tensor(0.3757) tensor(0.6239) 1 (0.8500000000000001, 0.8500000000000001)\n",
      "tensor(0.2836) tensor(0.3580) tensor(0.6416) 1 (0.8500000000000001, 0.9)\n",
      "tensor(0.3204) tensor(0.3395) tensor(0.6600) 1 (0.8500000000000001, 0.9500000000000001)\n",
      "tensor(0.3587) tensor(0.3204) tensor(0.6791) 1 (0.8500000000000001, 1.0)\n",
      "tensor(0.1411) tensor(0.5704) tensor(0.4293) 0 (0.9, 0.05)\n",
      "tensor(0.1284) tensor(0.5640) tensor(0.4356) 0 (0.9, 0.1)\n",
      "tensor(0.1138) tensor(0.5568) tensor(0.4429) 0 (0.9, 0.15000000000000002)\n",
      "tensor(0.0974) tensor(0.5485) tensor(0.4511) 0 (0.9, 0.2)\n",
      "tensor(0.0791) tensor(0.5394) tensor(0.4603) 0 (0.9, 0.25)\n",
      "tensor(0.0590) tensor(0.5293) tensor(0.4703) 0 (0.9, 0.30000000000000004)\n",
      "tensor(0.0371) tensor(0.5184) tensor(0.4813) 0 (0.9, 0.35000000000000003)\n",
      "tensor(0.0133) tensor(0.5065) tensor(0.4932) 0 (0.9, 0.4)\n",
      "tensor(0.0123) tensor(0.4937) tensor(0.5060) 1 (0.9, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.9, 0.5)\n",
      "tensor(0.0691) tensor(0.4653) tensor(0.5344) 1 (0.9, 0.55)\n",
      "tensor(0.1003) tensor(0.4497) tensor(0.5499) 1 (0.9, 0.6000000000000001)\n",
      "tensor(0.1333) tensor(0.4332) tensor(0.5664) 1 (0.9, 0.65)\n",
      "tensor(0.1681) tensor(0.4157) tensor(0.5838) 1 (0.9, 0.7000000000000001)\n",
      "tensor(0.2048) tensor(0.3974) tensor(0.6022) 1 (0.9, 0.75)\n",
      "tensor(0.2433) tensor(0.3781) tensor(0.6214) 1 (0.9, 0.8)\n",
      "tensor(0.2836) tensor(0.3580) tensor(0.6416) 1 (0.9, 0.8500000000000001)\n",
      "tensor(0.3258) tensor(0.3369) tensor(0.6627) 1 (0.9, 0.9)\n",
      "tensor(0.3699) tensor(0.3148) tensor(0.6847) 1 (0.9, 0.9500000000000001)\n",
      "tensor(0.4157) tensor(0.2919) tensor(0.7076) 1 (0.9, 1.0)\n",
      "tensor(0.1533) tensor(0.5765) tensor(0.4232) 0 (0.9500000000000001, 0.05)\n",
      "tensor(0.1411) tensor(0.5704) tensor(0.4293) 0 (0.9500000000000001, 0.1)\n",
      "tensor(0.1267) tensor(0.5632) tensor(0.4365) 0 (0.9500000000000001, 0.15000000000000002)\n",
      "tensor(0.1099) tensor(0.5548) tensor(0.4449) 0 (0.9500000000000001, 0.2)\n",
      "tensor(0.0908) tensor(0.5452) tensor(0.4545) 0 (0.9500000000000001, 0.25)\n",
      "tensor(0.0693) tensor(0.5345) tensor(0.4652) 0 (0.9500000000000001, 0.30000000000000004)\n",
      "tensor(0.0455) tensor(0.5226) tensor(0.4771) 0 (0.9500000000000001, 0.35000000000000003)\n",
      "tensor(0.0194) tensor(0.5095) tensor(0.4901) 0 (0.9500000000000001, 0.4)\n",
      "tensor(0.0090) tensor(0.4953) tensor(0.5043) 0 (0.9500000000000001, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (0.9500000000000001, 0.5)\n",
      "tensor(0.0729) tensor(0.4634) tensor(0.5363) 1 (0.9500000000000001, 0.55)\n",
      "tensor(0.1083) tensor(0.4456) tensor(0.5540) 1 (0.9500000000000001, 0.6000000000000001)\n",
      "tensor(0.1461) tensor(0.4267) tensor(0.5728) 1 (0.9500000000000001, 0.65)\n",
      "tensor(0.1862) tensor(0.4067) tensor(0.5929) 1 (0.9500000000000001, 0.7000000000000001)\n",
      "tensor(0.2286) tensor(0.3855) tensor(0.6141) 1 (0.9500000000000001, 0.75)\n",
      "tensor(0.2734) tensor(0.3631) tensor(0.6365) 1 (0.9500000000000001, 0.8)\n",
      "tensor(0.3204) tensor(0.3395) tensor(0.6600) 1 (0.9500000000000001, 0.8500000000000001)\n",
      "tensor(0.3699) tensor(0.3148) tensor(0.6847) 1 (0.9500000000000001, 0.9)\n",
      "tensor(0.4216) tensor(0.2889) tensor(0.7105) 1 (0.9500000000000001, 0.9500000000000001)\n",
      "tensor(0.4757) tensor(0.2619) tensor(0.7376) 1 (0.9500000000000001, 1.0)\n",
      "tensor(0.1631) tensor(0.5814) tensor(0.4183) 0 (1.0, 0.05)\n",
      "tensor(0.1520) tensor(0.5759) tensor(0.4238) 0 (1.0, 0.1)\n",
      "tensor(0.1381) tensor(0.5689) tensor(0.4308) 0 (1.0, 0.15000000000000002)\n",
      "tensor(0.1213) tensor(0.5605) tensor(0.4392) 0 (1.0, 0.2)\n",
      "tensor(0.1017) tensor(0.5507) tensor(0.4490) 0 (1.0, 0.25)\n",
      "tensor(0.0791) tensor(0.5394) tensor(0.4603) 0 (1.0, 0.30000000000000004)\n",
      "tensor(0.0537) tensor(0.5267) tensor(0.4730) 0 (1.0, 0.35000000000000003)\n",
      "tensor(0.0254) tensor(0.5125) tensor(0.4871) 0 (1.0, 0.4)\n",
      "tensor(0.0058) tensor(0.4969) tensor(0.5027) 0 (1.0, 0.45)\n",
      "tensor(0.0398) tensor(0.4799) tensor(0.5197) 1 (1.0, 0.5)\n",
      "tensor(0.0767) tensor(0.4614) tensor(0.5382) 1 (1.0, 0.55)\n",
      "tensor(0.1165) tensor(0.4415) tensor(0.5581) 1 (1.0, 0.6000000000000001)\n",
      "tensor(0.1592) tensor(0.4202) tensor(0.5794) 1 (1.0, 0.65)\n",
      "tensor(0.2048) tensor(0.3974) tensor(0.6022) 1 (1.0, 0.7000000000000001)\n",
      "tensor(0.2532) tensor(0.3732) tensor(0.6264) 1 (1.0, 0.75)\n",
      "tensor(0.3045) tensor(0.3475) tensor(0.6520) 1 (1.0, 0.8)\n",
      "tensor(0.3587) tensor(0.3204) tensor(0.6791) 1 (1.0, 0.8500000000000001)\n",
      "tensor(0.4157) tensor(0.2919) tensor(0.7076) 1 (1.0, 0.9)\n",
      "tensor(0.4757) tensor(0.2619) tensor(0.7376) 1 (1.0, 0.9500000000000001)\n",
      "tensor(0.5385) tensor(0.2305) tensor(0.7690) 1 (1.0, 1.0)\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ],
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error"
    },
    {
     "name": "stderr",
     "text": [
      "/home/weiwen/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3327: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    " \n",
    "\n",
    "class BinarizeF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(cxt, input):\n",
    "        output = input.new(input.size())\n",
    "        output[input >= 0] = 1\n",
    "        output[input < 0] = -1\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(cxt, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input\n",
    "# aliases\n",
    "binarize = BinarizeF.apply\n",
    "\n",
    "\n",
    "class ClipF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = input.clone().detach()\n",
    "        # output = input.new(input.size())\n",
    "        output[input >= 1] = 1\n",
    "        output[input <= 0] = 0\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input >= 1] = 0\n",
    "        grad_input[input <= 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# aliases\n",
    "clipfunc = ClipF.apply\n",
    "\n",
    "\n",
    "class BinaryLinear(nn.Linear):\n",
    "\n",
    "\n",
    "    def do_slp_via_th(self,input_ori,w_ori):\n",
    "        p = input_ori\n",
    "        d = 4*p*(1-p)\n",
    "        e = (2*p-1)\n",
    "        # e_sq = torch.tensor(1)\n",
    "        w = w_ori\n",
    "\n",
    "        sum_of_sq = (d+e.pow(2)).sum(-1)\n",
    "        sum_of_sq = sum_of_sq.unsqueeze(-1)        \n",
    "        sum_of_sq = sum_of_sq.expand(p.shape[0], w.shape[0])\n",
    "\n",
    "        diag_p = torch.diag_embed(e)        \n",
    "\n",
    "        p_w = torch.matmul(w,diag_p)\n",
    "\n",
    "        z_p_w = torch.zeros_like(p_w)        \n",
    "        shft_p_w = torch.cat((p_w, z_p_w), -1)\n",
    "\n",
    "        sum_of_cross = torch.zeros_like(p_w)\n",
    "        length = p.shape[1]    \n",
    "\n",
    "        for shft in range(1,length):    \n",
    "            sum_of_cross += shft_p_w[:,:,0:length]*shft_p_w[:,:,shft:length+shft]\n",
    "\n",
    "        sum_of_cross = sum_of_cross.sum(-1)\n",
    "\n",
    "        return (sum_of_sq+2*sum_of_cross)/(length**2) \n",
    "\n",
    "    def forward(self, input):        \n",
    "        binary_weight = binarize(self.weight)        \n",
    "        if self.bias is None:\n",
    "            return self.do_slp_via_th(input,binary_weight)\n",
    "\n",
    "        else:   \n",
    "\n",
    "            bias_one  = torch.ones(input.shape[0],1)            \n",
    "            new_input = torch.cat((input, bias_one), -1)            \n",
    "            bias = clipfunc(self.bias).unsqueeze(1)            \n",
    "            new_weight = binary_weight            \n",
    "            new_weight = torch.cat((new_weight,bias),-1)                        \n",
    "            return self.do_slp_via_th(new_input,new_weight)\n",
    "\n",
    "\n",
    "            torch.set_printoptions(edgeitems=64)\n",
    "            # binary_bias = binarize(self.bias)/float(len(input[0].flatten())+1)\n",
    "            binary_bias = binarize(self.bias)/float(len(input[0].flatten())+1)\n",
    "            res = F.linear(input, binary_weight/float(len(input[0].flatten())+1), binary_bias)\n",
    "            return res\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Glorot initialization\n",
    "        in_features, out_features = self.weight.size()\n",
    "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "        self.weight.lr_scale = 1. / stdv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm(nn.Module):\n",
    "    def __init__(self, num_features, init_ang_inc = 10, momentum=0.1):        \n",
    "        super(QC_Norm, self).__init__()\n",
    "        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)        \n",
    "        self.ang_inc = Parameter(torch.ones(1)*init_ang_inc)\n",
    "        \n",
    "        self.momentum = momentum\n",
    "                \n",
    "        self.printed = False\n",
    "        self.x_mean_ancle=0\n",
    "        self.x_mean_rote = 0\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                print(\"self.ang_inc\",self.ang_inc)\n",
    "                self.printed = True\n",
    "                    \n",
    "            x = x.transpose(0,1)\n",
    "  \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)\n",
    "            x_1 = (x_final.cos()+1)/2\n",
    "                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            # print(x_1)\n",
    "        else:\n",
    "            self.printed = False\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "            x_lack_sum = x_sum - x    \n",
    "            x_mean = x_lack_sum/x.shape[-1]\n",
    "            \n",
    "            # print(x)\n",
    "            # print(x_sum)\n",
    "            # print(x_lack_sum)\n",
    "            # print(x_mean)\n",
    "            # \n",
    "                    \n",
    "            x_mean_ancle = (x_mean*2-1).acos()  \n",
    "            \n",
    "            ang_inc = self.ang_inc.unsqueeze(-1).expand(x_mean_ancle.shape) \n",
    "            # ang_inc = np.pi/2/(x.max(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) -x.min(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) )\n",
    "            \n",
    "            if given_ang!=-1:\n",
    "                x_mean_rote = (np.pi/2 - x_mean_ancle)*given_ang\n",
    "            else:\n",
    "                x_mean_rote = (np.pi/2 - x_mean_ancle)*ang_inc\n",
    "            \n",
    "            x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot\n",
    "                                                \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+x_mean_rote  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "      \n",
    "        return x_1\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        self.ang_inc.data.zeros_()\n",
    "        \n",
    "def print_degree(x,name=\"x\"):\n",
    "    print(name,x/np.pi*180)\n",
    "    \n",
    "    \n",
    "class QC_Norm_Real(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Real, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.x_max = 0\n",
    "        self.x_min = 0\n",
    "        # print(\"Using Normal without real\")\n",
    "        \n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            # x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_final = ((x_ancle-self.x_min)/(self.x_max-self.x_min))*np.pi\n",
    "            \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            x = x.transpose(0,1)        \n",
    "            x_ancle = (x*2-1).acos()     \n",
    "            x_rectify_ancle = (x_ancle.max(-1)[0]-x_ancle.min(-1)[0]).unsqueeze(-1).expand(x.shape)                                                                         \n",
    "            x_final = ((x_ancle-x_ancle.min(-1)[0].unsqueeze(-1))/(x_rectify_ancle))*np.pi\n",
    "            \n",
    "            x_moving_rot = x_final - x_ancle\n",
    "            \n",
    "            x_moving_rot_mean = x_moving_rot.sum(-1)/x.shape[-1] \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot_mean      \n",
    "            \n",
    "            self.x_max = self.momentum * x_ancle.max(-1)[0].unsqueeze(-1) + \\\n",
    "                                    (1 - self.momentum) * self.x_max\n",
    "            self.x_min = self.momentum * x_ancle.min(-1)[0].unsqueeze(-1) + \\\n",
    "                                    (1 - self.momentum) * self.x_min\n",
    "            \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "            \n",
    "        return x_1\n",
    "\n",
    "\n",
    "class QC_Norm_Real_Correction(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Real_Correction, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:            \n",
    "            \n",
    "            x = x.transpose(0,1)                    \n",
    "            x_ancle = (x*2-1).acos()                        \n",
    "            x_moving_rot = -1*(x_ancle.min(-1)[0])\n",
    "            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot                                                    \n",
    "            x_final = x_ancle+x_moving_rot.unsqueeze(-1)                                    \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        return x_1\n",
    "\n",
    "class QC_Norm_Correction(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Correction, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)                \n",
    "            x_mean = x_sum/x.shape[-1]\n",
    "                                \n",
    "            x_mean_ancle = (x_mean*2-1).acos()    \n",
    "            x_mean_rote = (np.pi/2 - x_mean_ancle) \n",
    "            \n",
    "            x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])\n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot                                        \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+x_mean_rote  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "        \n",
    "        return x_1\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm_True(nn.Module):\n",
    "    def __init__(self,num_features):        \n",
    "        super(QC_Norm_True, self).__init__()\n",
    "        \n",
    "        x_r_init =  torch.zeros(num_features)\n",
    "        x_r_init += 0\n",
    "        \n",
    "        self.x_qft = Parameter(torch.zeros(num_features),requires_grad=True)\n",
    "        self.x_hzh = Parameter(torch.zeros(num_features),requires_grad=True)\n",
    "        \n",
    "        self.x_running_rot = Parameter(x_r_init,requires_grad=True)\n",
    "        \n",
    "        self.x_running_means = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.x_running_min = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.x_running_max = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        \n",
    "        self.printed = False\n",
    "        self.momentum = 0.1\n",
    "    def forward(self,x,training=True):\n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                # print(self.x_qft)\n",
    "                # print(self.x_hzh)\n",
    "                print(self.x_running_rot)\n",
    "\n",
    "                # print(((binarize(self.x_qft)+1)/2)*0.5)\n",
    "                # print(((binarize(self.x_qft)+1)/2)*0.5*x)\n",
    "                # print(((1-x)-x)*self.x_running_rot + x)\n",
    "                self.printed = True\n",
    "            x_min_rt = self.x_running_min.detach()\n",
    "            x_max_rt = self.x_running_max.detach()\n",
    "        else:\n",
    "            self.printed = False           \n",
    "            # x_running_rot = self.x_running_rot    \n",
    "            x = x.transpose(0,1)\n",
    "            # x_sum = x.sum(-1)\n",
    "            # x_mean = x_sum/x.shape[-1]\n",
    "            x_min = x.min(1)[0]\n",
    "            x_max = x.max(1)[0]            \n",
    "            x = x.transpose(0,1)\n",
    "            self.x_running_min[:] = self.momentum * self.x_running_min + \\\n",
    "                                (1 - self.momentum) * x_min \n",
    "            self.x_running_max[:] = self.momentum * self.x_running_max + \\\n",
    "                                (1 - self.momentum) * x_max\n",
    "            x_min_rt = x_min\n",
    "            x_max_rt = x_max\n",
    "        \n",
    "        x = (x_min_rt*(1-2*x)+x)\n",
    "            \n",
    "        # x = x+x-2*x*x\n",
    "        \n",
    "        # ry\n",
    "        # init_rot = (x<0.5).float()*np.pi        \n",
    "        # final_rot = init_rot + self.x_running_rot                 \n",
    "        # r = (2*x-1).abs()        \n",
    "        # x = (r*final_rot.cos()+1)/2 \n",
    "        \n",
    "        return x\n",
    "        # QFT\n",
    "        # \n",
    "        # qft_g_10 = (binarize(self.x_qft-10)+1)/2\n",
    "        # qft_g_5 = (binarize(self.x_qft-5)+1)/2 - qft_g_10\n",
    "        # qft_g_0 = (binarize(self.x_qft-0)+1)/2 - qft_g_5 - qft_g_10\n",
    "        # x = qft_g_10*0.038*x + (1-qft_g_10)*x\n",
    "        # x = qft_g_5*0.146*x + (1-qft_g_5)*x\n",
    "        # x = qft_g_0*0.5*x + (1-qft_g_0)*x\n",
    "        \n",
    "        # x = ((binarize(self.x_qft)+1)/2*0.5)*x\n",
    "        # x = (((binarize(self.x_qft-5)+1)/2 - (binarize(self.x_qft-10)+1)/2)*0.146)*x\n",
    "        # x = (((binarize(self.x_qft-0)+1)/2 - (binarize(self.x_qft-5)+1)/2 - (binarize(self.x_qft-10)+1)/2)*0.5)*x\n",
    "        \n",
    "        \n",
    "        # x = (x+(binarize(self.x_hzh)+1)/2*(1-2*x))\n",
    "        # \n",
    "        # x = (((binarize(self.x_qft)+1)/2)*0.5)*x + (((1-(binarize(self.x_qft)+1)/2))*1)*x \n",
    "        # x = (((binarize(self.x_qft-10)+1)/2)*0.1)*x + (((1-(binarize(self.x_qft-10)+1)/2))*1)*x\n",
    "        # \n",
    "        # # hzh <- y\n",
    "        # x = ((1-x)-x)*clipfunc(self.x_hzh) + x        \n",
    "        # \n",
    "        # \n",
    "        # # ry\n",
    "        # init_rot = (x<0.5).float()*np.pi        \n",
    "        # final_rot = init_rot + self.x_running_rot                 \n",
    "        # r = (2*x-1).abs()        \n",
    "        # x1 = (r*final_rot.cos()+1)/2   \n",
    "\n",
    "\n",
    "            \n",
    "        # if not training:\n",
    "        #     \n",
    "        #     # init_rot = (x<0.5).float()*np.pi        \n",
    "        #     # final_rot = init_rot + self.x_running_rot                 \n",
    "        #     # r = (2*x-1).abs()        \n",
    "        #     # x = (r*final_rot.cos()+1)/2   \n",
    "        #     # \n",
    "        #     # x = 1-(x+self.x_running_means-2*x*self.x_running_means)\n",
    "        #     # x = (1-x)/2\n",
    "        #     # x = self.x_running_means*(1-2*x)+x\n",
    "        #     # \n",
    "        #     # if not self.printed:\n",
    "        #     #     print(\"self.x_running_means\",self.x_running_means)\n",
    "        #     #     self.printed = True\n",
    "        #     #     # print(x)\n",
    "        #     return x\n",
    "        # \n",
    "        # else:\n",
    "        #     self.printed = False\n",
    "            \n",
    "            # # ry\n",
    "            # init_rot = (x<0.5).float()*np.pi        \n",
    "            # final_rot = init_rot + self.x_running_rot                 \n",
    "            # r = (2*x-1).abs()        \n",
    "            # x = (r*final_rot.cos()+1)/2   \n",
    "\n",
    "            \n",
    "            \n",
    "            # x = x.transpose(0,1)\n",
    "            # x_sum = x.sum(-1)\n",
    "            # \n",
    "            # # self.x_running_rot = self.x_running_rot.clamp(0,1) \n",
    "            # \n",
    "            # x_mean = x_sum/x.shape[-1]\n",
    "            # \n",
    "            #          \n",
    "            # #          *self.x_hzh+self.x_running_rot/10\n",
    "            # # x_mean = x_mean.clamp(0,1)        \n",
    "            # x = x.transpose(0,1)\n",
    "            #     \n",
    "            \n",
    "            \n",
    "        #     return x\n",
    "        #     \n",
    "        #     \n",
    "        #     sys.exit(0)\n",
    "        #     x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "        #     x_lack_sum = x_sum - x    \n",
    "        #     x_mean = x_lack_sum/x.shape[-1]\n",
    "        #     \n",
    "        #     \n",
    "        #             \n",
    "        #     x_mean_ancle = (x_mean*2-1).acos()  \n",
    "        #     \n",
    "        #     ang_inc = self.ang_inc.unsqueeze(-1).expand(x_mean_ancle.shape) \n",
    "        #     # ang_inc = np.pi/2/(x.max(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) -x.min(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) )\n",
    "        #     \n",
    "        #     if given_ang!=-1:\n",
    "        #         x_mean_rote = (np.pi/2 - x_mean_ancle)*given_ang\n",
    "        #     else:\n",
    "        #         x_mean_rote = (np.pi/2 - x_mean_ancle)*ang_inc\n",
    "        #     \n",
    "        #     x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])            \n",
    "        #     self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "        #                           (1 - self.momentum) * x_moving_rot\n",
    "        #                                         \n",
    "        #     x_ancle = (x*2-1).acos()\n",
    "        #     x_final = x_ancle+x_mean_rote  \n",
    "        #     x_1 = (x_final.cos()+1)/2                                \n",
    "        #     x_1 = x_1.transpose(0,1)\n",
    "        # return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm_try2(nn.Module):\n",
    "    def __init__(self, num_features, batch_size=32, init_ang_inc = 0, momentum=0.1):        \n",
    "        super(QC_Norm_try2, self).__init__()\n",
    "        \n",
    "        self.x_running_rot = Parameter(torch.zeros((num_features,batch_size)),requires_grad=False)        \n",
    "        self.ang_inc = Parameter(torch.ones(1)*init_ang_inc)\n",
    "        \n",
    "        self.momentum = momentum\n",
    "                \n",
    "        self.printed = False\n",
    "        self.x_mean_ancle=0\n",
    "        self.x_mean_rote = 0\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "        \n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                print(\"self.x_running_rot\",self.x_running_rot)\n",
    "                self.printed = True\n",
    "                    \n",
    "            x = x.transpose(0,1)\n",
    "            x_1  = (self.x_running_rot*(1-x)+x)  \n",
    "            x_1 = x_1.transpose(0,1) \n",
    "            # x_ancle = (x*2-1).acos()\n",
    "            # x_final = x_ancle+self.x_running_rot.unsqueeze(-1)\n",
    "            # x_1 = (x_final.cos()+1)/2\n",
    "            #                     \n",
    "            # x_1 = x_1.transpose(0,1)\n",
    "            # print(x_1)\n",
    "        else:\n",
    "            self.printed = False\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "            x_lack_sum = x_sum - x    \n",
    "            x_mean = x_lack_sum/x.shape[-1]\n",
    "            \n",
    "            \n",
    "                                  \n",
    "            # ang_inc = self.ang_inc.unsqueeze(-1).expand(x.shape)    \n",
    "            \n",
    "            y  = ((0.5-x_mean)/(1-x_mean))\n",
    "            # y = clipfunc(y)\n",
    "            \n",
    "            # print(y)\n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * y\n",
    "\n",
    "            # self.x_running_rot[:] = y\n",
    "            \n",
    "            x_1  = (y*(1-x)+x)\n",
    "            x_1 = x_1.transpose(0,1) \n",
    "            # x_1  = (y*(1-x)+x)\n",
    "            # sys.exit(0)\n",
    "            # \n",
    "            # \n",
    "            # x_mean_ancle = (x_mean*2-1).acos()\n",
    "            # ang_inc = self.ang_inc.unsqueeze(-1).expand(x_mean_ancle.shape)             \n",
    "            # \n",
    "            # if given_ang!=-1:\n",
    "            #     x_mean_rote = (np.pi/2 - x_mean_ancle)*given_ang\n",
    "            # else:\n",
    "            #     x_mean_rote = (np.pi/2 - x_mean_ancle)*ang_inc\n",
    "            # \n",
    "            # x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])            \n",
    "            # self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "            #                       (1 - self.momentum) * x_moving_rot\n",
    "            #                                     \n",
    "            # x_ancle = (x*2-1).acos()\n",
    "            # x_final = x_ancle+x_mean_rote  \n",
    "            # x_1 = (x_final.cos()+1)/2                                \n",
    "            # x_1 = x_1.transpose(0,1)\n",
    "      \n",
    "        return x_1\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        self.ang_inc.data.zeros_()\n",
    "\n",
    "\n",
    "class QC_Norm_try3(nn.Module):\n",
    "    def __init__(self, num_features, batch_size=32, init_ang_inc = 1, momentum=0.1):        \n",
    "        super(QC_Norm_try3, self).__init__()\n",
    "        \n",
    "        self.x_running_rot = Parameter(torch.zeros((num_features)),requires_grad=False)        \n",
    "        # self.ang_inc = Parameter(torch.ones((num_features)),requires_grad=True)\n",
    "        self.ang_inc = Parameter(torch.tensor(init_ang_inc,dtype=torch.float32),requires_grad=True)        \n",
    "        self.momentum = momentum\n",
    "                \n",
    "        self.printed = False\n",
    "        self.x_mean_ancle=0\n",
    "        self.x_mean_rote = 0\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "        \n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                # print(\"self.ang_inc\",self.ang_inc)\n",
    "                self.printed = True                            \n",
    "            x_1  = (self.x_running_rot*x)               \n",
    "     \n",
    "        else:\n",
    "            self.printed = False\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "            x_lack_sum = x_sum + x    \n",
    "            x_mean = x_lack_sum/x.shape[-1]                    \n",
    "            \n",
    "            \n",
    "            \n",
    "            ang_inc = (self.ang_inc>0).float()*self.ang_inc+1\n",
    "            # ang_inc = self.ang_inc.abs()+1\n",
    "            # ang_inc = 2\n",
    "            y  = 0.5/x_mean\n",
    "            y = y.transpose(0,1)\n",
    "            y = y/ang_inc\n",
    "            y = y.transpose(0,1)\n",
    "    \n",
    "            x_moving_rot = (y.sum(-1)/x.shape[-1]) \n",
    "            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot\n",
    "            \n",
    "            x_1  = y*x\n",
    "            x_1 = x_1.transpose(0,1) \n",
    "            \n",
    "        return x_1\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        self.ang_inc.data.zeros_()\n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm_Correction_try2(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Correction_try2, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        self.x_l_0_5 = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.x_g_0_5 = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:            \n",
    "            x_1  = self.x_l_0_5*(self.x_running_rot*(1-x)+x)\n",
    "            x_1 += self.x_g_0_5*(self.x_running_rot*x)            \n",
    "        else:\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1)            \n",
    "            x_mean = x_sum/x.shape[-1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.x_l_0_5[:] = ((x_mean<=0.5).float())\n",
    "            self.x_g_0_5[:] = ((x_mean>0.5).float())\n",
    "            \n",
    "            y  = self.x_l_0_5*((0.5-x_mean)/(1-x_mean))\n",
    "            y += self.x_g_0_5*(0.5/x_mean)\n",
    "            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * y\n",
    "            \n",
    "            \n",
    "            x = x.transpose(0,1)  \n",
    "            x_1  = self.x_l_0_5*(y*(1-x)+x)\n",
    "            x_1 += self.x_g_0_5*(y*x)\n",
    "            \n",
    "        return x_1\n",
    "    \n",
    "\n",
    "class BinaryLinearClassic(nn.Linear):\n",
    "\n",
    "    def forward(self, input):\n",
    "        binary_weight = binarize(self.weight)\n",
    "        if self.bias is None:\n",
    "            output = F.linear(input, binary_weight)\n",
    "            output = torch.div(output, input.shape[-1])\n",
    "            output = torch.pow(output, 2)\n",
    "\n",
    "            return output\n",
    "        else:\n",
    "            print(\"Not Implement\")\n",
    "            sys.exit(0)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Glorot initialization\n",
    "        in_features, out_features = self.weight.size()\n",
    "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "        self.weight.lr_scale = 1. / stdv\n",
    "\n",
    "\n",
    "\n",
    "## Define the NN architecture\n",
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc1 = BinaryLinear(2,num_f1,bias=False)\n",
    "        self.fc2 = BinaryLinear(num_f1,num_f2,bias=False)\n",
    "        # self.fc3 = BinaryLinear(num_f2,num_f3,bias=False)\n",
    "        # # # \n",
    "        # \n",
    "        # # if not with_norm or True:\n",
    "        # self.clc_fc1 = BinaryLinearClassic(in_features=2, out_features=num_f1, bias=False)            \n",
    "        # self.clc_fc2 = BinaryLinearClassic(in_features=num_f1, out_features=num_f2, bias=False)\n",
    "        # self.clc_fc3 = BinaryLinearClassic(in_features=num_f2, out_features=num_f3, bias=False)\n",
    "        # \n",
    "        # \n",
    "        # self.clc_bn1 = nn.BatchNorm1d(num_features=num_f1)\n",
    "        # self.clc_bn2 = nn.BatchNorm1d(num_features=num_f2)\n",
    "        if with_norm:\n",
    "            self.qc1 = QC_Norm_try3(num_features=num_f1,init_ang_inc=[1,1])\n",
    "            self.qc2 = QC_Norm_try3(num_features=num_f2,init_ang_inc=[1,1])\n",
    "            # self.qc3 = QC_Norm(num_features=num_f3)\n",
    "    \n",
    "            # self.qc1a = QC_Norm_True(num_features=num_f1)\n",
    "            # self.qc2a = QC_Norm_True(num_features=num_f2)\n",
    "            # self.qc3a = QC_Norm_Correction(num_features=num_f3)\n",
    "            \n",
    "            self.qc1a = QC_Norm_Correction_try2(num_features=num_f1)\n",
    "            self.qc2a = QC_Norm_Correction_try2(num_features=num_f2)\n",
    "            \n",
    "            # \n",
    "            # self.qc1 = QC_Norm_Real(num_features=num_f1)\n",
    "            # self.qc2 = QC_Norm_Real(num_features=num_f2)\n",
    "            # self.qc3 = QC_Norm_Real(num_features=num_f3)\n",
    "\n",
    "\n",
    "        # self.qc1a = QC_Norm_Real_Correction(num_features=num_f1)\n",
    "        # self.qc2a = QC_Norm_Real_Correction(num_features=num_f2)\n",
    "        # self.qc3a = QC_Norm_Real_Correction(num_features=num_f3)\n",
    "        # \n",
    "    def forward(self, x, training=1):        \n",
    "        x = x.view(-1, 2)\n",
    "        \n",
    "        if training == 1:\n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            # \n",
    "            \n",
    "\n",
    "            if with_norm:\n",
    "                # x = self.qc1a(self.fc1(x))                \n",
    "                # x = self.qc2a(self.fc2(x))  \n",
    "                x = self.qc1(self.qc1a(self.fc1(x)))\n",
    "                x = (self.qc2a(self.fc2(x)))\n",
    "                \n",
    "                # print(x)\n",
    "                # x = self.fc1(x)\n",
    "                # print(x)\n",
    "                # x = self.qc1a(x)\n",
    "                # print(x)\n",
    "                # x = self.qc1(x)\n",
    "                # print(x)\n",
    "                # \n",
    "                # print(x)\n",
    "                # x = self.fc2(x)\n",
    "                # print(x)\n",
    "                # x = self.qc2a(x)\n",
    "                # print(x)\n",
    "                # x = self.qc2(x)\n",
    "                # print(x)\n",
    "                \n",
    "                # print(x)\n",
    "                # sys.exit(0)\n",
    "            else:\n",
    "                x = self.clc_fc1(x)        \n",
    "                x = self.clc_fc2(x)\n",
    "                # x = self.clc_fc3(x)  \n",
    "\n",
    "            # x = self.qc3(self.qc3a(self.fc3(x)))\n",
    "            # \n",
    "            # x = self.qc1((self.fc1(x)))        \n",
    "            # x = self.qc2((self.fc2(x)))                           \n",
    "            # x = self.qc3((self.fc3(x)))\n",
    "            # \n",
    "        elif training == 2:\n",
    "            \n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            \n",
    "            print(\"=\"*10,\"layer 1\",\"=\"*10)\n",
    "            print(x)\n",
    "            torch.set_printoptions(profile=\"full\")            \n",
    "            print(binarize(self.fc1.weight))                                            \n",
    "            torch.set_printoptions(profile=\"default\")\n",
    "            x = self.fc1(x)            \n",
    "            \n",
    "            print(\"=\"*10,\"layer 2\",\"=\"*10)\n",
    "            print(x)\n",
    "            torch.set_printoptions(profile=\"full\")            \n",
    "            print(binarize(self.fc2.weight))                                            \n",
    "            torch.set_printoptions(profile=\"default\")\n",
    "            x = self.fc2(x)\n",
    "            \n",
    "            print(\"=\"*10,\"results\",\"=\"*10)\n",
    "            print(x)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # x = self.qc1(self.fc1(x),training=False)\n",
    "            #     x = self.qc2(self.fc2(x),training=False)\n",
    "            # x = self.qc3(self.fc3(x),training=False)\n",
    "            # \n",
    "            \n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            # \n",
    "            # x = self.qc3(self.qc3a(self.fc3(x),training=False),training=False)\n",
    "            \n",
    "            if with_norm:                \n",
    "                if not print_detail:\n",
    "                    x = self.qc1(self.qc1a(self.fc1(x),training=False),training=False)                \n",
    "                    x = (self.qc2a(self.fc2(x),training=False))\n",
    "                else:\n",
    "                    print(x)\n",
    "                    x = self.fc1(x)\n",
    "                    print(x)\n",
    "                    x = self.qc1a(x,training=False)\n",
    "                    print(x)\n",
    "                    x = self.qc1(x,training=False)\n",
    "                    print(x)\n",
    "    \n",
    "                    print(x)\n",
    "                    x = self.fc2(x)\n",
    "                    print(x)\n",
    "                    x = self.qc2a(x,training=False)\n",
    "                    print(x)\n",
    "                    # x = self.qc2(x,training=False)\n",
    "                    # print(x)\n",
    "                    # \n",
    "                    # x = self.qc1a(self.fc1(x),training=False)                \n",
    "                    # x = self.qc2a(self.fc2(x),training=False)                                        \n",
    "            else:\n",
    "                x = self.clc_fc1(x)                    \n",
    "                x = self.clc_fc2(x)\n",
    "                # x = self.clc_fc3(x)  \n",
    "            \n",
    "        if num_f2==1 or num_f1==1:            \n",
    "            x = torch.cat((x,1-x),-1)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    epoch_loss = []\n",
    "    batch_idx = 0\n",
    "    for (data, target) in dataset:\n",
    "        \n",
    "        \n",
    "        target = target.view(batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # target,new_target = modify_target(target)\n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        # \n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data,True)\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()    \n",
    "        \n",
    "        # print(output.shape,target.shape)\n",
    "        \n",
    "        # sys.exit(0)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "                \n",
    "        if batch_idx % 20 == 0:        \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}/{} ({:.2f}%)'.format(\n",
    "                epoch, batch_idx * len(data), len(dataset),\n",
    "                100. * batch_idx / len(dataset), loss, correct, (batch_idx+1) * len(data),\n",
    "                100. * float(correct) / float(((batch_idx+1) * len(data)) )))                \n",
    "        batch_idx += 1\n",
    "    print(\"-\"*20,\"training done, loss\",\"-\"*20)\n",
    "    print(\"Training Set: Average loss: {}\".format(round(sum(epoch_loss)/len(epoch_loss),6)))\n",
    "    \n",
    "accur=[]\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in dataset:\n",
    "        # target,new_target = modify_target(target)\n",
    "        target = target.view(batch_size)\n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        \n",
    "        # print(\"=\"*100)        \n",
    "        # print(data)\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # print(\"Debug\")\n",
    "        # output = model(data,2)\n",
    "        # \n",
    "        \n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data,False)\n",
    "        \n",
    "        # \n",
    "        test_loss += criterion(output, target) # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get t\n",
    "        # print(pred.view(batch_size))# he index of the max log-probability\n",
    "        # print(target,output)\n",
    "        # sys.exit(0)\n",
    "        # print(pred.eq(target.data.view_as(pred)).cpu().sum())\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    a=100.*correct / len(dataset)*batch_size\n",
    "    accur.append(a)  \n",
    "    test_loss /= len(dataset)*batch_size\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(dataset)*batch_size,\n",
    "        100. * float(correct) / float(len(dataset)*batch_size)))\n",
    "    \n",
    "    return float(correct) / (len(dataset)*batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Training\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "print(\"=\"*10,\"Model Info\",\"=\"*10)\n",
    "print(model)\n",
    "# summary(model,(1,img_size,img_size))\n",
    "\n",
    "# \n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "if with_norm and given_ang==-1:\n",
    "    optimizer = torch.optim.Adam([\n",
    "                    {'params': model.fc1.parameters()},\n",
    "                    {'params': model.fc2.parameters()},\n",
    "                    # {'params': model.fc3.parameters()},\n",
    "                    {'params': model.qc1.parameters(), 'lr': init_qc_lr},\n",
    "                    {'params': model.qc2.parameters(), 'lr': init_qc_lr},\n",
    "                    # {'params': model.qc3.parameters(), 'lr': 1},\n",
    "                ], lr=init_lr)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# optimizer = torch.optim.SGD([\n",
    "#                 {'params': model.fc1.parameters()},\n",
    "#                 {'params': model.fc2.parameters()},\n",
    "#                 {'params': model.fc3.parameters()},\n",
    "#                 {'params': model.qc1.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc2.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc3.parameters(), 'lr': 1},\n",
    "#             ], lr=0.1, momentum=0.9)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \\\n",
    "#         base_lr=[1e-1,1e-1,1e-1,1,1,1], \\\n",
    "#         max_lr=[1e-3,1e-3,1e-3,1e-2,1e-2,1e-2], \\\n",
    "#         step_size_up=100\n",
    "#         )\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)\n",
    "\n",
    "# \n",
    "# \n",
    "# test()\n",
    "# \n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile(resume_path):\n",
    "    print(\"=> loading checkpoint from '{}'<=\".format(resume_path))\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    epoch_init,acc = checkpoint[\"epoch\"],checkpoint[\"acc\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])    \n",
    "    scheduler.milestones = Counter(milestones)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "else:\n",
    "    epoch_init,acc = 0,0\n",
    "\n",
    "# for name,para in model.named_parameters():\n",
    "#     print(name,para)\n",
    "\n",
    "# sys.exit(0)\n",
    "import copy\n",
    "best_model = -1\n",
    "if training:\n",
    "    for epoch in range(epoch_init, max_epoch + 1):\n",
    "        print(\"=\"*20,epoch,\"epoch\",\"=\"*20)  \n",
    "        print(\"Epoch Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))        \n",
    "\n",
    "        print(\"-\"*20,\"learning rates\",\"-\"*20)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(param_group['lr'],end=\",\")\n",
    "        print()    \n",
    "        \n",
    "        print(\"-\"*20,\"training\",\"-\"*20)\n",
    "        print(\"Trainign Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        train(epoch)\n",
    "        print(\"Trainign End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        \n",
    "        # \n",
    "        # for name,para in model.named_parameters():\n",
    "        #     print(name,para)\n",
    "        # \n",
    "        print(\"-\"*20,\"testing\",\"-\"*20)\n",
    "        print(\"Testing Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))        \n",
    "        cur_acc = test()\n",
    "        print(\"Testing End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"-\"*60)\n",
    "        print()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        is_best = False\n",
    "        if cur_acc > acc:\n",
    "            is_best = True\n",
    "            acc=cur_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "            \n",
    "            \n",
    "        \n",
    "        print(\"Best accuracy: {}; Current accuracy {}. Checkpointing\".format(acc,cur_acc))\n",
    "        if save_chkp:\n",
    "            save_checkpoint({\n",
    "              'epoch': epoch + 1,\n",
    "              'acc': acc, \n",
    "              'state_dict': model.state_dict(),      \n",
    "              'optimizer' : optimizer.state_dict(),\n",
    "               'scheduler': scheduler.state_dict(),\n",
    "            }, is_best, save_path, 'checkpoint_{}_{}.pth.tar'.format(epoch,round(cur_acc,4)))\n",
    "        print(\"Epoch End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"=\"*60)\n",
    "        print()        \n",
    "else:    \n",
    "    # print(\"=\"*20,epoch,\"Testing\",\"=\"*20)  \n",
    "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "    #     num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "    # test()\n",
    "\n",
    "\n",
    "    # print(\"=\"*100)\n",
    "    # for name,para in model.named_parameters():\n",
    "    #     print(name,para)\n",
    "    # print(\"=\"*100)\n",
    "    # \n",
    "    # # # \n",
    "    # # model(torch.tensor([0.95,0.95]),False)\n",
    "    # sys.exit(0)\n",
    "    # \n",
    "    gap = []\n",
    "    pair = {}\n",
    "    for i in range(i_dim):\n",
    "        for j in range(j_dim):\n",
    "            input = ([(1.0/i_dim)*(i+1),(1.0/j_dim)*(j+1)])            \n",
    "            output = model(torch.tensor(input),False)\n",
    "            gap.append((output[0][0]-output[0][1]).data)\n",
    "            pair[tuple(input)] = [output[0][0],output[0][1]]\n",
    "            # print(input,output.data,(output[0][0]-output[0][1]).data)\n",
    "    \n",
    "    \n",
    "    for k,v in pair.items():        \n",
    "        # if abs(v[0]-v[1]) in torch.tensor(gap).abs().topk(70)[0]: #and golden_data[k]==1:\n",
    "        print((v[0].data-v[1].data).abs(),v[0].data,v[1].data,golden_data[k],k)\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "if training:\n",
    "    print(\"=\"*100)\n",
    "    for name,para in best_model.named_parameters():\n",
    "        print(name,para)\n",
    "    print(\"=\"*100) \n",
    "    for i in range(i_dim):\n",
    "        for j in range(j_dim):\n",
    "            data = dataset_ori[i*j_dim+j][0]\n",
    "            output = model(data,False)\n",
    "    \n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            print(pred.data.item(),end = \" \")\n",
    "        print()\n",
    "    \n",
    "    for i in range(i_dim):\n",
    "        for j in range(j_dim):\n",
    "            data = dataset_ori[i*j_dim+j][0]\n",
    "            output = model(data,False)\n",
    "    \n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            print(pred.data.item(),end = \",\")\n",
    "    print()     \n",
    "\n",
    "sys.exit(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "==============================\n",
      "tensor([1.2661])\n",
      "tensor([1.9823])\n",
      "tensor([2.4981])\n",
      "==============================\n",
      "tensor([2.3766])\n",
      "tensor([1.7236])\n",
      "tensor([0.8371])\n",
      "tensor([2.5933])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch\n",
    "print(\"=\"*30)\n",
    "print(torch.tensor([(0.65*2)-1]).acos())\n",
    "print(torch.tensor([(0.3*2)-1]).acos())\n",
    "print(torch.tensor([(0.1*2)-1]).acos())\n",
    "print(\"=\"*30)\n",
    "print(torch.tensor([(0.1393*2)-1]).acos())\n",
    "print(torch.tensor([(0.4239*2)-1]).acos())\n",
    "print(torch.tensor([(0.8607*0.9699*2)-1]).acos())\n",
    "print(torch.tensor([(0.0733*2)-1]).acos())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../interfae\")\n",
    "from qiskit_simulator import *\n",
    "\n",
    "\n",
    "\n",
    "accur=[]\n",
    "def test_debug():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target,new_target = modify_target(target)\n",
    "        \n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # print(\"Debug\",data,target)\n",
    "        # output = model(data,2)\n",
    "        # \n",
    "        # run_simulator(model,data)\n",
    "        \n",
    "        output = model(data,False)\n",
    "        # print(data)\n",
    "        # print(output)\n",
    "        # print(target)\n",
    "        # sys.exit(0)\n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "        # output = model(data,False)\n",
    "        test_loss += criterion(output, target) # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    a=100.*correct / len(test_loader.dataset)\n",
    "    accur.append(a)  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / float(len(test_loader.dataset))))\n",
    "    \n",
    "    return float(correct) / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile(resume_path):\n",
    "    print(\"=> loading checkpoint from '{}'<=\".format(resume_path))\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    epoch_init,acc = checkpoint[\"epoch\"],checkpoint[\"acc\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])    \n",
    "    scheduler.milestones = Counter(milestones)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "else:\n",
    "    epoch_init,acc = 0,0\n",
    "\n",
    "print(\"HERE\")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "        num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "test_debug()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for name, para in model.named_parameters():\n",
    "# #     print(name,para)\n",
    "# print(\"HERE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8213722",
   "language": "python",
   "display_name": "PyCharm (qiskit_practice)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}