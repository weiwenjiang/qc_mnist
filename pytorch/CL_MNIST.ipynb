{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Training procedure for Quantum Computer:\n",
      "\tStart at: 04/22/2020 00:56:52\n",
      "\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\n",
      "\tEnjoy and Good Luck!\n",
      "====================================================================================================\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import functools\n",
    "from collections import Counter\n",
    "print = functools.partial(print, flush=True)\n",
    "\n",
    "# interest_num = [0,1,2,3,4,5,6,7,8,9]\n",
    "interest_num = [3,6]\n",
    "img_size = 4\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 16\n",
    "inference_batch_size = 16\n",
    "num_f1 = 4\n",
    "num_f2 = len(interest_num)\n",
    "init_lr = 0.01\n",
    "\n",
    "\n",
    "save_to_file = False\n",
    "if save_to_file:\n",
    "    sys.stdout = open(save_path+\"/log\", 'w')\n",
    "save_path = \"./model/CL\"+os.path.basename(sys.argv[0])+\"_\"+time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# resume_path = \"./model/ipykernel_launcher.py_2020_04_22-00_17_32/checkpoint_8_0.9568.pth.tar\"\n",
    "\n",
    "resume_path = \"\"\n",
    "training = True\n",
    "max_epoch = 10\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Training procedure for Quantum Computer:\")\n",
    "print(\"\\tStart at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "print(\"\\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\")\n",
    "print(\"\\tEnjoy and Good Luck!\")\n",
    "print(\"=\"*100)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def modify_target(target):\n",
    "    for j in range(len(target)):\n",
    "        for idx in range(len(interest_num)):\n",
    "            if target[j] == interest_num[idx]:\n",
    "                target[j] = idx\n",
    "                break\n",
    "    \n",
    "    new_target = torch.zeros(target.shape[0],2)\n",
    "        \n",
    "    for i in range(target.shape[0]):        \n",
    "        if target[i].item() == 0:            \n",
    "            new_target[i] = torch.tensor([1,0]).clone()     \n",
    "        else:\n",
    "            new_target[i] = torch.tensor([0,1]).clone()\n",
    "               \n",
    "    return target,new_target\n",
    "\n",
    "def select_num(dataset,interest_num):\n",
    "    labels = dataset.targets #get labels\n",
    "    labels = labels.numpy()\n",
    "    idx = {}\n",
    "    for num in interest_num:\n",
    "        idx[num] = np.where(labels == num)\n",
    "        \n",
    "    fin_idx = idx[interest_num[0]]\n",
    "    for i in range(1,len(interest_num)):           \n",
    "        \n",
    "        fin_idx = (np.concatenate((fin_idx[0],idx[interest_num[i]][0])),)\n",
    "    \n",
    "    fin_idx = fin_idx[0]    \n",
    "    \n",
    "    dataset.targets = labels[fin_idx]\n",
    "    dataset.data = dataset.data[fin_idx]\n",
    "    \n",
    "    # print(dataset.targets.shape)\n",
    "    \n",
    "    dataset.targets,_ = modify_target(dataset.targets)\n",
    "    # print(dataset.targets.shape)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.Compose([transforms.Resize((img_size,img_size)),transforms.ToTensor()])\n",
    "# transform = transforms.Compose([transforms.Resize((img_size,img_size)),transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "train_data = select_num(train_data,interest_num)\n",
    "test_data =  select_num(test_data,interest_num)\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "    num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    filename = os.path.join(save_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        bestname = os.path.join(save_path, 'model_best.tar')\n",
    "        shutil.copyfile(filename, bestname)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "========== Model Info ==========\n",
      "Net(\n",
      "  (fc1): Linear(in_features=16, out_features=4, bias=False)\n",
      "  (fc2): Linear(in_features=4, out_features=2, bias=False)\n",
      ")\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    " \n",
    "\n",
    "class BinarizeF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(cxt, input):\n",
    "        output = input.new(input.size())\n",
    "        output[input >= 0] = 1\n",
    "        output[input < 0] = -1\n",
    "        \n",
    "              \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(cxt, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input\n",
    "# aliases\n",
    "binarize = BinarizeF.apply\n",
    "\n",
    "\n",
    "class ClipF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = input.clone().detach()\n",
    "        # output = input.new(input.size())\n",
    "        output[input >= 1] = 1\n",
    "        output[input <= 0] = 0\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input >= 1] = 0\n",
    "        grad_input[input <= 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# aliases\n",
    "clipfunc = ClipF.apply\n",
    "\n",
    "\n",
    "class BinaryLinear(nn.Linear):\n",
    "    \n",
    "    \n",
    "    def do_slp_via_th(self,input_ori,w_ori):\n",
    "        p = input_ori\n",
    "        d = 4*p*(1-p)\n",
    "        e = (2*p-1)\n",
    "        # e_sq = torch.tensor(1)\n",
    "        w = w_ori\n",
    "        \n",
    "        sum_of_sq = (d+e.pow(2)).sum(-1)\n",
    "        sum_of_sq = sum_of_sq.unsqueeze(-1)        \n",
    "        sum_of_sq = sum_of_sq.expand(p.shape[0], w.shape[0])\n",
    "                \n",
    "        diag_p = torch.diag_embed(e)        \n",
    "        \n",
    "        p_w = torch.matmul(w,diag_p)\n",
    "        \n",
    "        z_p_w = torch.zeros_like(p_w)        \n",
    "        shft_p_w = torch.cat((p_w, z_p_w), -1)\n",
    "        \n",
    "        sum_of_cross = torch.zeros_like(p_w)\n",
    "        length = p.shape[1]    \n",
    "        \n",
    "        for shft in range(1,length):    \n",
    "            sum_of_cross += shft_p_w[:,:,0:length]*shft_p_w[:,:,shft:length+shft]\n",
    "\n",
    "        sum_of_cross = sum_of_cross.sum(-1)\n",
    "                \n",
    "        return (sum_of_sq+2*sum_of_cross)/(length**2) \n",
    "    \n",
    "    def forward(self, input):        \n",
    "        binary_weight = binarize(self.weight)        \n",
    "        if self.bias is None:\n",
    "            return self.do_slp_via_th(input,binary_weight)\n",
    "                      \n",
    "        else:   \n",
    "            \n",
    "            bias_one  = torch.ones(input.shape[0],1)            \n",
    "            new_input = torch.cat((input, bias_one), -1)            \n",
    "            bias = clipfunc(self.bias).unsqueeze(1)            \n",
    "            new_weight = binary_weight            \n",
    "            new_weight = torch.cat((new_weight,bias),-1)                        \n",
    "            return self.do_slp_via_th(new_input,new_weight)\n",
    "            \n",
    "            \n",
    "            torch.set_printoptions(edgeitems=64)\n",
    "            # binary_bias = binarize(self.bias)/float(len(input[0].flatten())+1)\n",
    "            binary_bias = binarize(self.bias)/float(len(input[0].flatten())+1)\n",
    "            res = F.linear(input, binary_weight/float(len(input[0].flatten())+1), binary_bias)\n",
    "            return res\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Glorot initialization\n",
    "        in_features, out_features = self.weight.size()\n",
    "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "        self.weight.lr_scale = 1. / stdv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.1):        \n",
    "        super(QC_Norm, self).__init__()\n",
    "        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)        \n",
    "        self.ang_inc = Parameter(torch.ones(1)*10)\n",
    "        \n",
    "        self.momentum = momentum\n",
    "                \n",
    "        self.printed = False\n",
    "        self.x_mean_ancle=0\n",
    "        self.x_mean_rote = 0\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                print(\"self.ang_inc\",self.ang_inc)\n",
    "                self.printed = True\n",
    "                    \n",
    "            x = x.transpose(0,1)\n",
    "  \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)\n",
    "            x_1 = (x_final.cos()+1)/2\n",
    "                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            self.printed = False\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "            x_lack_sum = x_sum - x    \n",
    "            x_mean = x_lack_sum/x.shape[-1]\n",
    "            \n",
    "            \n",
    "                    \n",
    "            x_mean_ancle = (x_mean*2-1).acos()  \n",
    "            \n",
    "            ang_inc = self.ang_inc.unsqueeze(-1).expand(x_mean_ancle.shape) \n",
    "            # ang_inc = np.pi/2/(x.max(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) -x.min(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) )\n",
    "            x_mean_rote = (np.pi/2 - x_mean_ancle)*20 # ang_inc\n",
    "            \n",
    "            x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot\n",
    "                                                \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+x_mean_rote  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "      \n",
    "        return x_1\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        self.ang_inc.data.zeros_()\n",
    "        \n",
    "def print_degree(x,name=\"x\"):\n",
    "    print(name,x/np.pi*180)\n",
    "    \n",
    "    \n",
    "class QC_Norm_Real(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Real, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.x_max = 0\n",
    "        self.x_min = 0\n",
    "        # print(\"Using Normal without real\")\n",
    "        \n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            # x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_final = ((x_ancle-self.x_min)/(self.x_max-self.x_min))*np.pi\n",
    "            \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            x = x.transpose(0,1)        \n",
    "            x_ancle = (x*2-1).acos()     \n",
    "            x_rectify_ancle = (x_ancle.max(-1)[0]-x_ancle.min(-1)[0]).unsqueeze(-1).expand(x.shape)                                                                         \n",
    "            x_final = ((x_ancle-x_ancle.min(-1)[0].unsqueeze(-1))/(x_rectify_ancle))*np.pi\n",
    "            \n",
    "            x_moving_rot = x_final - x_ancle\n",
    "            \n",
    "            x_moving_rot_mean = x_moving_rot.sum(-1)/x.shape[-1] \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot_mean      \n",
    "            \n",
    "            self.x_max = self.momentum * x_ancle.max(-1)[0].unsqueeze(-1) + \\\n",
    "                                    (1 - self.momentum) * self.x_max\n",
    "            self.x_min = self.momentum * x_ancle.min(-1)[0].unsqueeze(-1) + \\\n",
    "                                    (1 - self.momentum) * self.x_min\n",
    "            \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "            \n",
    "        return x_1\n",
    "\n",
    "\n",
    "class QC_Norm_Real_Correction(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Real_Correction, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:            \n",
    "            \n",
    "            x = x.transpose(0,1)                    \n",
    "            x_ancle = (x*2-1).acos()                        \n",
    "            x_moving_rot = -1*(x_ancle.min(-1)[0])\n",
    "            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot                                                    \n",
    "            x_final = x_ancle+x_moving_rot.unsqueeze(-1)                                    \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        return x_1\n",
    "\n",
    "class QC_Norm_Correction(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Correction, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)                \n",
    "            x_mean = x_sum/x.shape[-1]\n",
    "                                \n",
    "            x_mean_ancle = (x_mean*2-1).acos()    \n",
    "            x_mean_rote = (np.pi/2 - x_mean_ancle) \n",
    "            \n",
    "            x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])\n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot                                        \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+x_mean_rote  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "        \n",
    "        return x_1\n",
    "\n",
    "## Define the NN architecture\n",
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(in_features=img_size*img_size, out_features=num_f1, bias=False)\n",
    "            \n",
    "        self.fc2 = torch.nn.Linear(in_features=num_f1, out_features=num_f2, bias=False)\n",
    "        # # self.fc3 = BinaryLinear(num_f2,num_f3,bias=False)\n",
    "        # # # \n",
    "        # self.qc1 = QC_Norm(num_features=num_f1)\n",
    "        # self.qc2 = QC_Norm(num_features=num_f2)\n",
    "        # # self.qc3 = QC_Norm(num_features=num_f3)\n",
    "        # \n",
    "        # self.qc1a = QC_Norm_Correction(num_features=num_f1)\n",
    "        # self.qc2a = QC_Norm_Correction(num_features=num_f2)\n",
    "        # self.qc3a = QC_Norm_Correction(num_features=num_f3)\n",
    "        \n",
    "        # \n",
    "        # self.qc1 = QC_Norm_Real(num_features=num_f1)\n",
    "        # self.qc2 = QC_Norm_Real(num_features=num_f2)\n",
    "        # self.qc3 = QC_Norm_Real(num_features=num_f3)\n",
    "\n",
    "\n",
    "        # self.qc1a = QC_Norm_Real_Correction(num_features=num_f1)\n",
    "        # self.qc2a = QC_Norm_Real_Correction(num_features=num_f2)\n",
    "        # self.qc3a = QC_Norm_Real_Correction(num_features=num_f3)\n",
    "        # \n",
    "    def forward(self, x, training=1):        \n",
    "        x = x.view(-1, img_size * img_size)\n",
    "        \n",
    "        if training == 1:\n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            # # \n",
    "            \n",
    "            x = self.fc1(x).div(img_size*img_size)                 \n",
    "            x = x.pow(2)\n",
    "            x = self.fc2(x).div(num_f1)\n",
    "            x = x.pow(2)\n",
    "            # \n",
    "            # x = self.qc1(self.qc1a(self.fc1(x)))        \n",
    "            # x = self.qc2(self.qc2a(self.fc2(x)))                           \n",
    "            # x = self.qc3(self.qc3a(self.fc3(x)))\n",
    "            # \n",
    "            # x = self.qc1((self.fc1(x)))        \n",
    "            # x = self.qc2((self.fc2(x)))                           \n",
    "            # x = self.qc3((self.fc3(x)))\n",
    "            # \n",
    "        elif training == 2:\n",
    "            \n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            \n",
    "            print(\"=\"*10,\"layer 1\",\"=\"*10)\n",
    "            print(x)\n",
    "            torch.set_printoptions(profile=\"full\")            \n",
    "            print(binarize(self.fc1.weight))                                            \n",
    "            torch.set_printoptions(profile=\"default\")\n",
    "            x = self.fc1(x)            \n",
    "            \n",
    "            print(\"=\"*10,\"layer 2\",\"=\"*10)\n",
    "            print(x)\n",
    "            torch.set_printoptions(profile=\"full\")            \n",
    "            print(binarize(self.fc2.weight))                                            \n",
    "            torch.set_printoptions(profile=\"default\")\n",
    "            x = self.fc2(x)\n",
    "            \n",
    "            print(\"=\"*10,\"results\",\"=\"*10)\n",
    "            print(x)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            # x = self.qc1(self.fc1(x),training=False)\n",
    "            # x = self.qc2(self.fc2(x),training=False)\n",
    "            # x = self.qc3(self.fc3(x),training=False)\n",
    "            # \n",
    "            \n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            # \n",
    "            # x = self.qc1(self.qc1a(self.fc1(x),training=False),training=False)                \n",
    "            # x = self.qc2(self.qc2a(self.fc2(x),training=False),training=False)            \n",
    "            # x = self.qc3(self.qc3a(self.fc3(x),training=False),training=False)\n",
    "            \n",
    "            # \n",
    "            x = self.fc1(x).div(img_size*img_size)       \n",
    "            x = x.pow(2)               \n",
    "            x = self.fc2(x).div(num_f1)   \n",
    "            x = x.pow(2)            \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    epoch_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target,new_target = modify_target(target)\n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        # \n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data,True)\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()    \n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "                \n",
    "        if batch_idx % 100 == 0:        \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}/{} ({:.2f}%)'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss, correct, (batch_idx+1) * len(data),\n",
    "                100. * float(correct) / float(((batch_idx+1) * len(data)) )))                \n",
    "    print(\"-\"*20,\"training done, loss\",\"-\"*20)\n",
    "    print(\"Training Set: Average loss: {}\".format(round(sum(epoch_loss)/len(epoch_loss),6)))\n",
    "    \n",
    "accur=[]\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target,new_target = modify_target(target)\n",
    "        \n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # print(\"Debug\")\n",
    "        # output = model(data,2)\n",
    "        # \n",
    "        # sys.exit(0)\n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data,False)\n",
    "        test_loss += criterion(output, target) # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    a=100.*correct / len(test_loader.dataset)\n",
    "    accur.append(a)  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / float(len(test_loader.dataset))))\n",
    "    \n",
    "    return float(correct) / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Training\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "print(\"=\"*10,\"Model Info\",\"=\"*10)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "# optimizer = torch.optim.Adam([\n",
    "#                 {'params': model.fc1.parameters()},\n",
    "#                 {'params': model.fc2.parameters()},\n",
    "#                 {'params': model.fc3.parameters()},\n",
    "#                 {'params': model.qc1.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc2.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc3.parameters(), 'lr': 1},\n",
    "#             ], lr=0.1)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# optimizer = torch.optim.SGD([\n",
    "#                 {'params': model.fc1.parameters()},\n",
    "#                 {'params': model.fc2.parameters()},\n",
    "#                 {'params': model.fc3.parameters()},\n",
    "#                 {'params': model.qc1.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc2.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc3.parameters(), 'lr': 1},\n",
    "#             ], lr=0.1, momentum=0.9)\n",
    "# \n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \\\n",
    "#         base_lr=[1e-1,1e-1,1e-1,1,1,1], \\\n",
    "#         max_lr=[1e-3,1e-3,1e-3,1e-2,1e-2,1e-2], \\\n",
    "#         step_size_up=100\n",
    "#         )\n",
    "\n",
    "milestones = [3, 5, 8]\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)\n",
    "\n",
    "# \n",
    "# \n",
    "# test()\n",
    "# \n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "==================== 0 epoch ====================\n",
      "Epoch Start at: 04/22/2020 00:56:53\n",
      "-------------------- learning rates --------------------\n",
      "0.01,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 00:56:53\n",
      "Train Epoch: 0 [0/12049 (0%)]\tLoss: 0.693147\tAccuracy: 7/16 (43.75%)\n",
      "Train Epoch: 0 [1600/12049 (13%)]\tLoss: 0.693147\tAccuracy: 765/1616 (47.34%)\n",
      "Train Epoch: 0 [3200/12049 (27%)]\tLoss: 0.693147\tAccuracy: 1538/3216 (47.82%)\n",
      "Train Epoch: 0 [4800/12049 (40%)]\tLoss: 0.693147\tAccuracy: 2299/4816 (47.74%)\n",
      "Train Epoch: 0 [6400/12049 (53%)]\tLoss: 0.693147\tAccuracy: 3077/6416 (47.96%)\n",
      "Train Epoch: 0 [8000/12049 (66%)]\tLoss: 0.693147\tAccuracy: 3883/8016 (48.44%)\n",
      "Train Epoch: 0 [9600/12049 (80%)]\tLoss: 0.693147\tAccuracy: 4648/9616 (48.34%)\n",
      "Train Epoch: 0 [11200/12049 (93%)]\tLoss: 0.693147\tAccuracy: 5442/11216 (48.52%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.693147\n",
      "Trainign End at: 04/22/2020 00:57:00\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 00:57:00\n",
      "Test set: Average loss: 0.0433, Accuracy: 934/1968 (47.46%)\n",
      "Testing End at: 04/22/2020 00:57:01\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.47459349593495936; Current accuracy 0.47459349593495936. Checkpointing\n",
      "Epoch End at: 04/22/2020 00:57:01\n",
      "============================================================\n",
      "\n",
      "==================== 1 epoch ====================\n",
      "Epoch Start at: 04/22/2020 00:57:01\n",
      "-------------------- learning rates --------------------\n",
      "0.01,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 00:57:01\n",
      "Train Epoch: 1 [0/12049 (0%)]\tLoss: 0.693147\tAccuracy: 7/16 (43.75%)\n",
      "Train Epoch: 1 [1600/12049 (13%)]\tLoss: 0.693147\tAccuracy: 792/1616 (49.01%)\n",
      "Train Epoch: 1 [3200/12049 (27%)]\tLoss: 0.693147\tAccuracy: 1585/3216 (49.28%)\n",
      "Train Epoch: 1 [4800/12049 (40%)]\tLoss: 0.693147\tAccuracy: 2372/4816 (49.25%)\n",
      "Train Epoch: 1 [6400/12049 (53%)]\tLoss: 0.693147\tAccuracy: 3161/6416 (49.27%)\n",
      "Train Epoch: 1 [8000/12049 (66%)]\tLoss: 0.693147\tAccuracy: 3988/8016 (49.75%)\n",
      "Train Epoch: 1 [9600/12049 (80%)]\tLoss: 0.693147\tAccuracy: 4828/9616 (50.21%)\n",
      "Train Epoch: 1 [11200/12049 (93%)]\tLoss: 0.693147\tAccuracy: 5638/11216 (50.27%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.693147\n",
      "Trainign End at: 04/22/2020 00:57:08\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 00:57:08\n",
      "Test set: Average loss: 0.0433, Accuracy: 978/1968 (49.70%)\n",
      "Testing End at: 04/22/2020 00:57:09\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.4969512195121951; Current accuracy 0.4969512195121951. Checkpointing\n",
      "Epoch End at: 04/22/2020 00:57:09\n",
      "============================================================\n",
      "\n",
      "==================== 2 epoch ====================\n",
      "Epoch Start at: 04/22/2020 00:57:09\n",
      "-------------------- learning rates --------------------\n",
      "0.01,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 00:57:09\n",
      "Train Epoch: 2 [0/12049 (0%)]\tLoss: 0.693147\tAccuracy: 10/16 (62.50%)\n",
      "Train Epoch: 2 [1600/12049 (13%)]\tLoss: 0.693147\tAccuracy: 829/1616 (51.30%)\n",
      "Train Epoch: 2 [3200/12049 (27%)]\tLoss: 0.693147\tAccuracy: 1670/3216 (51.93%)\n",
      "Train Epoch: 2 [4800/12049 (40%)]\tLoss: 0.693147\tAccuracy: 2502/4816 (51.95%)\n",
      "Train Epoch: 2 [6400/12049 (53%)]\tLoss: 0.693147\tAccuracy: 3339/6416 (52.04%)\n",
      "Train Epoch: 2 [8000/12049 (66%)]\tLoss: 0.693147\tAccuracy: 4159/8016 (51.88%)\n",
      "Train Epoch: 2 [9600/12049 (80%)]\tLoss: 0.693147\tAccuracy: 4984/9616 (51.83%)\n",
      "Train Epoch: 2 [11200/12049 (93%)]\tLoss: 0.693147\tAccuracy: 5797/11216 (51.69%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.693147\n",
      "Trainign End at: 04/22/2020 00:57:16\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 00:57:16\n",
      "Test set: Average loss: 0.0433, Accuracy: 969/1968 (49.24%)\n",
      "Testing End at: 04/22/2020 00:57:16\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.4969512195121951; Current accuracy 0.4923780487804878. Checkpointing\n",
      "Epoch End at: 04/22/2020 00:57:16\n",
      "============================================================\n",
      "\n",
      "==================== 3 epoch ====================\n",
      "Epoch Start at: 04/22/2020 00:57:16\n",
      "-------------------- learning rates --------------------\n",
      "0.001,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 00:57:16\n",
      "Train Epoch: 3 [0/12049 (0%)]\tLoss: 0.693147\tAccuracy: 7/16 (43.75%)\n",
      "Train Epoch: 3 [1600/12049 (13%)]\tLoss: 0.693147\tAccuracy: 839/1616 (51.92%)\n",
      "Train Epoch: 3 [3200/12049 (27%)]\tLoss: 0.693147\tAccuracy: 1671/3216 (51.96%)\n",
      "Train Epoch: 3 [4800/12049 (40%)]\tLoss: 0.693147\tAccuracy: 2507/4816 (52.06%)\n",
      "Train Epoch: 3 [6400/12049 (53%)]\tLoss: 0.693147\tAccuracy: 3314/6416 (51.65%)\n",
      "Train Epoch: 3 [8000/12049 (66%)]\tLoss: 0.693147\tAccuracy: 4134/8016 (51.57%)\n",
      "Train Epoch: 3 [9600/12049 (80%)]\tLoss: 0.693147\tAccuracy: 4937/9616 (51.34%)\n",
      "Train Epoch: 3 [11200/12049 (93%)]\tLoss: 0.693147\tAccuracy: 5730/11216 (51.09%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.693147\n",
      "Trainign End at: 04/22/2020 00:57:23\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 00:57:23\n",
      "Test set: Average loss: 0.0433, Accuracy: 973/1968 (49.44%)\n",
      "Testing End at: 04/22/2020 00:57:24\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.4969512195121951; Current accuracy 0.49441056910569103. Checkpointing\n",
      "Epoch End at: 04/22/2020 00:57:24\n",
      "============================================================\n",
      "\n",
      "==================== 4 epoch ====================\n",
      "Epoch Start at: 04/22/2020 00:57:24\n",
      "-------------------- learning rates --------------------\n",
      "0.001,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 00:57:24\n",
      "Train Epoch: 4 [0/12049 (0%)]\tLoss: 0.693147\tAccuracy: 7/16 (43.75%)\n",
      "Train Epoch: 4 [1600/12049 (13%)]\tLoss: 0.693147\tAccuracy: 863/1616 (53.40%)\n",
      "Train Epoch: 4 [3200/12049 (27%)]\tLoss: 0.693147\tAccuracy: 1652/3216 (51.37%)\n",
      "Train Epoch: 4 [4800/12049 (40%)]\tLoss: 0.693147\tAccuracy: 2469/4816 (51.27%)\n",
      "Train Epoch: 4 [6400/12049 (53%)]\tLoss: 0.693147\tAccuracy: 3248/6416 (50.62%)\n",
      "Train Epoch: 4 [8000/12049 (66%)]\tLoss: 0.693147\tAccuracy: 4064/8016 (50.70%)\n",
      "Train Epoch: 4 [9600/12049 (80%)]\tLoss: 0.693147\tAccuracy: 4850/9616 (50.44%)\n",
      "Train Epoch: 4 [11200/12049 (93%)]\tLoss: 0.693147\tAccuracy: 5657/11216 (50.44%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.693147\n",
      "Trainign End at: 04/22/2020 00:57:31\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 00:57:31\n",
      "Test set: Average loss: 0.0433, Accuracy: 968/1968 (49.19%)\n",
      "Testing End at: 04/22/2020 00:57:32\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.4969512195121951; Current accuracy 0.491869918699187. Checkpointing\n",
      "Epoch End at: 04/22/2020 00:57:32\n",
      "============================================================\n",
      "\n",
      "==================== 5 epoch ====================\n",
      "Epoch Start at: 04/22/2020 00:57:32\n",
      "-------------------- learning rates --------------------\n",
      "0.00010000000000000002,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 00:57:32\n",
      "Train Epoch: 5 [0/12049 (0%)]\tLoss: 0.693147\tAccuracy: 10/16 (62.50%)\n",
      "Train Epoch: 5 [1600/12049 (13%)]\tLoss: 0.693147\tAccuracy: 818/1616 (50.62%)\n",
      "Train Epoch: 5 [3200/12049 (27%)]\tLoss: 0.693147\tAccuracy: 1643/3216 (51.09%)\n",
      "Train Epoch: 5 [4800/12049 (40%)]\tLoss: 0.693147\tAccuracy: 2388/4816 (49.58%)\n",
      "Train Epoch: 5 [6400/12049 (53%)]\tLoss: 0.693147\tAccuracy: 3215/6416 (50.11%)\n",
      "Train Epoch: 5 [8000/12049 (66%)]\tLoss: 0.693147\tAccuracy: 4019/8016 (50.14%)\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-8b09bccc0eb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainign Start at:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%m/%d/%Y %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainign End at:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%m/%d/%Y %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-65325ced16a6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "if os.path.isfile(resume_path):\n",
    "    print(\"=> loading checkpoint from '{}'<=\".format(resume_path))\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    epoch_init,acc = checkpoint[\"epoch\"],checkpoint[\"acc\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])    \n",
    "    scheduler.milestones = Counter(milestones)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "else:\n",
    "    epoch_init,acc = 0,0\n",
    "\n",
    "\n",
    "\n",
    "if training:\n",
    "    for epoch in range(epoch_init, max_epoch + 1):\n",
    "        print(\"=\"*20,epoch,\"epoch\",\"=\"*20)  \n",
    "        print(\"Epoch Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))        \n",
    "\n",
    "        print(\"-\"*20,\"learning rates\",\"-\"*20)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(param_group['lr'],end=\",\")\n",
    "        print()    \n",
    "        \n",
    "        print(\"-\"*20,\"training\",\"-\"*20)\n",
    "        print(\"Trainign Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        train(epoch)\n",
    "        print(\"Trainign End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"-\"*20,\"testing\",\"-\"*20)\n",
    "        print(\"Testing Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))        \n",
    "        cur_acc = test()\n",
    "        print(\"Testing End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"-\"*60)\n",
    "        print()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        is_best = False\n",
    "        if cur_acc > acc:\n",
    "            is_best = True\n",
    "            acc=cur_acc\n",
    "        \n",
    "        print(\"Best accuracy: {}; Current accuracy {}. Checkpointing\".format(acc,cur_acc))\n",
    "        save_checkpoint({\n",
    "          'epoch': epoch + 1,\n",
    "          'acc': acc, \n",
    "          'state_dict': model.state_dict(),      \n",
    "          'optimizer' : optimizer.state_dict(),\n",
    "           'scheduler': scheduler.state_dict(),\n",
    "        }, is_best, save_path, 'checkpoint_{}_{}.pth.tar'.format(epoch,round(cur_acc,4)))\n",
    "        print(\"Epoch End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"=\"*60)\n",
    "        print()        \n",
    "else:    \n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, \n",
    "        num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "    test()\n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "accur=[]\n",
    "def test_debug():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target,new_target = modify_target(target)\n",
    "        \n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        print(\"Debug\",data,target)\n",
    "        output = model(data,2)\n",
    "            \n",
    "        \n",
    "        sys.exit(0)\n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data,False)\n",
    "        test_loss += criterion(output, target) # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    a=100.*correct / len(test_loader.dataset)\n",
    "    accur.append(a)  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / float(len(test_loader.dataset))))\n",
    "    \n",
    "    return float(correct) / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "# \n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, \n",
    "#         num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "# test_debug()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for name, para in model.named_parameters():\n",
    "#     print(name,para)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8213722",
   "language": "python",
   "display_name": "PyCharm (qiskit_practice)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}