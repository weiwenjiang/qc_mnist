{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Training procedure for Quantum Computer:\n",
      "\tStart at: 06/19/2020 10:53:23\n",
      "\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\n",
      "\tEnjoy and Good Luck!\n",
      "====================================================================================================\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from lib_model_summary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import functools\n",
    "\n",
    "# from lib_util import *\n",
    "\n",
    "from collections import Counter\n",
    "print = functools.partial(print, flush=True)\n",
    "\n",
    "# For 4*4, 16->4->2: batch_size=32; init_lr=0.01; with_norm=True or False\n",
    "# For 4*4, 16->4->1: batch_size=16; init_lr=0.1; with_norm=True, ang:20; or train\n",
    "\n",
    "# interest_num = [0,1,2,3,4,5,6,7,8,9]\n",
    "# how many samples per batch to load\n",
    "batch_size = 32\n",
    "inference_batch_size = 32\n",
    "num_f1 = 1\n",
    "# num_f2 = len(interest_num)\n",
    "num_f2 = 2\n",
    "num_f3 = 2\n",
    "init_lr = 0.1\n",
    "init_qc_lr = 1\n",
    "with_norm = True\n",
    "\n",
    "# Given_ang to -1 to train the variable \n",
    "given_ang = -1\n",
    "milestones = [6, 10, 14]\n",
    "\n",
    "save_to_file = False\n",
    "if save_to_file:\n",
    "    sys.stdout = open(save_path+\"/log\", 'w')\n",
    "\n",
    "# resume_path = \"././model/ipykernel_launcher.py_2020_05_04-10_08_35/checkpoint_5_0.9401.pth.tar\"\n",
    "resume_path = \"././model/ipykernel_launcher.py_2020_05_04-13_52_53/checkpoint_0_0.9714.pth.tar\"\n",
    "resume_path = \"././model/ipykernel_launcher.py_2020_05_04-14_22_13/checkpoint_0_0.9714.pth.tar\"\n",
    "resume_path = \"././model/ipykernel_launcher.py_2020_05_05-20_06_18/checkpoint_1_1.0.pth.tar\"\n",
    "# resume_path = \"\"\n",
    "save_chkp = 0\n",
    "training = 0\n",
    "print_detail = 0\n",
    "max_epoch = 3\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "if save_chkp:\n",
    "    save_path = \"./model/\"+os.path.basename(sys.argv[0])+\"_\"+time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Training procedure for Quantum Computer:\")\n",
    "print(\"\\tStart at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "print(\"\\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\")\n",
    "print(\"\\tEnjoy and Good Luck!\")\n",
    "print(\"=\"*100)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 \n",
      "0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 \n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input = []\n",
    "output = []\n",
    "i_dim = 20\n",
    "j_dim = 20\n",
    "# ori_out = [1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,\n",
    "#            1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "#            0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "ori_out = [1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,\n",
    "           1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,\n",
    "           0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "# ori_out = [1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1]\n",
    "\n",
    "\n",
    "# 1 layer 2 neural\n",
    "ori_out = [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "golden_data = {}\n",
    "for i in range(i_dim):\n",
    "    for j in range(j_dim):\n",
    "        # if i==0 and j==0:\n",
    "        #     input.append([0.1*i+0.0001,0.1*j+0.0001])\n",
    "        \n",
    "        input.append([(1.0/i_dim)*(i+1),(1.0/j_dim)*(j+1)])\n",
    "        # if i > j or i>j_dim-j:\n",
    "        # # if i > j:\n",
    "        #     output.append([0])\n",
    "        # else:\n",
    "        #     output.append([1])\n",
    "        output.append(ori_out[i*j_dim+j])\n",
    "        golden_data[tuple([(1.0/i_dim)*(i+1),(1.0/j_dim)*(j+1)])] = ori_out[i*j_dim+j]\n",
    "\n",
    "for i in range(i_dim):\n",
    "    for j in range(j_dim):\n",
    "        print(output[i*j_dim+j],end=\" \")\n",
    "    print()\n",
    "\n",
    "img = []\n",
    "\n",
    "for i in range(i_dim):\n",
    "    row = []    \n",
    "    for j in range(j_dim):\n",
    "        # print(input[i*j_https://docs.python.org/3/library/stdtypes.html#listdim+j],end=\" \")\n",
    "        row.append(output[i*j_dim+j])\n",
    "    img.append(row)\n",
    "    # print()\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    # img = img / 2 + 0.5     # unnormalize\n",
    "    img = np.array(img)\n",
    "    image = np.asarray(img * 255, np.uint8)    \n",
    "    im = Image.fromarray(image,mode=\"L\")    \n",
    "    im = im.resize((100,100),Image.NEAREST )    \n",
    "    # im.thumbnail((64, 64), Image.ANTIALIAS)  # \n",
    "    plt.imshow(im,cmap='Greys')\n",
    "    plt.show()\n",
    "    im.save(\"../results/Figures/example.pdf\")\n",
    "\n",
    "\n",
    "\n",
    "imshow(img)\n",
    "# print labels\n",
    "# print(' '.join('%5s' % filtered_class[labels[j]] for j in range(batch_size)))\n",
    "\n",
    "\n",
    "\n",
    "# sys.exit(0)\n",
    "dataset_ori = []\n",
    "for i in range(i_dim):\n",
    "    for j in range(j_dim):\n",
    "        dataset_ori.append([torch.tensor(input[i*j_dim+j]),torch.tensor(output[i*j_dim+j])])\n",
    "        \n",
    "\n",
    "import random\n",
    "def batch_generator(dataset,batch_size):\n",
    "    batch_dataset = []\n",
    "    for i in range(int(len(dataset)/batch_size)):\n",
    "        batch = torch.zeros((batch_size,2))        \n",
    "        batch_target = torch.zeros((batch_size,1),dtype=torch.long)\n",
    "        for j in range(batch_size):\n",
    "            item = dataset[i*batch_size+j]            \n",
    "            batch[j] = item[0]\n",
    "            batch_target[j] = item[1]\n",
    "            \n",
    "        batch_dataset.append((batch,batch_target))\n",
    "        \n",
    "    return batch_dataset\n",
    "\n",
    "dataset = batch_generator(dataset_ori,batch_size)\n",
    "# dataset = torch.tensor(dataset)\n",
    "        \n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    filename = os.path.join(save_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        bestname = os.path.join(save_path, 'model_best.tar')\n",
    "        shutil.copyfile(filename, bestname)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "========== Model Info ==========\n",
      "Net(\n",
      "  (fc1): BinaryLinear(in_features=2, out_features=1, bias=False)\n",
      "  (qc1): QC_Norm_try3()\n",
      "  (qc1a): QC_Norm_Correction_try2()\n",
      ")\n",
      "=> loading checkpoint from '././model/ipykernel_launcher.py_2020_05_05-20_06_18/checkpoint_1_1.0.pth.tar'<=\n",
      "====================================================================================================\n",
      "fc1.weight Parameter containing:\n",
      "tensor([[ 1.1075, -1.6998]], requires_grad=True)\n",
      "qc1.x_running_rot Parameter containing:\n",
      "tensor([0.9699])\n",
      "qc1.ang_inc Parameter containing:\n",
      "tensor([-5.7613], requires_grad=True)\n",
      "qc1a.x_running_rot Parameter containing:\n",
      "tensor([0.1393])\n",
      "qc1a.x_l_0_5 Parameter containing:\n",
      "tensor([1.])\n",
      "qc1a.x_g_0_5 Parameter containing:\n",
      "tensor([0.])\n",
      "====================================================================================================\n",
      "tensor(0.5712) tensor(0.2144) tensor(0.7856) 1 (0.05, 0.05)\n",
      "tensor(0.4961) tensor(0.2519) tensor(0.7481) 1 (0.05, 0.1)\n",
      "tensor(0.4210) tensor(0.2895) tensor(0.7105) 1 (0.05, 0.15000000000000002)\n",
      "tensor(0.3458) tensor(0.3271) tensor(0.6729) 1 (0.05, 0.2)\n",
      "tensor(0.2707) tensor(0.3647) tensor(0.6353) 1 (0.05, 0.25)\n",
      "tensor(0.1956) tensor(0.4022) tensor(0.5978) 1 (0.05, 0.30000000000000004)\n",
      "tensor(0.1204) tensor(0.4398) tensor(0.5602) 1 (0.05, 0.35000000000000003)\n",
      "tensor(0.0453) tensor(0.4774) tensor(0.5226) 1 (0.05, 0.4)\n",
      "tensor(0.0299) tensor(0.5149) tensor(0.4851) 0 (0.05, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.05, 0.5)\n",
      "tensor(0.1801) tensor(0.5901) tensor(0.4099) 0 (0.05, 0.55)\n",
      "tensor(0.2553) tensor(0.6276) tensor(0.3724) 0 (0.05, 0.6000000000000001)\n",
      "tensor(0.3304) tensor(0.6652) tensor(0.3348) 0 (0.05, 0.65)\n",
      "tensor(0.4055) tensor(0.7028) tensor(0.2972) 0 (0.05, 0.7000000000000001)\n",
      "tensor(0.4807) tensor(0.7403) tensor(0.2597) 0 (0.05, 0.75)\n",
      "tensor(0.5558) tensor(0.7779) tensor(0.2221) 0 (0.05, 0.8)\n",
      "tensor(0.6310) tensor(0.8155) tensor(0.1845) 0 (0.05, 0.8500000000000001)\n",
      "tensor(0.7061) tensor(0.8530) tensor(0.1470) 0 (0.05, 0.9)\n",
      "tensor(0.7812) tensor(0.8906) tensor(0.1094) 0 (0.05, 0.9500000000000001)\n",
      "tensor(0.8564) tensor(0.9282) tensor(0.0718) 0 (0.05, 1.0)\n",
      "tensor(0.4961) tensor(0.2519) tensor(0.7481) 1 (0.1, 0.05)\n",
      "tensor(0.4293) tensor(0.2853) tensor(0.7147) 1 (0.1, 0.1)\n",
      "tensor(0.3625) tensor(0.3187) tensor(0.6813) 1 (0.1, 0.15000000000000002)\n",
      "tensor(0.2957) tensor(0.3521) tensor(0.6479) 1 (0.1, 0.2)\n",
      "tensor(0.2289) tensor(0.3855) tensor(0.6145) 1 (0.1, 0.25)\n",
      "tensor(0.1622) tensor(0.4189) tensor(0.5811) 1 (0.1, 0.30000000000000004)\n",
      "tensor(0.0954) tensor(0.4523) tensor(0.5477) 1 (0.1, 0.35000000000000003)\n",
      "tensor(0.0286) tensor(0.4857) tensor(0.5143) 1 (0.1, 0.4)\n",
      "tensor(0.0382) tensor(0.5191) tensor(0.4809) 0 (0.1, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.1, 0.5)\n",
      "tensor(0.1718) tensor(0.5859) tensor(0.4141) 0 (0.1, 0.55)\n",
      "tensor(0.2386) tensor(0.6193) tensor(0.3807) 0 (0.1, 0.6000000000000001)\n",
      "tensor(0.3054) tensor(0.6527) tensor(0.3473) 0 (0.1, 0.65)\n",
      "tensor(0.3722) tensor(0.6861) tensor(0.3139) 0 (0.1, 0.7000000000000001)\n",
      "tensor(0.4389) tensor(0.7195) tensor(0.2805) 0 (0.1, 0.75)\n",
      "tensor(0.5057) tensor(0.7529) tensor(0.2471) 0 (0.1, 0.8)\n",
      "tensor(0.5725) tensor(0.7863) tensor(0.2137) 0 (0.1, 0.8500000000000001)\n",
      "tensor(0.6393) tensor(0.8197) tensor(0.1803) 0 (0.1, 0.9)\n",
      "tensor(0.7061) tensor(0.8530) tensor(0.1470) 0 (0.1, 0.9500000000000001)\n",
      "tensor(0.7729) tensor(0.8864) tensor(0.1136) 0 (0.1, 1.0)\n",
      "tensor(0.4210) tensor(0.2895) tensor(0.7105) 1 (0.15000000000000002, 0.05)\n",
      "tensor(0.3625) tensor(0.3187) tensor(0.6813) 1 (0.15000000000000002, 0.1)\n",
      "tensor(0.3041) tensor(0.3480) tensor(0.6520) 1 (0.15000000000000002, 0.15000000000000002)\n",
      "tensor(0.2456) tensor(0.3772) tensor(0.6228) 1 (0.15000000000000002, 0.2)\n",
      "tensor(0.1872) tensor(0.4064) tensor(0.5936) 1 (0.15000000000000002, 0.25)\n",
      "tensor(0.1288) tensor(0.4356) tensor(0.5644) 1 (0.15000000000000002, 0.30000000000000004)\n",
      "tensor(0.0703) tensor(0.4648) tensor(0.5352) 1 (0.15000000000000002, 0.35000000000000003)\n",
      "tensor(0.0119) tensor(0.4941) tensor(0.5059) 1 (0.15000000000000002, 0.4)\n",
      "tensor(0.0466) tensor(0.5233) tensor(0.4767) 0 (0.15000000000000002, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.15000000000000002, 0.5)\n",
      "tensor(0.1634) tensor(0.5817) tensor(0.4183) 0 (0.15000000000000002, 0.55)\n",
      "tensor(0.2219) tensor(0.6109) tensor(0.3891) 0 (0.15000000000000002, 0.6000000000000001)\n",
      "tensor(0.2803) tensor(0.6402) tensor(0.3598) 0 (0.15000000000000002, 0.65)\n",
      "tensor(0.3388) tensor(0.6694) tensor(0.3306) 0 (0.15000000000000002, 0.7000000000000001)\n",
      "tensor(0.3972) tensor(0.6986) tensor(0.3014) 0 (0.15000000000000002, 0.75)\n",
      "tensor(0.4556) tensor(0.7278) tensor(0.2722) 0 (0.15000000000000002, 0.8)\n",
      "tensor(0.5141) tensor(0.7570) tensor(0.2430) 0 (0.15000000000000002, 0.8500000000000001)\n",
      "tensor(0.5725) tensor(0.7863) tensor(0.2137) 0 (0.15000000000000002, 0.9)\n",
      "tensor(0.6310) tensor(0.8155) tensor(0.1845) 0 (0.15000000000000002, 0.9500000000000001)\n",
      "tensor(0.6894) tensor(0.8447) tensor(0.1553) 0 (0.15000000000000002, 1.0)\n",
      "tensor(0.3458) tensor(0.3271) tensor(0.6729) 1 (0.2, 0.05)\n",
      "tensor(0.2957) tensor(0.3521) tensor(0.6479) 1 (0.2, 0.1)\n",
      "tensor(0.2456) tensor(0.3772) tensor(0.6228) 1 (0.2, 0.15000000000000002)\n",
      "tensor(0.1956) tensor(0.4022) tensor(0.5978) 1 (0.2, 0.2)\n",
      "tensor(0.1455) tensor(0.4273) tensor(0.5727) 1 (0.2, 0.25)\n",
      "tensor(0.0954) tensor(0.4523) tensor(0.5477) 1 (0.2, 0.30000000000000004)\n",
      "tensor(0.0453) tensor(0.4774) tensor(0.5226) 1 (0.2, 0.35000000000000003)\n",
      "tensor(0.0048) tensor(0.5024) tensor(0.4976) 0 (0.2, 0.4)\n",
      "tensor(0.0549) tensor(0.5275) tensor(0.4725) 0 (0.2, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.2, 0.5)\n",
      "tensor(0.1551) tensor(0.5775) tensor(0.4225) 0 (0.2, 0.55)\n",
      "tensor(0.2052) tensor(0.6026) tensor(0.3974) 0 (0.2, 0.6000000000000001)\n",
      "tensor(0.2553) tensor(0.6276) tensor(0.3724) 0 (0.2, 0.65)\n",
      "tensor(0.3054) tensor(0.6527) tensor(0.3473) 0 (0.2, 0.7000000000000001)\n",
      "tensor(0.3555) tensor(0.6777) tensor(0.3223) 0 (0.2, 0.75)\n",
      "tensor(0.4055) tensor(0.7028) tensor(0.2972) 0 (0.2, 0.8)\n",
      "tensor(0.4556) tensor(0.7278) tensor(0.2722) 0 (0.2, 0.8500000000000001)\n",
      "tensor(0.5057) tensor(0.7529) tensor(0.2471) 0 (0.2, 0.9)\n",
      "tensor(0.5558) tensor(0.7779) tensor(0.2221) 0 (0.2, 0.9500000000000001)\n",
      "tensor(0.6059) tensor(0.8030) tensor(0.1970) 0 (0.2, 1.0)\n",
      "tensor(0.2707) tensor(0.3647) tensor(0.6353) 1 (0.25, 0.05)\n",
      "tensor(0.2289) tensor(0.3855) tensor(0.6145) 1 (0.25, 0.1)\n",
      "tensor(0.1872) tensor(0.4064) tensor(0.5936) 1 (0.25, 0.15000000000000002)\n",
      "tensor(0.1455) tensor(0.4273) tensor(0.5727) 1 (0.25, 0.2)\n",
      "tensor(0.1037) tensor(0.4481) tensor(0.5519) 1 (0.25, 0.25)\n",
      "tensor(0.0620) tensor(0.4690) tensor(0.5310) 1 (0.25, 0.30000000000000004)\n",
      "tensor(0.0202) tensor(0.4899) tensor(0.5101) 1 (0.25, 0.35000000000000003)\n",
      "tensor(0.0215) tensor(0.5108) tensor(0.4892) 0 (0.25, 0.4)\n",
      "tensor(0.0633) tensor(0.5316) tensor(0.4684) 0 (0.25, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.25, 0.5)\n",
      "tensor(0.1467) tensor(0.5734) tensor(0.4266) 0 (0.25, 0.55)\n",
      "tensor(0.1885) tensor(0.5942) tensor(0.4058) 0 (0.25, 0.6000000000000001)\n",
      "tensor(0.2302) tensor(0.6151) tensor(0.3849) 0 (0.25, 0.65)\n",
      "tensor(0.2720) tensor(0.6360) tensor(0.3640) 0 (0.25, 0.7000000000000001)\n",
      "tensor(0.3137) tensor(0.6569) tensor(0.3431) 0 (0.25, 0.75)\n",
      "tensor(0.3555) tensor(0.6777) tensor(0.3223) 0 (0.25, 0.8)\n",
      "tensor(0.3972) tensor(0.6986) tensor(0.3014) 0 (0.25, 0.8500000000000001)\n",
      "tensor(0.4389) tensor(0.7195) tensor(0.2805) 0 (0.25, 0.9)\n",
      "tensor(0.4807) tensor(0.7403) tensor(0.2597) 0 (0.25, 0.9500000000000001)\n",
      "tensor(0.5224) tensor(0.7612) tensor(0.2388) 0 (0.25, 1.0)\n",
      "tensor(0.1956) tensor(0.4022) tensor(0.5978) 1 (0.30000000000000004, 0.05)\n",
      "tensor(0.1622) tensor(0.4189) tensor(0.5811) 1 (0.30000000000000004, 0.1)\n",
      "tensor(0.1288) tensor(0.4356) tensor(0.5644) 1 (0.30000000000000004, 0.15000000000000002)\n",
      "tensor(0.0954) tensor(0.4523) tensor(0.5477) 1 (0.30000000000000004, 0.2)\n",
      "tensor(0.0620) tensor(0.4690) tensor(0.5310) 1 (0.30000000000000004, 0.25)\n",
      "tensor(0.0286) tensor(0.4857) tensor(0.5143) 1 (0.30000000000000004, 0.30000000000000004)\n",
      "tensor(0.0048) tensor(0.5024) tensor(0.4976) 0 (0.30000000000000004, 0.35000000000000003)\n",
      "tensor(0.0382) tensor(0.5191) tensor(0.4809) 0 (0.30000000000000004, 0.4)\n",
      "tensor(0.0716) tensor(0.5358) tensor(0.4642) 0 (0.30000000000000004, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.30000000000000004, 0.5)\n",
      "tensor(0.1384) tensor(0.5692) tensor(0.4308) 0 (0.30000000000000004, 0.55)\n",
      "tensor(0.1718) tensor(0.5859) tensor(0.4141) 0 (0.30000000000000004, 0.6000000000000001)\n",
      "tensor(0.2052) tensor(0.6026) tensor(0.3974) 0 (0.30000000000000004, 0.65)\n",
      "tensor(0.2386) tensor(0.6193) tensor(0.3807) 0 (0.30000000000000004, 0.7000000000000001)\n",
      "tensor(0.2720) tensor(0.6360) tensor(0.3640) 0 (0.30000000000000004, 0.75)\n",
      "tensor(0.3054) tensor(0.6527) tensor(0.3473) 0 (0.30000000000000004, 0.8)\n",
      "tensor(0.3388) tensor(0.6694) tensor(0.3306) 0 (0.30000000000000004, 0.8500000000000001)\n",
      "tensor(0.3722) tensor(0.6861) tensor(0.3139) 0 (0.30000000000000004, 0.9)\n",
      "tensor(0.4055) tensor(0.7028) tensor(0.2972) 0 (0.30000000000000004, 0.9500000000000001)\n",
      "tensor(0.4389) tensor(0.7195) tensor(0.2805) 0 (0.30000000000000004, 1.0)\n",
      "tensor(0.1204) tensor(0.4398) tensor(0.5602) 1 (0.35000000000000003, 0.05)\n",
      "tensor(0.0954) tensor(0.4523) tensor(0.5477) 1 (0.35000000000000003, 0.1)\n",
      "tensor(0.0703) tensor(0.4648) tensor(0.5352) 1 (0.35000000000000003, 0.15000000000000002)\n",
      "tensor(0.0453) tensor(0.4774) tensor(0.5226) 1 (0.35000000000000003, 0.2)\n",
      "tensor(0.0202) tensor(0.4899) tensor(0.5101) 1 (0.35000000000000003, 0.25)\n",
      "tensor(0.0048) tensor(0.5024) tensor(0.4976) 0 (0.35000000000000003, 0.30000000000000004)\n",
      "tensor(0.0299) tensor(0.5149) tensor(0.4851) 0 (0.35000000000000003, 0.35000000000000003)\n",
      "tensor(0.0549) tensor(0.5275) tensor(0.4725) 0 (0.35000000000000003, 0.4)\n",
      "tensor(0.0800) tensor(0.5400) tensor(0.4600) 0 (0.35000000000000003, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.35000000000000003, 0.5)\n",
      "tensor(0.1300) tensor(0.5650) tensor(0.4350) 0 (0.35000000000000003, 0.55)\n",
      "tensor(0.1551) tensor(0.5775) tensor(0.4225) 0 (0.35000000000000003, 0.6000000000000001)\n",
      "tensor(0.1801) tensor(0.5901) tensor(0.4099) 0 (0.35000000000000003, 0.65)\n",
      "tensor(0.2052) tensor(0.6026) tensor(0.3974) 0 (0.35000000000000003, 0.7000000000000001)\n",
      "tensor(0.2302) tensor(0.6151) tensor(0.3849) 0 (0.35000000000000003, 0.75)\n",
      "tensor(0.2553) tensor(0.6276) tensor(0.3724) 0 (0.35000000000000003, 0.8)\n",
      "tensor(0.2803) tensor(0.6402) tensor(0.3598) 0 (0.35000000000000003, 0.8500000000000001)\n",
      "tensor(0.3054) tensor(0.6527) tensor(0.3473) 0 (0.35000000000000003, 0.9)\n",
      "tensor(0.3304) tensor(0.6652) tensor(0.3348) 0 (0.35000000000000003, 0.9500000000000001)\n",
      "tensor(0.3555) tensor(0.6777) tensor(0.3223) 0 (0.35000000000000003, 1.0)\n",
      "tensor(0.0453) tensor(0.4774) tensor(0.5226) 1 (0.4, 0.05)\n",
      "tensor(0.0286) tensor(0.4857) tensor(0.5143) 1 (0.4, 0.1)\n",
      "tensor(0.0119) tensor(0.4941) tensor(0.5059) 1 (0.4, 0.15000000000000002)\n",
      "tensor(0.0048) tensor(0.5024) tensor(0.4976) 0 (0.4, 0.2)\n",
      "tensor(0.0215) tensor(0.5108) tensor(0.4892) 0 (0.4, 0.25)\n",
      "tensor(0.0382) tensor(0.5191) tensor(0.4809) 0 (0.4, 0.30000000000000004)\n",
      "tensor(0.0549) tensor(0.5275) tensor(0.4725) 0 (0.4, 0.35000000000000003)\n",
      "tensor(0.0716) tensor(0.5358) tensor(0.4642) 0 (0.4, 0.4)\n",
      "tensor(0.0883) tensor(0.5442) tensor(0.4558) 0 (0.4, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.4, 0.5)\n",
      "tensor(0.1217) tensor(0.5608) tensor(0.4392) 0 (0.4, 0.55)\n",
      "tensor(0.1384) tensor(0.5692) tensor(0.4308) 0 (0.4, 0.6000000000000001)\n",
      "tensor(0.1551) tensor(0.5775) tensor(0.4225) 0 (0.4, 0.65)\n",
      "tensor(0.1718) tensor(0.5859) tensor(0.4141) 0 (0.4, 0.7000000000000001)\n",
      "tensor(0.1885) tensor(0.5942) tensor(0.4058) 0 (0.4, 0.75)\n",
      "tensor(0.2052) tensor(0.6026) tensor(0.3974) 0 (0.4, 0.8)\n",
      "tensor(0.2219) tensor(0.6109) tensor(0.3891) 0 (0.4, 0.8500000000000001)\n",
      "tensor(0.2386) tensor(0.6193) tensor(0.3807) 0 (0.4, 0.9)\n",
      "tensor(0.2553) tensor(0.6276) tensor(0.3724) 0 (0.4, 0.9500000000000001)\n",
      "tensor(0.2720) tensor(0.6360) tensor(0.3640) 0 (0.4, 1.0)\n",
      "tensor(0.0299) tensor(0.5149) tensor(0.4851) 0 (0.45, 0.05)\n",
      "tensor(0.0382) tensor(0.5191) tensor(0.4809) 0 (0.45, 0.1)\n",
      "tensor(0.0466) tensor(0.5233) tensor(0.4767) 0 (0.45, 0.15000000000000002)\n",
      "tensor(0.0549) tensor(0.5275) tensor(0.4725) 0 (0.45, 0.2)\n",
      "tensor(0.0633) tensor(0.5316) tensor(0.4684) 0 (0.45, 0.25)\n",
      "tensor(0.0716) tensor(0.5358) tensor(0.4642) 0 (0.45, 0.30000000000000004)\n",
      "tensor(0.0800) tensor(0.5400) tensor(0.4600) 0 (0.45, 0.35000000000000003)\n",
      "tensor(0.0883) tensor(0.5442) tensor(0.4558) 0 (0.45, 0.4)\n",
      "tensor(0.0966) tensor(0.5483) tensor(0.4517) 0 (0.45, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.45, 0.5)\n",
      "tensor(0.1133) tensor(0.5567) tensor(0.4433) 0 (0.45, 0.55)\n",
      "tensor(0.1217) tensor(0.5608) tensor(0.4392) 0 (0.45, 0.6000000000000001)\n",
      "tensor(0.1300) tensor(0.5650) tensor(0.4350) 0 (0.45, 0.65)\n",
      "tensor(0.1384) tensor(0.5692) tensor(0.4308) 0 (0.45, 0.7000000000000001)\n",
      "tensor(0.1467) tensor(0.5734) tensor(0.4266) 0 (0.45, 0.75)\n",
      "tensor(0.1551) tensor(0.5775) tensor(0.4225) 0 (0.45, 0.8)\n",
      "tensor(0.1634) tensor(0.5817) tensor(0.4183) 0 (0.45, 0.8500000000000001)\n",
      "tensor(0.1718) tensor(0.5859) tensor(0.4141) 0 (0.45, 0.9)\n",
      "tensor(0.1801) tensor(0.5901) tensor(0.4099) 0 (0.45, 0.9500000000000001)\n",
      "tensor(0.1885) tensor(0.5942) tensor(0.4058) 0 (0.45, 1.0)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.05)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.1)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.15000000000000002)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.2)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.25)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.30000000000000004)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.35000000000000003)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.4)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.5)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.55)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.6000000000000001)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.65)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.7000000000000001)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.75)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.8)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.8500000000000001)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.9)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 0.9500000000000001)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.5, 1.0)\n",
      "tensor(0.1801) tensor(0.5901) tensor(0.4099) 0 (0.55, 0.05)\n",
      "tensor(0.1718) tensor(0.5859) tensor(0.4141) 0 (0.55, 0.1)\n",
      "tensor(0.1634) tensor(0.5817) tensor(0.4183) 0 (0.55, 0.15000000000000002)\n",
      "tensor(0.1551) tensor(0.5775) tensor(0.4225) 0 (0.55, 0.2)\n",
      "tensor(0.1467) tensor(0.5734) tensor(0.4266) 0 (0.55, 0.25)\n",
      "tensor(0.1384) tensor(0.5692) tensor(0.4308) 0 (0.55, 0.30000000000000004)\n",
      "tensor(0.1300) tensor(0.5650) tensor(0.4350) 0 (0.55, 0.35000000000000003)\n",
      "tensor(0.1217) tensor(0.5608) tensor(0.4392) 0 (0.55, 0.4)\n",
      "tensor(0.1133) tensor(0.5567) tensor(0.4433) 0 (0.55, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.55, 0.5)\n",
      "tensor(0.0966) tensor(0.5483) tensor(0.4517) 0 (0.55, 0.55)\n",
      "tensor(0.0883) tensor(0.5442) tensor(0.4558) 0 (0.55, 0.6000000000000001)\n",
      "tensor(0.0800) tensor(0.5400) tensor(0.4600) 0 (0.55, 0.65)\n",
      "tensor(0.0716) tensor(0.5358) tensor(0.4642) 0 (0.55, 0.7000000000000001)\n",
      "tensor(0.0633) tensor(0.5316) tensor(0.4684) 0 (0.55, 0.75)\n",
      "tensor(0.0549) tensor(0.5275) tensor(0.4725) 0 (0.55, 0.8)\n",
      "tensor(0.0466) tensor(0.5233) tensor(0.4767) 0 (0.55, 0.8500000000000001)\n",
      "tensor(0.0382) tensor(0.5191) tensor(0.4809) 0 (0.55, 0.9)\n",
      "tensor(0.0299) tensor(0.5149) tensor(0.4851) 0 (0.55, 0.9500000000000001)\n",
      "tensor(0.0215) tensor(0.5108) tensor(0.4892) 0 (0.55, 1.0)\n",
      "tensor(0.2553) tensor(0.6276) tensor(0.3724) 0 (0.6000000000000001, 0.05)\n",
      "tensor(0.2386) tensor(0.6193) tensor(0.3807) 0 (0.6000000000000001, 0.1)\n",
      "tensor(0.2219) tensor(0.6109) tensor(0.3891) 0 (0.6000000000000001, 0.15000000000000002)\n",
      "tensor(0.2052) tensor(0.6026) tensor(0.3974) 0 (0.6000000000000001, 0.2)\n",
      "tensor(0.1885) tensor(0.5942) tensor(0.4058) 0 (0.6000000000000001, 0.25)\n",
      "tensor(0.1718) tensor(0.5859) tensor(0.4141) 0 (0.6000000000000001, 0.30000000000000004)\n",
      "tensor(0.1551) tensor(0.5775) tensor(0.4225) 0 (0.6000000000000001, 0.35000000000000003)\n",
      "tensor(0.1384) tensor(0.5692) tensor(0.4308) 0 (0.6000000000000001, 0.4)\n",
      "tensor(0.1217) tensor(0.5608) tensor(0.4392) 0 (0.6000000000000001, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.6000000000000001, 0.5)\n",
      "tensor(0.0883) tensor(0.5442) tensor(0.4558) 0 (0.6000000000000001, 0.55)\n",
      "tensor(0.0716) tensor(0.5358) tensor(0.4642) 0 (0.6000000000000001, 0.6000000000000001)\n",
      "tensor(0.0549) tensor(0.5275) tensor(0.4725) 0 (0.6000000000000001, 0.65)\n",
      "tensor(0.0382) tensor(0.5191) tensor(0.4809) 0 (0.6000000000000001, 0.7000000000000001)\n",
      "tensor(0.0215) tensor(0.5108) tensor(0.4892) 0 (0.6000000000000001, 0.75)\n",
      "tensor(0.0048) tensor(0.5024) tensor(0.4976) 0 (0.6000000000000001, 0.8)\n",
      "tensor(0.0119) tensor(0.4941) tensor(0.5059) 1 (0.6000000000000001, 0.8500000000000001)\n",
      "tensor(0.0286) tensor(0.4857) tensor(0.5143) 1 (0.6000000000000001, 0.9)\n",
      "tensor(0.0453) tensor(0.4774) tensor(0.5226) 1 (0.6000000000000001, 0.9500000000000001)\n",
      "tensor(0.0620) tensor(0.4690) tensor(0.5310) 1 (0.6000000000000001, 1.0)\n",
      "tensor(0.3304) tensor(0.6652) tensor(0.3348) 0 (0.65, 0.05)\n",
      "tensor(0.3054) tensor(0.6527) tensor(0.3473) 0 (0.65, 0.1)\n",
      "tensor(0.2803) tensor(0.6402) tensor(0.3598) 0 (0.65, 0.15000000000000002)\n",
      "tensor(0.2553) tensor(0.6276) tensor(0.3724) 0 (0.65, 0.2)\n",
      "tensor(0.2302) tensor(0.6151) tensor(0.3849) 0 (0.65, 0.25)\n",
      "tensor(0.2052) tensor(0.6026) tensor(0.3974) 0 (0.65, 0.30000000000000004)\n",
      "tensor(0.1801) tensor(0.5901) tensor(0.4099) 0 (0.65, 0.35000000000000003)\n",
      "tensor(0.1551) tensor(0.5775) tensor(0.4225) 0 (0.65, 0.4)\n",
      "tensor(0.1300) tensor(0.5650) tensor(0.4350) 0 (0.65, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.65, 0.5)\n",
      "tensor(0.0800) tensor(0.5400) tensor(0.4600) 0 (0.65, 0.55)\n",
      "tensor(0.0549) tensor(0.5275) tensor(0.4725) 0 (0.65, 0.6000000000000001)\n",
      "tensor(0.0299) tensor(0.5149) tensor(0.4851) 0 (0.65, 0.65)\n",
      "tensor(0.0048) tensor(0.5024) tensor(0.4976) 0 (0.65, 0.7000000000000001)\n",
      "tensor(0.0202) tensor(0.4899) tensor(0.5101) 1 (0.65, 0.75)\n",
      "tensor(0.0453) tensor(0.4774) tensor(0.5226) 1 (0.65, 0.8)\n",
      "tensor(0.0703) tensor(0.4648) tensor(0.5352) 1 (0.65, 0.8500000000000001)\n",
      "tensor(0.0954) tensor(0.4523) tensor(0.5477) 1 (0.65, 0.9)\n",
      "tensor(0.1204) tensor(0.4398) tensor(0.5602) 1 (0.65, 0.9500000000000001)\n",
      "tensor(0.1455) tensor(0.4273) tensor(0.5727) 1 (0.65, 1.0)\n",
      "tensor(0.4055) tensor(0.7028) tensor(0.2972) 0 (0.7000000000000001, 0.05)\n",
      "tensor(0.3722) tensor(0.6861) tensor(0.3139) 0 (0.7000000000000001, 0.1)\n",
      "tensor(0.3388) tensor(0.6694) tensor(0.3306) 0 (0.7000000000000001, 0.15000000000000002)\n",
      "tensor(0.3054) tensor(0.6527) tensor(0.3473) 0 (0.7000000000000001, 0.2)\n",
      "tensor(0.2720) tensor(0.6360) tensor(0.3640) 0 (0.7000000000000001, 0.25)\n",
      "tensor(0.2386) tensor(0.6193) tensor(0.3807) 0 (0.7000000000000001, 0.30000000000000004)\n",
      "tensor(0.2052) tensor(0.6026) tensor(0.3974) 0 (0.7000000000000001, 0.35000000000000003)\n",
      "tensor(0.1718) tensor(0.5859) tensor(0.4141) 0 (0.7000000000000001, 0.4)\n",
      "tensor(0.1384) tensor(0.5692) tensor(0.4308) 0 (0.7000000000000001, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.7000000000000001, 0.5)\n",
      "tensor(0.0716) tensor(0.5358) tensor(0.4642) 0 (0.7000000000000001, 0.55)\n",
      "tensor(0.0382) tensor(0.5191) tensor(0.4809) 0 (0.7000000000000001, 0.6000000000000001)\n",
      "tensor(0.0048) tensor(0.5024) tensor(0.4976) 0 (0.7000000000000001, 0.65)\n",
      "tensor(0.0286) tensor(0.4857) tensor(0.5143) 1 (0.7000000000000001, 0.7000000000000001)\n",
      "tensor(0.0620) tensor(0.4690) tensor(0.5310) 1 (0.7000000000000001, 0.75)\n",
      "tensor(0.0954) tensor(0.4523) tensor(0.5477) 1 (0.7000000000000001, 0.8)\n",
      "tensor(0.1288) tensor(0.4356) tensor(0.5644) 1 (0.7000000000000001, 0.8500000000000001)\n",
      "tensor(0.1622) tensor(0.4189) tensor(0.5811) 1 (0.7000000000000001, 0.9)\n",
      "tensor(0.1956) tensor(0.4022) tensor(0.5978) 1 (0.7000000000000001, 0.9500000000000001)\n",
      "tensor(0.2289) tensor(0.3855) tensor(0.6145) 1 (0.7000000000000001, 1.0)\n",
      "tensor(0.4807) tensor(0.7403) tensor(0.2597) 0 (0.75, 0.05)\n",
      "tensor(0.4389) tensor(0.7195) tensor(0.2805) 0 (0.75, 0.1)\n",
      "tensor(0.3972) tensor(0.6986) tensor(0.3014) 0 (0.75, 0.15000000000000002)\n",
      "tensor(0.3555) tensor(0.6777) tensor(0.3223) 0 (0.75, 0.2)\n",
      "tensor(0.3137) tensor(0.6569) tensor(0.3431) 0 (0.75, 0.25)\n",
      "tensor(0.2720) tensor(0.6360) tensor(0.3640) 0 (0.75, 0.30000000000000004)\n",
      "tensor(0.2302) tensor(0.6151) tensor(0.3849) 0 (0.75, 0.35000000000000003)\n",
      "tensor(0.1885) tensor(0.5942) tensor(0.4058) 0 (0.75, 0.4)\n",
      "tensor(0.1467) tensor(0.5734) tensor(0.4266) 0 (0.75, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.75, 0.5)\n",
      "tensor(0.0633) tensor(0.5316) tensor(0.4684) 0 (0.75, 0.55)\n",
      "tensor(0.0215) tensor(0.5108) tensor(0.4892) 0 (0.75, 0.6000000000000001)\n",
      "tensor(0.0202) tensor(0.4899) tensor(0.5101) 1 (0.75, 0.65)\n",
      "tensor(0.0620) tensor(0.4690) tensor(0.5310) 1 (0.75, 0.7000000000000001)\n",
      "tensor(0.1037) tensor(0.4481) tensor(0.5519) 1 (0.75, 0.75)\n",
      "tensor(0.1455) tensor(0.4273) tensor(0.5727) 1 (0.75, 0.8)\n",
      "tensor(0.1872) tensor(0.4064) tensor(0.5936) 1 (0.75, 0.8500000000000001)\n",
      "tensor(0.2289) tensor(0.3855) tensor(0.6145) 1 (0.75, 0.9)\n",
      "tensor(0.2707) tensor(0.3647) tensor(0.6353) 1 (0.75, 0.9500000000000001)\n",
      "tensor(0.3124) tensor(0.3438) tensor(0.6562) 1 (0.75, 1.0)\n",
      "tensor(0.5558) tensor(0.7779) tensor(0.2221) 0 (0.8, 0.05)\n",
      "tensor(0.5057) tensor(0.7529) tensor(0.2471) 0 (0.8, 0.1)\n",
      "tensor(0.4556) tensor(0.7278) tensor(0.2722) 0 (0.8, 0.15000000000000002)\n",
      "tensor(0.4055) tensor(0.7028) tensor(0.2972) 0 (0.8, 0.2)\n",
      "tensor(0.3555) tensor(0.6777) tensor(0.3223) 0 (0.8, 0.25)\n",
      "tensor(0.3054) tensor(0.6527) tensor(0.3473) 0 (0.8, 0.30000000000000004)\n",
      "tensor(0.2553) tensor(0.6276) tensor(0.3724) 0 (0.8, 0.35000000000000003)\n",
      "tensor(0.2052) tensor(0.6026) tensor(0.3974) 0 (0.8, 0.4)\n",
      "tensor(0.1551) tensor(0.5775) tensor(0.4225) 0 (0.8, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.8, 0.5)\n",
      "tensor(0.0549) tensor(0.5275) tensor(0.4725) 0 (0.8, 0.55)\n",
      "tensor(0.0048) tensor(0.5024) tensor(0.4976) 0 (0.8, 0.6000000000000001)\n",
      "tensor(0.0453) tensor(0.4774) tensor(0.5226) 1 (0.8, 0.65)\n",
      "tensor(0.0954) tensor(0.4523) tensor(0.5477) 1 (0.8, 0.7000000000000001)\n",
      "tensor(0.1455) tensor(0.4273) tensor(0.5727) 1 (0.8, 0.75)\n",
      "tensor(0.1956) tensor(0.4022) tensor(0.5978) 1 (0.8, 0.8)\n",
      "tensor(0.2456) tensor(0.3772) tensor(0.6228) 1 (0.8, 0.8500000000000001)\n",
      "tensor(0.2957) tensor(0.3521) tensor(0.6479) 1 (0.8, 0.9)\n",
      "tensor(0.3458) tensor(0.3271) tensor(0.6729) 1 (0.8, 0.9500000000000001)\n",
      "tensor(0.3959) tensor(0.3020) tensor(0.6980) 1 (0.8, 1.0)\n",
      "tensor(0.6310) tensor(0.8155) tensor(0.1845) 0 (0.8500000000000001, 0.05)\n",
      "tensor(0.5725) tensor(0.7863) tensor(0.2137) 0 (0.8500000000000001, 0.1)\n",
      "tensor(0.5141) tensor(0.7570) tensor(0.2430) 0 (0.8500000000000001, 0.15000000000000002)\n",
      "tensor(0.4556) tensor(0.7278) tensor(0.2722) 0 (0.8500000000000001, 0.2)\n",
      "tensor(0.3972) tensor(0.6986) tensor(0.3014) 0 (0.8500000000000001, 0.25)\n",
      "tensor(0.3388) tensor(0.6694) tensor(0.3306) 0 (0.8500000000000001, 0.30000000000000004)\n",
      "tensor(0.2803) tensor(0.6402) tensor(0.3598) 0 (0.8500000000000001, 0.35000000000000003)\n",
      "tensor(0.2219) tensor(0.6109) tensor(0.3891) 0 (0.8500000000000001, 0.4)\n",
      "tensor(0.1634) tensor(0.5817) tensor(0.4183) 0 (0.8500000000000001, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.8500000000000001, 0.5)\n",
      "tensor(0.0466) tensor(0.5233) tensor(0.4767) 0 (0.8500000000000001, 0.55)\n",
      "tensor(0.0119) tensor(0.4941) tensor(0.5059) 1 (0.8500000000000001, 0.6000000000000001)\n",
      "tensor(0.0703) tensor(0.4648) tensor(0.5352) 1 (0.8500000000000001, 0.65)\n",
      "tensor(0.1288) tensor(0.4356) tensor(0.5644) 1 (0.8500000000000001, 0.7000000000000001)\n",
      "tensor(0.1872) tensor(0.4064) tensor(0.5936) 1 (0.8500000000000001, 0.75)\n",
      "tensor(0.2456) tensor(0.3772) tensor(0.6228) 1 (0.8500000000000001, 0.8)\n",
      "tensor(0.3041) tensor(0.3480) tensor(0.6520) 1 (0.8500000000000001, 0.8500000000000001)\n",
      "tensor(0.3625) tensor(0.3187) tensor(0.6813) 1 (0.8500000000000001, 0.9)\n",
      "tensor(0.4210) tensor(0.2895) tensor(0.7105) 1 (0.8500000000000001, 0.9500000000000001)\n",
      "tensor(0.4794) tensor(0.2603) tensor(0.7397) 1 (0.8500000000000001, 1.0)\n",
      "tensor(0.7061) tensor(0.8530) tensor(0.1470) 0 (0.9, 0.05)\n",
      "tensor(0.6393) tensor(0.8197) tensor(0.1803) 0 (0.9, 0.1)\n",
      "tensor(0.5725) tensor(0.7863) tensor(0.2137) 0 (0.9, 0.15000000000000002)\n",
      "tensor(0.5057) tensor(0.7529) tensor(0.2471) 0 (0.9, 0.2)\n",
      "tensor(0.4389) tensor(0.7195) tensor(0.2805) 0 (0.9, 0.25)\n",
      "tensor(0.3722) tensor(0.6861) tensor(0.3139) 0 (0.9, 0.30000000000000004)\n",
      "tensor(0.3054) tensor(0.6527) tensor(0.3473) 0 (0.9, 0.35000000000000003)\n",
      "tensor(0.2386) tensor(0.6193) tensor(0.3807) 0 (0.9, 0.4)\n",
      "tensor(0.1718) tensor(0.5859) tensor(0.4141) 0 (0.9, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.9, 0.5)\n",
      "tensor(0.0382) tensor(0.5191) tensor(0.4809) 0 (0.9, 0.55)\n",
      "tensor(0.0286) tensor(0.4857) tensor(0.5143) 1 (0.9, 0.6000000000000001)\n",
      "tensor(0.0954) tensor(0.4523) tensor(0.5477) 1 (0.9, 0.65)\n",
      "tensor(0.1622) tensor(0.4189) tensor(0.5811) 1 (0.9, 0.7000000000000001)\n",
      "tensor(0.2289) tensor(0.3855) tensor(0.6145) 1 (0.9, 0.75)\n",
      "tensor(0.2957) tensor(0.3521) tensor(0.6479) 1 (0.9, 0.8)\n",
      "tensor(0.3625) tensor(0.3187) tensor(0.6813) 1 (0.9, 0.8500000000000001)\n",
      "tensor(0.4293) tensor(0.2853) tensor(0.7147) 1 (0.9, 0.9)\n",
      "tensor(0.4961) tensor(0.2519) tensor(0.7481) 1 (0.9, 0.9500000000000001)\n",
      "tensor(0.5629) tensor(0.2186) tensor(0.7814) 1 (0.9, 1.0)\n",
      "tensor(0.7812) tensor(0.8906) tensor(0.1094) 0 (0.9500000000000001, 0.05)\n",
      "tensor(0.7061) tensor(0.8530) tensor(0.1470) 0 (0.9500000000000001, 0.1)\n",
      "tensor(0.6310) tensor(0.8155) tensor(0.1845) 0 (0.9500000000000001, 0.15000000000000002)\n",
      "tensor(0.5558) tensor(0.7779) tensor(0.2221) 0 (0.9500000000000001, 0.2)\n",
      "tensor(0.4807) tensor(0.7403) tensor(0.2597) 0 (0.9500000000000001, 0.25)\n",
      "tensor(0.4055) tensor(0.7028) tensor(0.2972) 0 (0.9500000000000001, 0.30000000000000004)\n",
      "tensor(0.3304) tensor(0.6652) tensor(0.3348) 0 (0.9500000000000001, 0.35000000000000003)\n",
      "tensor(0.2553) tensor(0.6276) tensor(0.3724) 0 (0.9500000000000001, 0.4)\n",
      "tensor(0.1801) tensor(0.5901) tensor(0.4099) 0 (0.9500000000000001, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (0.9500000000000001, 0.5)\n",
      "tensor(0.0299) tensor(0.5149) tensor(0.4851) 0 (0.9500000000000001, 0.55)\n",
      "tensor(0.0453) tensor(0.4774) tensor(0.5226) 1 (0.9500000000000001, 0.6000000000000001)\n",
      "tensor(0.1204) tensor(0.4398) tensor(0.5602) 1 (0.9500000000000001, 0.65)\n",
      "tensor(0.1956) tensor(0.4022) tensor(0.5978) 1 (0.9500000000000001, 0.7000000000000001)\n",
      "tensor(0.2707) tensor(0.3647) tensor(0.6353) 1 (0.9500000000000001, 0.75)\n",
      "tensor(0.3458) tensor(0.3271) tensor(0.6729) 1 (0.9500000000000001, 0.8)\n",
      "tensor(0.4210) tensor(0.2895) tensor(0.7105) 1 (0.9500000000000001, 0.8500000000000001)\n",
      "tensor(0.4961) tensor(0.2519) tensor(0.7481) 1 (0.9500000000000001, 0.9)\n",
      "tensor(0.5712) tensor(0.2144) tensor(0.7856) 1 (0.9500000000000001, 0.9500000000000001)\n",
      "tensor(0.6464) tensor(0.1768) tensor(0.8232) 1 (0.9500000000000001, 1.0)\n",
      "tensor(0.8564) tensor(0.9282) tensor(0.0718) 0 (1.0, 0.05)\n",
      "tensor(0.7729) tensor(0.8864) tensor(0.1136) 0 (1.0, 0.1)\n",
      "tensor(0.6894) tensor(0.8447) tensor(0.1553) 0 (1.0, 0.15000000000000002)\n",
      "tensor(0.6059) tensor(0.8030) tensor(0.1970) 0 (1.0, 0.2)\n",
      "tensor(0.5224) tensor(0.7612) tensor(0.2388) 0 (1.0, 0.25)\n",
      "tensor(0.4389) tensor(0.7195) tensor(0.2805) 0 (1.0, 0.30000000000000004)\n",
      "tensor(0.3555) tensor(0.6777) tensor(0.3223) 0 (1.0, 0.35000000000000003)\n",
      "tensor(0.2720) tensor(0.6360) tensor(0.3640) 0 (1.0, 0.4)\n",
      "tensor(0.1885) tensor(0.5942) tensor(0.4058) 0 (1.0, 0.45)\n",
      "tensor(0.1050) tensor(0.5525) tensor(0.4475) 0 (1.0, 0.5)\n",
      "tensor(0.0215) tensor(0.5108) tensor(0.4892) 0 (1.0, 0.55)\n",
      "tensor(0.0620) tensor(0.4690) tensor(0.5310) 1 (1.0, 0.6000000000000001)\n",
      "tensor(0.1455) tensor(0.4273) tensor(0.5727) 1 (1.0, 0.65)\n",
      "tensor(0.2289) tensor(0.3855) tensor(0.6145) 1 (1.0, 0.7000000000000001)\n",
      "tensor(0.3124) tensor(0.3438) tensor(0.6562) 1 (1.0, 0.75)\n",
      "tensor(0.3959) tensor(0.3020) tensor(0.6980) 1 (1.0, 0.8)\n",
      "tensor(0.4794) tensor(0.2603) tensor(0.7397) 1 (1.0, 0.8500000000000001)\n",
      "tensor(0.5629) tensor(0.2186) tensor(0.7814) 1 (1.0, 0.9)\n",
      "tensor(0.6464) tensor(0.1768) tensor(0.8232) 1 (1.0, 0.9500000000000001)\n",
      "tensor(0.7299) tensor(0.1351) tensor(0.8649) 1 (1.0, 1.0)\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ],
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "from torch.nn.parameter import Parameter\n",
    " \n",
    "\n",
    "class BinarizeF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(cxt, input):\n",
    "        output = input.new(input.size())\n",
    "        output[input >= 0] = 1\n",
    "        output[input < 0] = -1\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(cxt, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input\n",
    "# aliases\n",
    "binarize = BinarizeF.apply\n",
    "\n",
    "\n",
    "class ClipF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = input.clone().detach()\n",
    "        # output = input.new(input.size())\n",
    "        output[input >= 1] = 1\n",
    "        output[input <= 0] = 0\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input >= 1] = 0\n",
    "        grad_input[input <= 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# aliases\n",
    "clipfunc = ClipF.apply\n",
    "\n",
    "\n",
    "class BinaryLinear(nn.Linear):\n",
    "\n",
    "\n",
    "    def do_slp_via_th(self,input_ori,w_ori):\n",
    "        p = input_ori\n",
    "        d = 4*p*(1-p)\n",
    "        e = (2*p-1)\n",
    "        # e_sq = torch.tensor(1)\n",
    "        w = w_ori\n",
    "\n",
    "        sum_of_sq = (d+e.pow(2)).sum(-1)\n",
    "        sum_of_sq = sum_of_sq.unsqueeze(-1)        \n",
    "        sum_of_sq = sum_of_sq.expand(p.shape[0], w.shape[0])\n",
    "\n",
    "        diag_p = torch.diag_embed(e)        \n",
    "\n",
    "        p_w = torch.matmul(w,diag_p)\n",
    "\n",
    "        z_p_w = torch.zeros_like(p_w)        \n",
    "        shft_p_w = torch.cat((p_w, z_p_w), -1)\n",
    "\n",
    "        sum_of_cross = torch.zeros_like(p_w)\n",
    "        length = p.shape[1]    \n",
    "\n",
    "        for shft in range(1,length):    \n",
    "            sum_of_cross += shft_p_w[:,:,0:length]*shft_p_w[:,:,shft:length+shft]\n",
    "\n",
    "        sum_of_cross = sum_of_cross.sum(-1)\n",
    "\n",
    "        return (sum_of_sq+2*sum_of_cross)/(length**2) \n",
    "\n",
    "    def forward(self, input):        \n",
    "        binary_weight = binarize(self.weight)        \n",
    "        if self.bias is None:\n",
    "            return self.do_slp_via_th(input,binary_weight)\n",
    "\n",
    "        else:   \n",
    "\n",
    "            bias_one  = torch.ones(input.shape[0],1)            \n",
    "            new_input = torch.cat((input, bias_one), -1)            \n",
    "            bias = clipfunc(self.bias).unsqueeze(1)            \n",
    "            new_weight = binary_weight            \n",
    "            new_weight = torch.cat((new_weight,bias),-1)                        \n",
    "            return self.do_slp_via_th(new_input,new_weight)\n",
    "\n",
    "\n",
    "            torch.set_printoptions(edgeitems=64)\n",
    "            # binary_bias = binarize(self.bias)/float(len(input[0].flatten())+1)\n",
    "            binary_bias = binarize(self.bias)/float(len(input[0].flatten())+1)\n",
    "            res = F.linear(input, binary_weight/float(len(input[0].flatten())+1), binary_bias)\n",
    "            return res\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Glorot initialization\n",
    "        in_features, out_features = self.weight.size()\n",
    "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "        self.weight.lr_scale = 1. / stdv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm(nn.Module):\n",
    "    def __init__(self, num_features, init_ang_inc = 10, momentum=0.1):        \n",
    "        super(QC_Norm, self).__init__()\n",
    "        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)        \n",
    "        self.ang_inc = Parameter(torch.ones(1)*init_ang_inc)\n",
    "        \n",
    "        self.momentum = momentum\n",
    "                \n",
    "        self.printed = False\n",
    "        self.x_mean_ancle=0\n",
    "        self.x_mean_rote = 0\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                print(\"self.ang_inc\",self.ang_inc)\n",
    "                self.printed = True\n",
    "                    \n",
    "            x = x.transpose(0,1)\n",
    "  \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)\n",
    "            x_1 = (x_final.cos()+1)/2\n",
    "                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            # print(x_1)\n",
    "        else:\n",
    "            self.printed = False\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "            x_lack_sum = x_sum - x    \n",
    "            x_mean = x_lack_sum/x.shape[-1]\n",
    "            \n",
    "            # print(x)\n",
    "            # print(x_sum)\n",
    "            # print(x_lack_sum)\n",
    "            # print(x_mean)\n",
    "            # \n",
    "                    \n",
    "            x_mean_ancle = (x_mean*2-1).acos()  \n",
    "            \n",
    "            ang_inc = self.ang_inc.unsqueeze(-1).expand(x_mean_ancle.shape) \n",
    "            # ang_inc = np.pi/2/(x.max(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) -x.min(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) )\n",
    "            \n",
    "            if given_ang!=-1:\n",
    "                x_mean_rote = (np.pi/2 - x_mean_ancle)*given_ang\n",
    "            else:\n",
    "                x_mean_rote = (np.pi/2 - x_mean_ancle)*ang_inc\n",
    "            \n",
    "            x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot\n",
    "                                                \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+x_mean_rote  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "      \n",
    "        return x_1\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        self.ang_inc.data.zeros_()\n",
    "        \n",
    "def print_degree(x,name=\"x\"):\n",
    "    print(name,x/np.pi*180)\n",
    "    \n",
    "    \n",
    "class QC_Norm_Real(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Real, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.x_max = 0\n",
    "        self.x_min = 0\n",
    "        # print(\"Using Normal without real\")\n",
    "        \n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            # x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_final = ((x_ancle-self.x_min)/(self.x_max-self.x_min))*np.pi\n",
    "            \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            x = x.transpose(0,1)        \n",
    "            x_ancle = (x*2-1).acos()     \n",
    "            x_rectify_ancle = (x_ancle.max(-1)[0]-x_ancle.min(-1)[0]).unsqueeze(-1).expand(x.shape)                                                                         \n",
    "            x_final = ((x_ancle-x_ancle.min(-1)[0].unsqueeze(-1))/(x_rectify_ancle))*np.pi\n",
    "            \n",
    "            x_moving_rot = x_final - x_ancle\n",
    "            \n",
    "            x_moving_rot_mean = x_moving_rot.sum(-1)/x.shape[-1] \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot_mean      \n",
    "            \n",
    "            self.x_max = self.momentum * x_ancle.max(-1)[0].unsqueeze(-1) + \\\n",
    "                                    (1 - self.momentum) * self.x_max\n",
    "            self.x_min = self.momentum * x_ancle.min(-1)[0].unsqueeze(-1) + \\\n",
    "                                    (1 - self.momentum) * self.x_min\n",
    "            \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "            \n",
    "        return x_1\n",
    "\n",
    "\n",
    "class QC_Norm_Real_Correction(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Real_Correction, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:            \n",
    "            \n",
    "            x = x.transpose(0,1)                    \n",
    "            x_ancle = (x*2-1).acos()                        \n",
    "            x_moving_rot = -1*(x_ancle.min(-1)[0])\n",
    "            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot                                                    \n",
    "            x_final = x_ancle+x_moving_rot.unsqueeze(-1)                                    \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        return x_1\n",
    "\n",
    "class QC_Norm_Correction(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Correction, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)                \n",
    "            x_mean = x_sum/x.shape[-1]\n",
    "                                \n",
    "            x_mean_ancle = (x_mean*2-1).acos()    \n",
    "            x_mean_rote = (np.pi/2 - x_mean_ancle) \n",
    "            \n",
    "            x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])\n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot                                        \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+x_mean_rote  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "        \n",
    "        return x_1\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm_True(nn.Module):\n",
    "    def __init__(self,num_features):        \n",
    "        super(QC_Norm_True, self).__init__()\n",
    "        \n",
    "        x_r_init =  torch.zeros(num_features)\n",
    "        x_r_init += 0\n",
    "        \n",
    "        self.x_qft = Parameter(torch.zeros(num_features),requires_grad=True)\n",
    "        self.x_hzh = Parameter(torch.zeros(num_features),requires_grad=True)\n",
    "        \n",
    "        self.x_running_rot = Parameter(x_r_init,requires_grad=True)\n",
    "        \n",
    "        self.x_running_means = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.x_running_min = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.x_running_max = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        \n",
    "        self.printed = False\n",
    "        self.momentum = 0.1\n",
    "    def forward(self,x,training=True):\n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                # print(self.x_qft)\n",
    "                # print(self.x_hzh)\n",
    "                print(self.x_running_rot)\n",
    "\n",
    "                # print(((binarize(self.x_qft)+1)/2)*0.5)\n",
    "                # print(((binarize(self.x_qft)+1)/2)*0.5*x)\n",
    "                # print(((1-x)-x)*self.x_running_rot + x)\n",
    "                self.printed = True\n",
    "            x_min_rt = self.x_running_min.detach()\n",
    "            x_max_rt = self.x_running_max.detach()\n",
    "        else:\n",
    "            self.printed = False           \n",
    "            # x_running_rot = self.x_running_rot    \n",
    "            x = x.transpose(0,1)\n",
    "            # x_sum = x.sum(-1)\n",
    "            # x_mean = x_sum/x.shape[-1]\n",
    "            x_min = x.min(1)[0]\n",
    "            x_max = x.max(1)[0]            \n",
    "            x = x.transpose(0,1)\n",
    "            self.x_running_min[:] = self.momentum * self.x_running_min + \\\n",
    "                                (1 - self.momentum) * x_min \n",
    "            self.x_running_max[:] = self.momentum * self.x_running_max + \\\n",
    "                                (1 - self.momentum) * x_max\n",
    "            x_min_rt = x_min\n",
    "            x_max_rt = x_max\n",
    "        \n",
    "        x = (x_min_rt*(1-2*x)+x)\n",
    "            \n",
    "        # x = x+x-2*x*x\n",
    "        \n",
    "        # ry\n",
    "        # init_rot = (x<0.5).float()*np.pi        \n",
    "        # final_rot = init_rot + self.x_running_rot                 \n",
    "        # r = (2*x-1).abs()        \n",
    "        # x = (r*final_rot.cos()+1)/2 \n",
    "        \n",
    "        return x\n",
    "        # QFT\n",
    "        # \n",
    "        # qft_g_10 = (binarize(self.x_qft-10)+1)/2\n",
    "        # qft_g_5 = (binarize(self.x_qft-5)+1)/2 - qft_g_10\n",
    "        # qft_g_0 = (binarize(self.x_qft-0)+1)/2 - qft_g_5 - qft_g_10\n",
    "        # x = qft_g_10*0.038*x + (1-qft_g_10)*x\n",
    "        # x = qft_g_5*0.146*x + (1-qft_g_5)*x\n",
    "        # x = qft_g_0*0.5*x + (1-qft_g_0)*x\n",
    "        \n",
    "        # x = ((binarize(self.x_qft)+1)/2*0.5)*x\n",
    "        # x = (((binarize(self.x_qft-5)+1)/2 - (binarize(self.x_qft-10)+1)/2)*0.146)*x\n",
    "        # x = (((binarize(self.x_qft-0)+1)/2 - (binarize(self.x_qft-5)+1)/2 - (binarize(self.x_qft-10)+1)/2)*0.5)*x\n",
    "        \n",
    "        \n",
    "        # x = (x+(binarize(self.x_hzh)+1)/2*(1-2*x))\n",
    "        # \n",
    "        # x = (((binarize(self.x_qft)+1)/2)*0.5)*x + (((1-(binarize(self.x_qft)+1)/2))*1)*x \n",
    "        # x = (((binarize(self.x_qft-10)+1)/2)*0.1)*x + (((1-(binarize(self.x_qft-10)+1)/2))*1)*x\n",
    "        # \n",
    "        # # hzh <- y\n",
    "        # x = ((1-x)-x)*clipfunc(self.x_hzh) + x        \n",
    "        # \n",
    "        # \n",
    "        # # ry\n",
    "        # init_rot = (x<0.5).float()*np.pi        \n",
    "        # final_rot = init_rot + self.x_running_rot                 \n",
    "        # r = (2*x-1).abs()        \n",
    "        # x1 = (r*final_rot.cos()+1)/2   \n",
    "\n",
    "\n",
    "            \n",
    "        # if not training:\n",
    "        #     \n",
    "        #     # init_rot = (x<0.5).float()*np.pi        \n",
    "        #     # final_rot = init_rot + self.x_running_rot                 \n",
    "        #     # r = (2*x-1).abs()        \n",
    "        #     # x = (r*final_rot.cos()+1)/2   \n",
    "        #     # \n",
    "        #     # x = 1-(x+self.x_running_means-2*x*self.x_running_means)\n",
    "        #     # x = (1-x)/2\n",
    "        #     # x = self.x_running_means*(1-2*x)+x\n",
    "        #     # \n",
    "        #     # if not self.printed:\n",
    "        #     #     print(\"self.x_running_means\",self.x_running_means)\n",
    "        #     #     self.printed = True\n",
    "        #     #     # print(x)\n",
    "        #     return x\n",
    "        # \n",
    "        # else:\n",
    "        #     self.printed = False\n",
    "            \n",
    "            # # ry\n",
    "            # init_rot = (x<0.5).float()*np.pi        \n",
    "            # final_rot = init_rot + self.x_running_rot                 \n",
    "            # r = (2*x-1).abs()        \n",
    "            # x = (r*final_rot.cos()+1)/2   \n",
    "\n",
    "            \n",
    "            \n",
    "            # x = x.transpose(0,1)\n",
    "            # x_sum = x.sum(-1)\n",
    "            # \n",
    "            # # self.x_running_rot = self.x_running_rot.clamp(0,1) \n",
    "            # \n",
    "            # x_mean = x_sum/x.shape[-1]\n",
    "            # \n",
    "            #          \n",
    "            # #          *self.x_hzh+self.x_running_rot/10\n",
    "            # # x_mean = x_mean.clamp(0,1)        \n",
    "            # x = x.transpose(0,1)\n",
    "            #     \n",
    "            \n",
    "            \n",
    "        #     return x\n",
    "        #     \n",
    "        #     \n",
    "        #     sys.exit(0)\n",
    "        #     x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "        #     x_lack_sum = x_sum - x    \n",
    "        #     x_mean = x_lack_sum/x.shape[-1]\n",
    "        #     \n",
    "        #     \n",
    "        #             \n",
    "        #     x_mean_ancle = (x_mean*2-1).acos()  \n",
    "        #     \n",
    "        #     ang_inc = self.ang_inc.unsqueeze(-1).expand(x_mean_ancle.shape) \n",
    "        #     # ang_inc = np.pi/2/(x.max(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) -x.min(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) )\n",
    "        #     \n",
    "        #     if given_ang!=-1:\n",
    "        #         x_mean_rote = (np.pi/2 - x_mean_ancle)*given_ang\n",
    "        #     else:\n",
    "        #         x_mean_rote = (np.pi/2 - x_mean_ancle)*ang_inc\n",
    "        #     \n",
    "        #     x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])            \n",
    "        #     self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "        #                           (1 - self.momentum) * x_moving_rot\n",
    "        #                                         \n",
    "        #     x_ancle = (x*2-1).acos()\n",
    "        #     x_final = x_ancle+x_mean_rote  \n",
    "        #     x_1 = (x_final.cos()+1)/2                                \n",
    "        #     x_1 = x_1.transpose(0,1)\n",
    "        # return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm_try2(nn.Module):\n",
    "    def __init__(self, num_features, batch_size=32, init_ang_inc = 0, momentum=0.1):        \n",
    "        super(QC_Norm_try2, self).__init__()\n",
    "        \n",
    "        self.x_running_rot = Parameter(torch.zeros((num_features,batch_size)),requires_grad=False)        \n",
    "        self.ang_inc = Parameter(torch.ones(1)*init_ang_inc)\n",
    "        \n",
    "        self.momentum = momentum\n",
    "                \n",
    "        self.printed = False\n",
    "        self.x_mean_ancle=0\n",
    "        self.x_mean_rote = 0\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "        \n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                print(\"self.x_running_rot\",self.x_running_rot)\n",
    "                self.printed = True\n",
    "                    \n",
    "            x = x.transpose(0,1)\n",
    "            x_1  = (self.x_running_rot*(1-x)+x)  \n",
    "            x_1 = x_1.transpose(0,1) \n",
    "            # x_ancle = (x*2-1).acos()\n",
    "            # x_final = x_ancle+self.x_running_rot.unsqueeze(-1)\n",
    "            # x_1 = (x_final.cos()+1)/2\n",
    "            #                     \n",
    "            # x_1 = x_1.transpose(0,1)\n",
    "            # print(x_1)\n",
    "        else:\n",
    "            self.printed = False\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "            x_lack_sum = x_sum - x    \n",
    "            x_mean = x_lack_sum/x.shape[-1]\n",
    "            \n",
    "            \n",
    "                                  \n",
    "            # ang_inc = self.ang_inc.unsqueeze(-1).expand(x.shape)    \n",
    "            \n",
    "            y  = ((0.5-x_mean)/(1-x_mean))\n",
    "            # y = clipfunc(y)\n",
    "            \n",
    "            # print(y)\n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * y\n",
    "\n",
    "            # self.x_running_rot[:] = y\n",
    "            \n",
    "            x_1  = (y*(1-x)+x)\n",
    "            x_1 = x_1.transpose(0,1) \n",
    "            # x_1  = (y*(1-x)+x)\n",
    "            # sys.exit(0)\n",
    "            # \n",
    "            # \n",
    "            # x_mean_ancle = (x_mean*2-1).acos()\n",
    "            # ang_inc = self.ang_inc.unsqueeze(-1).expand(x_mean_ancle.shape)             \n",
    "            # \n",
    "            # if given_ang!=-1:\n",
    "            #     x_mean_rote = (np.pi/2 - x_mean_ancle)*given_ang\n",
    "            # else:\n",
    "            #     x_mean_rote = (np.pi/2 - x_mean_ancle)*ang_inc\n",
    "            # \n",
    "            # x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])            \n",
    "            # self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "            #                       (1 - self.momentum) * x_moving_rot\n",
    "            #                                     \n",
    "            # x_ancle = (x*2-1).acos()\n",
    "            # x_final = x_ancle+x_mean_rote  \n",
    "            # x_1 = (x_final.cos()+1)/2                                \n",
    "            # x_1 = x_1.transpose(0,1)\n",
    "      \n",
    "        return x_1\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        self.ang_inc.data.zeros_()\n",
    "\n",
    "\n",
    "class QC_Norm_try3(nn.Module):\n",
    "    def __init__(self, num_features, batch_size=32, init_ang_inc = 1, momentum=0.1):        \n",
    "        super(QC_Norm_try3, self).__init__()\n",
    "        \n",
    "        self.x_running_rot = Parameter(torch.zeros((num_features)),requires_grad=False)        \n",
    "        # self.ang_inc = Parameter(torch.ones((num_features)),requires_grad=True)\n",
    "        self.ang_inc = Parameter(torch.tensor(init_ang_inc,dtype=torch.float32),requires_grad=True)        \n",
    "        self.momentum = momentum\n",
    "                \n",
    "        self.printed = False\n",
    "        self.x_mean_ancle=0\n",
    "        self.x_mean_rote = 0\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "        \n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                # print(\"self.ang_inc\",self.ang_inc)\n",
    "                self.printed = True                            \n",
    "            x_1  = (self.x_running_rot*x)               \n",
    "     \n",
    "        else:\n",
    "            self.printed = False\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "            x_lack_sum = x_sum + x    \n",
    "            x_mean = x_lack_sum/x.shape[-1]                    \n",
    "            \n",
    "            \n",
    "            \n",
    "            ang_inc = (self.ang_inc>0).float()*self.ang_inc+1\n",
    "            # ang_inc = self.ang_inc.abs()+1\n",
    "            # ang_inc = 2\n",
    "            y  = 0.5/x_mean\n",
    "            y = y.transpose(0,1)\n",
    "            y = y/ang_inc\n",
    "            y = y.transpose(0,1)\n",
    "    \n",
    "            x_moving_rot = (y.sum(-1)/x.shape[-1]) \n",
    "            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot\n",
    "            \n",
    "            x_1  = y*x\n",
    "            x_1 = x_1.transpose(0,1) \n",
    "            \n",
    "        return x_1\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        self.ang_inc.data.zeros_()\n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm_Correction_try2(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Correction_try2, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        self.x_l_0_5 = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.x_g_0_5 = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:            \n",
    "            x_1  = self.x_l_0_5*(self.x_running_rot*(1-x)+x)\n",
    "            x_1 += self.x_g_0_5*(self.x_running_rot*x)            \n",
    "        else:\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1)            \n",
    "            x_mean = x_sum/x.shape[-1]\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.x_l_0_5[:] = ((x_mean<=0.5).float())\n",
    "            self.x_g_0_5[:] = ((x_mean>0.5).float())\n",
    "            \n",
    "            y  = self.x_l_0_5*((0.5-x_mean)/(1-x_mean))\n",
    "            y += self.x_g_0_5*(0.5/x_mean)\n",
    "            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * y\n",
    "            \n",
    "            \n",
    "            x = x.transpose(0,1)  \n",
    "            x_1  = self.x_l_0_5*(y*(1-x)+x)\n",
    "            x_1 += self.x_g_0_5*(y*x)\n",
    "            \n",
    "        return x_1\n",
    "    \n",
    "\n",
    "class BinaryLinearClassic(nn.Linear):\n",
    "\n",
    "    def forward(self, input):\n",
    "        binary_weight = binarize(self.weight)\n",
    "        if self.bias is None:\n",
    "            output = F.linear(input, binary_weight)\n",
    "            output = torch.div(output, input.shape[-1])\n",
    "            output = torch.pow(output, 2)\n",
    "\n",
    "            return output\n",
    "        else:\n",
    "            print(\"Not Implement\")\n",
    "            sys.exit(0)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Glorot initialization\n",
    "        in_features, out_features = self.weight.size()\n",
    "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "        self.weight.lr_scale = 1. / stdv\n",
    "\n",
    "\n",
    "\n",
    "## Define the NN architecture\n",
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc1 = BinaryLinear(2,num_f1,bias=False)\n",
    "        # self.fc2 = BinaryLinear(num_f1,num_f2,bias=False)\n",
    "        # self.fc3 = BinaryLinear(num_f2,num_f3,bias=False)\n",
    "        # # # \n",
    "        # \n",
    "        # # if not with_norm or True:\n",
    "        # self.clc_fc1 = BinaryLinearClassic(in_features=2, out_features=num_f1, bias=False)            \n",
    "        # self.clc_fc2 = BinaryLinearClassic(in_features=num_f1, out_features=num_f2, bias=False)\n",
    "        # self.clc_fc3 = BinaryLinearClassic(in_features=num_f2, out_features=num_f3, bias=False)\n",
    "        # \n",
    "        # \n",
    "        # self.clc_bn1 = nn.BatchNorm1d(num_features=num_f1)\n",
    "        # self.clc_bn2 = nn.BatchNorm1d(num_features=num_f2)\n",
    "        if with_norm:\n",
    "            self.qc1 = QC_Norm_try3(num_features=num_f1,init_ang_inc=[1])\n",
    "            # self.qc2 = QC_Norm_try3(num_features=num_f2,init_ang_inc=[1,1])\n",
    "            # self.qc3 = QC_Norm(num_features=num_f3)\n",
    "    \n",
    "            # self.qc1a = QC_Norm_True(num_features=num_f1)\n",
    "            # self.qc2a = QC_Norm_True(num_features=num_f2)\n",
    "            # self.qc3a = QC_Norm_Correction(num_features=num_f3)\n",
    "            \n",
    "            self.qc1a = QC_Norm_Correction_try2(num_features=num_f1)\n",
    "            # self.qc2a = QC_Norm_Correction_try2(num_features=num_f2)\n",
    "            \n",
    "            # \n",
    "            # self.qc1 = QC_Norm_Real(num_features=num_f1)\n",
    "            # self.qc2 = QC_Norm_Real(num_features=num_f2)\n",
    "            # self.qc3 = QC_Norm_Real(num_features=num_f3)\n",
    "\n",
    "\n",
    "        # self.qc1a = QC_Norm_Real_Correction(num_features=num_f1)\n",
    "        # self.qc2a = QC_Norm_Real_Correction(num_features=num_f2)\n",
    "        # self.qc3a = QC_Norm_Real_Correction(num_features=num_f3)\n",
    "        # \n",
    "    def forward(self, x, training=1):        \n",
    "        x = x.view(-1, 2)\n",
    "        \n",
    "        if training == 1:\n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            # \n",
    "            \n",
    "\n",
    "            if with_norm:\n",
    "                # x = self.qc1a(self.fc1(x))                \n",
    "                # x = self.qc2a(self.fc2(x))\n",
    "                                                \n",
    "                \n",
    "                x = self.qc1(self.qc1a(self.fc1(x)))\n",
    "                # x = (self.qc2a(self.fc2(x)))\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                # print(x)\n",
    "                # x = self.fc1(x)\n",
    "                # print(x)\n",
    "                # x = self.qc1a(x)\n",
    "                # print(x)\n",
    "                # x = self.qc1(x)\n",
    "                # print(x)\n",
    "                # \n",
    "                # print(x)\n",
    "                # x = self.fc2(x)\n",
    "                # print(x)\n",
    "                # x = self.qc2a(x)\n",
    "                # print(x)\n",
    "                # x = self.qc2(x)\n",
    "                # print(x)\n",
    "                \n",
    "                # print(x)\n",
    "                # sys.exit(0)\n",
    "            else:\n",
    "                x = self.clc_fc1(x)        \n",
    "                x = self.clc_fc2(x)\n",
    "                # x = self.clc_fc3(x)  \n",
    "\n",
    "            # x = self.qc3(self.qc3a(self.fc3(x)))\n",
    "            # \n",
    "            # x = self.qc1((self.fc1(x)))        \n",
    "            # x = self.qc2((self.fc2(x)))                           \n",
    "            # x = self.qc3((self.fc3(x)))\n",
    "            # \n",
    "        elif training == 2:\n",
    "            \n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            \n",
    "            print(\"=\"*10,\"layer 1\",\"=\"*10)\n",
    "            print(x)\n",
    "            torch.set_printoptions(profile=\"full\")            \n",
    "            print(binarize(self.fc1.weight))                                            \n",
    "            torch.set_printoptions(profile=\"default\")\n",
    "            x = self.fc1(x)            \n",
    "            \n",
    "            print(\"=\"*10,\"layer 2\",\"=\"*10)\n",
    "            print(x)\n",
    "            torch.set_printoptions(profile=\"full\")            \n",
    "            print(binarize(self.fc2.weight))                                            \n",
    "            torch.set_printoptions(profile=\"default\")\n",
    "            x = self.fc2(x)\n",
    "            \n",
    "            print(\"=\"*10,\"results\",\"=\"*10)\n",
    "            print(x)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # x = self.qc1(self.fc1(x),training=False)\n",
    "            #     x = self.qc2(self.fc2(x),training=False)\n",
    "            # x = self.qc3(self.fc3(x),training=False)\n",
    "            # \n",
    "            \n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            # \n",
    "            # x = self.qc3(self.qc3a(self.fc3(x),training=False),training=False)\n",
    "            \n",
    "            if with_norm:                \n",
    "                if not print_detail:\n",
    "                    x = self.qc1(self.qc1a(self.fc1(x),training=False),training=False)                \n",
    "                    # x = (self.qc2a(self.fc2(x),training=False))\n",
    "                else:\n",
    "                    print(x)\n",
    "                    x = self.fc1(x)\n",
    "                    print(x)\n",
    "                    x = self.qc1a(x,training=False)\n",
    "                    print(x)\n",
    "                    x = self.qc1(x,training=False)\n",
    "                    print(x)\n",
    "    \n",
    "                    # print(x)\n",
    "                    # x = self.fc2(x)\n",
    "                    # print(x)\n",
    "                    # x = self.qc2a(x,training=False)\n",
    "                    # print(x)\n",
    "                    # # x = self.qc2(x,training=False)\n",
    "                    # print(x)\n",
    "                    # \n",
    "                    # x = self.qc1a(self.fc1(x),training=False)                \n",
    "                    # x = self.qc2a(self.fc2(x),training=False)                                        \n",
    "            else:\n",
    "                x = self.clc_fc1(x)                    \n",
    "                x = self.clc_fc2(x)\n",
    "                # x = self.clc_fc3(x)  \n",
    "            \n",
    "        if num_f2==1 or num_f1==1:            \n",
    "            x = torch.cat((x,1-x),-1)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    epoch_loss = []\n",
    "    batch_idx = 0\n",
    "    for (data, target) in dataset:\n",
    "        \n",
    "        \n",
    "        target = target.view(batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # target,new_target = modify_target(target)\n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        # \n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data,True)\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        \n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()    \n",
    "        \n",
    "        # print(output.shape,target.shape)\n",
    "        \n",
    "        # sys.exit(0)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "                \n",
    "        if batch_idx % 20 == 0:        \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}/{} ({:.2f}%)'.format(\n",
    "                epoch, batch_idx * len(data), len(dataset),\n",
    "                100. * batch_idx / len(dataset), loss, correct, (batch_idx+1) * len(data),\n",
    "                100. * float(correct) / float(((batch_idx+1) * len(data)) )))                \n",
    "        batch_idx += 1\n",
    "    print(\"-\"*20,\"training done, loss\",\"-\"*20)\n",
    "    print(\"Training Set: Average loss: {}\".format(round(sum(epoch_loss)/len(epoch_loss),6)))\n",
    "    \n",
    "accur=[]\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in dataset:\n",
    "        # target,new_target = modify_target(target)\n",
    "        target = target.view(batch_size)\n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        \n",
    "        # print(\"=\"*100)        \n",
    "        # print(data)\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # print(\"Debug\")\n",
    "        # output = model(data,2)\n",
    "        # \n",
    "        \n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data,False)\n",
    "        \n",
    "        # \n",
    "        test_loss += criterion(output, target) # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get t\n",
    "        # print(pred.view(batch_size))# he index of the max log-probability\n",
    "        # print(target,output)\n",
    "        # sys.exit(0)\n",
    "        # print(pred.eq(target.data.view_as(pred)).cpu().sum())\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    a=100.*correct / len(dataset)*batch_size\n",
    "    accur.append(a)  \n",
    "    test_loss /= len(dataset)*batch_size\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(dataset)*batch_size,\n",
    "        100. * float(correct) / float(len(dataset)*batch_size)))\n",
    "    \n",
    "    return float(correct) / (len(dataset)*batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Training\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "print(\"=\"*10,\"Model Info\",\"=\"*10)\n",
    "print(model)\n",
    "# summary(model,(1,img_size,img_size))\n",
    "\n",
    "# \n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "if with_norm and given_ang==-1:\n",
    "    optimizer = torch.optim.Adam([\n",
    "                    {'params': model.fc1.parameters()},\n",
    "                    # {'params': model.fc2.parameters()},\n",
    "                    # {'params': model.fc3.parameters()},\n",
    "                    {'params': model.qc1.parameters(), 'lr': init_qc_lr},\n",
    "                    # {'params': model.qc2.parameters(), 'lr': init_qc_lr},\n",
    "                    # {'params': model.qc3.parameters(), 'lr': 1},\n",
    "                ], lr=init_lr)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# optimizer = torch.optim.SGD([\n",
    "#                 {'params': model.fc1.parameters()},\n",
    "#                 {'params': model.fc2.parameters()},\n",
    "#                 {'params': model.fc3.parameters()},\n",
    "#                 {'params': model.qc1.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc2.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc3.parameters(), 'lr': 1},\n",
    "#             ], lr=0.1, momentum=0.9)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \\\n",
    "#         base_lr=[1e-1,1e-1,1e-1,1,1,1], \\\n",
    "#         max_lr=[1e-3,1e-3,1e-3,1e-2,1e-2,1e-2], \\\n",
    "#         step_size_up=100\n",
    "#         )\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)\n",
    "\n",
    "# \n",
    "# \n",
    "# test()\n",
    "# \n",
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile(resume_path):\n",
    "    print(\"=> loading checkpoint from '{}'<=\".format(resume_path))\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    epoch_init,acc = checkpoint[\"epoch\"],checkpoint[\"acc\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])    \n",
    "    scheduler.milestones = Counter(milestones)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "else:\n",
    "    epoch_init,acc = 0,0\n",
    "\n",
    "# for name,para in model.named_parameters():\n",
    "#     print(name,para)\n",
    "\n",
    "# sys.exit(0)\n",
    "import copy\n",
    "best_model = -1\n",
    "acc= 0\n",
    "if training:\n",
    "    for epoch in range(epoch_init, max_epoch + 1):\n",
    "        print(\"=\"*20,epoch,\"epoch\",\"=\"*20)  \n",
    "        print(\"Epoch Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))        \n",
    "\n",
    "        print(\"-\"*20,\"learning rates\",\"-\"*20)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(param_group['lr'],end=\",\")\n",
    "        print()    \n",
    "        \n",
    "        print(\"-\"*20,\"training\",\"-\"*20)\n",
    "        print(\"Trainign Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        train(epoch)\n",
    "        print(\"Trainign End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        \n",
    "        # \n",
    "        # for name,para in model.named_parameters():\n",
    "        #     print(name,para)\n",
    "        # \n",
    "        print(\"-\"*20,\"testing\",\"-\"*20)\n",
    "        print(\"Testing Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))        \n",
    "        cur_acc = test()\n",
    "        print(\"Testing End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"-\"*60)\n",
    "        print()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        is_best = False\n",
    "        if cur_acc > acc:\n",
    "            is_best = True\n",
    "            acc=cur_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "            \n",
    "            \n",
    "        \n",
    "        print(\"Best accuracy: {}; Current accuracy {}. Checkpointing\".format(acc,cur_acc))\n",
    "        if save_chkp:\n",
    "            save_checkpoint({\n",
    "              'epoch': epoch + 1,\n",
    "              'acc': acc, \n",
    "              'state_dict': model.state_dict(),      \n",
    "              'optimizer' : optimizer.state_dict(),\n",
    "               'scheduler': scheduler.state_dict(),\n",
    "            }, is_best, save_path, 'checkpoint_{}_{}.pth.tar'.format(epoch,round(cur_acc,4)))\n",
    "        print(\"Epoch End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"=\"*60)\n",
    "        print()        \n",
    "else:    \n",
    "    # print(\"=\"*20,epoch,\"Testing\",\"=\"*20)  \n",
    "    # test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "    #     num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "    # test()\n",
    "\n",
    "\n",
    "\n",
    "    print(\"=\"*100)\n",
    "    for name,para in model.named_parameters():\n",
    "        print(name,para)\n",
    "    print(\"=\"*100)\n",
    "\n",
    "\n",
    "    # output = model(torch.tensor([0.2,0.6]),False)\n",
    "    # print(output[0][0],output[0][1])\n",
    "    \n",
    "\n",
    "    # \n",
    "    # # # \n",
    "    # # model(torch.tensor([0.95,0.95]),False)\n",
    "    # sys.exit(0)\n",
    "    # \n",
    "    gap = []\n",
    "    pair = {}\n",
    "    for i in range(i_dim):\n",
    "        for j in range(j_dim):\n",
    "            input = ([(1.0/i_dim)*(i+1),(1.0/j_dim)*(j+1)])            \n",
    "            output = model(torch.tensor(input),False)\n",
    "            gap.append((output[0][0]-output[0][1]).data)\n",
    "            pair[tuple(input)] = [output[0][0],output[0][1]]\n",
    "            # print(input,output.data,(output[0][0]-output[0][1]).data)\n",
    "    \n",
    "    \n",
    "    for k,v in pair.items():        \n",
    "        # if abs(v[0]-v[1]) in torch.tensor(gap).abs().topk(70)[0]: #and golden_data[k]==1:\n",
    "        print((v[0].data-v[1].data).abs(),v[0].data,v[1].data,golden_data[k],k)\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "if training:\n",
    "    print(\"=\"*100)\n",
    "    for name,para in best_model.named_parameters():\n",
    "        print(name,para)\n",
    "    print(\"=\"*100) \n",
    "    for i in range(i_dim):\n",
    "        for j in range(j_dim):\n",
    "            data = dataset_ori[i*j_dim+j][0]\n",
    "            output = model(data,False)\n",
    "    \n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            print(pred.data.item(),end = \" \")\n",
    "        print()\n",
    "    \n",
    "    for i in range(i_dim):\n",
    "        for j in range(j_dim):\n",
    "            data = dataset_ori[i*j_dim+j][0]\n",
    "            output = model(data,False)\n",
    "    \n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            print(pred.data.item(),end = \",\")\n",
    "    print()     \n",
    "\n",
    "sys.exit(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "==============================\n",
      "tensor([1.8442])\n",
      "==============================\n",
      "tensor([0.7650])\n",
      "tensor([0.3488])\n",
      "==========\n",
      "tensor([2.2143])\n",
      "tensor([1.7722])\n",
      "tensor([0.6435])\n",
      "tensor([1.1593])\n",
      "tensor([1.4505, 2.0715, 1.8132])\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch\n",
    "print(\"=\"*30)\n",
    "x = 0.95\n",
    "y = 0.65\n",
    "print(torch.tensor([((x*(1-y)+y*(1-x))*2)-1]).acos())\n",
    "# print(torch.tensor([(0.3*2)-1]).acos())\n",
    "# print(torch.tensor([(0.1*2)-1]).acos())\n",
    "print(\"=\"*30)\n",
    "print(torch.tensor([1-(0.1393*2)]).acos())\n",
    "print(torch.tensor([(0.9699*2)-1]).acos())\n",
    "# print(torch.tensor([(0.8607*0.9699*2)-1]).acos())\n",
    "# print(torch.tensor([(0.0733*2)-1]).acos())\n",
    "\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(torch.tensor([1-(0.8*2)]).acos())\n",
    "print(torch.tensor([1-(0.6*2)]).acos())\n",
    "print(torch.tensor([1-(0.1*2)]).acos())\n",
    "print(torch.tensor([1-(0.3*2)]).acos())\n",
    "\n",
    "x = torch.tensor([0.8,0.8,0.8])\n",
    "y = torch.tensor([0.6,0.1,0.3])\n",
    "print((1-((1-x)*y+(1-y)*x)*2).acos())\n",
    "# \n",
    "# print(torch.tensor([1-((0.8*0.4+0.2*0.6)*2)]).acos())\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../interfae\")\n",
    "from qiskit_simulator import *\n",
    "\n",
    "\n",
    "\n",
    "accur=[]\n",
    "def test_debug():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target,new_target = modify_target(target)\n",
    "        \n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # print(\"Debug\",data,target)\n",
    "        # output = model(data,2)\n",
    "        # \n",
    "        # run_simulator(model,data)\n",
    "        \n",
    "        output = model(data,False)\n",
    "        # print(data)\n",
    "        # print(output)\n",
    "        # print(target)\n",
    "        # sys.exit(0)\n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "        # output = model(data,False)\n",
    "        test_loss += criterion(output, target) # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    a=100.*correct / len(test_loader.dataset)\n",
    "    accur.append(a)  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / float(len(test_loader.dataset))))\n",
    "    \n",
    "    return float(correct) / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile(resume_path):\n",
    "    print(\"=> loading checkpoint from '{}'<=\".format(resume_path))\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    epoch_init,acc = checkpoint[\"epoch\"],checkpoint[\"acc\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])    \n",
    "    scheduler.milestones = Counter(milestones)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "else:\n",
    "    epoch_init,acc = 0,0\n",
    "\n",
    "print(\"HERE\")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "        num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "test_debug()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for name, para in model.named_parameters():\n",
    "# #     print(name,para)\n",
    "# print(\"HERE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8213722",
   "language": "python",
   "display_name": "PyCharm (qiskit_practice)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}