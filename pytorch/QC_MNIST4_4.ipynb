{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Training procedure for Quantum Computer:\n",
      "\tStart at: 04/23/2020 21:48:03\n",
      "\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\n",
      "\tEnjoy and Good Luck!\n",
      "====================================================================================================\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import functools\n",
    "\n",
    "from mnist import *\n",
    "\n",
    "from collections import Counter\n",
    "print = functools.partial(print, flush=True)\n",
    "\n",
    "# For 4*4, 16->4->2: batch_size=32; init_lr=0.01; with_norm=True or False\n",
    "# For 4*4, 16->4->1: batch_size=16; init_lr=0.1; with_norm=True, ang:20; or train\n",
    "\n",
    "# interest_num = [0,1,2,3,4,5,6,7,8,9]\n",
    "interest_num = [1,3,6]\n",
    "img_size = 4\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 16\n",
    "inference_batch_size = 16\n",
    "num_f1 = 8\n",
    "# num_f2 = len(interest_num)\n",
    "num_f2 = len(interest_num)\n",
    "init_lr = 0.1\n",
    "init_qc_lr = 1\n",
    "with_norm = False\n",
    "save_chkp = False\n",
    "# Given_ang to -1 to train the variable \n",
    "given_ang = -1\n",
    "\n",
    "\n",
    "save_to_file = False\n",
    "if save_to_file:\n",
    "    sys.stdout = open(save_path+\"/log\", 'w')\n",
    "\n",
    "if save_chkp:\n",
    "    save_path = \"./model/\"+os.path.basename(sys.argv[0])+\"_\"+time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "    Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# resume_path = \"./model/ipykernel_launcher.py_2020_04_22-15_15_31/model_best.tar\"\n",
    "\n",
    "resume_path = \"\"\n",
    "training = True\n",
    "max_epoch = 10\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Training procedure for Quantum Computer:\")\n",
    "print(\"\\tStart at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "print(\"\\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\")\n",
    "print(\"\\tEnjoy and Good Luck!\")\n",
    "print(\"=\"*100)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def modify_target_ori(target):\n",
    "    for j in range(len(target)):\n",
    "        for idx in range(len(interest_num)):\n",
    "            if target[j] == interest_num[idx]:                \n",
    "                target[j] = idx\n",
    "                break\n",
    "    \n",
    "\n",
    "    new_target = torch.zeros(target.shape[0],len(interest_num))\n",
    "        \n",
    "    for i in range(target.shape[0]):        \n",
    "        one_shot = torch.zeros(len(interest_num))\n",
    "        one_shot[target[i].item()] = 1        \n",
    "        new_target[i] = one_shot.clone()\n",
    "             \n",
    "    return target,new_target\n",
    "\n",
    "def modify_target(target):\n",
    "    new_target = torch.zeros(target.shape[0],len(interest_num))\n",
    "        \n",
    "    for i in range(target.shape[0]):        \n",
    "        one_shot = torch.zeros(len(interest_num))\n",
    "        one_shot[target[i].item()] = 1        \n",
    "        new_target[i] = one_shot.clone()\n",
    "    return target,new_target\n",
    "\n",
    "def select_num(dataset,interest_num):\n",
    "    labels = dataset.targets #get labels\n",
    "    labels = labels.numpy()\n",
    "    idx = {}\n",
    "    for num in interest_num:\n",
    "        idx[num] = np.where(labels == num)\n",
    "        \n",
    "    fin_idx = idx[interest_num[0]]\n",
    "    for i in range(1,len(interest_num)):           \n",
    "        \n",
    "        fin_idx = (np.concatenate((fin_idx[0],idx[interest_num[i]][0])),)\n",
    "    \n",
    "    fin_idx = fin_idx[0]    \n",
    "    \n",
    "    dataset.targets = labels[fin_idx]\n",
    "    dataset.data = dataset.data[fin_idx]\n",
    "    \n",
    "    # print(dataset.targets.shape)\n",
    "    \n",
    "    dataset.targets,_ = modify_target_ori(dataset.targets)\n",
    "    # print(dataset.targets.shape)\n",
    "    \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.Compose([transforms.Resize((img_size,img_size)),transforms.ToTensor()])\n",
    "# transform = transforms.Compose([transforms.Resize((img_size,img_size)),transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "train_data = select_num(train_data,interest_num)\n",
    "test_data =  select_num(test_data,interest_num)\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "    num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    filename = os.path.join(save_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        bestname = os.path.join(save_path, 'model_best.tar')\n",
    "        shutil.copyfile(filename, bestname)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "========== Model Info ==========\n",
      "Net(\n",
      "  (fc1): BinaryLinear(in_features=16, out_features=8, bias=False)\n",
      "  (fc2): BinaryLinear(in_features=8, out_features=3, bias=False)\n",
      ")\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    " \n",
    "# \n",
    "# class BinarizeF(Function):\n",
    "# \n",
    "#     @staticmethod\n",
    "#     def forward(cxt, input):\n",
    "#         output = input.new(input.size())\n",
    "#         output[input >= 0] = 1\n",
    "#         output[input < 0] = -1\n",
    "#         \n",
    "#               \n",
    "#         return output\n",
    "# \n",
    "#     @staticmethod\n",
    "#     def backward(cxt, grad_output):\n",
    "#         grad_input = grad_output.clone()\n",
    "#         return grad_input\n",
    "# # aliases\n",
    "# binarize = BinarizeF.apply\n",
    "# \n",
    "# \n",
    "# class ClipF(Function):\n",
    "# \n",
    "#     @staticmethod\n",
    "#     def forward(ctx, input):\n",
    "#         output = input.clone().detach()\n",
    "#         # output = input.new(input.size())\n",
    "#         output[input >= 1] = 1\n",
    "#         output[input <= 0] = 0\n",
    "#         ctx.save_for_backward(input)\n",
    "#         return output\n",
    "# \n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         input, = ctx.saved_tensors\n",
    "#         grad_input = grad_output.clone()\n",
    "#         grad_input[input >= 1] = 0\n",
    "#         grad_input[input <= 0] = 0\n",
    "#         return grad_input\n",
    "# \n",
    "# \n",
    "# # aliases\n",
    "# clipfunc = ClipF.apply\n",
    "# \n",
    "# \n",
    "# class BinaryLinear(nn.Linear):\n",
    "#     \n",
    "#     \n",
    "#     def do_slp_via_th(self,input_ori,w_ori):\n",
    "#         p = input_ori\n",
    "#         d = 4*p*(1-p)\n",
    "#         e = (2*p-1)\n",
    "#         # e_sq = torch.tensor(1)\n",
    "#         w = w_ori\n",
    "#         \n",
    "#         sum_of_sq = (d+e.pow(2)).sum(-1)\n",
    "#         sum_of_sq = sum_of_sq.unsqueeze(-1)        \n",
    "#         sum_of_sq = sum_of_sq.expand(p.shape[0], w.shape[0])\n",
    "#                 \n",
    "#         diag_p = torch.diag_embed(e)        \n",
    "#         \n",
    "#         p_w = torch.matmul(w,diag_p)\n",
    "#         \n",
    "#         z_p_w = torch.zeros_like(p_w)        \n",
    "#         shft_p_w = torch.cat((p_w, z_p_w), -1)\n",
    "#         \n",
    "#         sum_of_cross = torch.zeros_like(p_w)\n",
    "#         length = p.shape[1]    \n",
    "#         \n",
    "#         for shft in range(1,length):    \n",
    "#             sum_of_cross += shft_p_w[:,:,0:length]*shft_p_w[:,:,shft:length+shft]\n",
    "# \n",
    "#         sum_of_cross = sum_of_cross.sum(-1)\n",
    "#                 \n",
    "#         return (sum_of_sq+2*sum_of_cross)/(length**2) \n",
    "#     \n",
    "#     def forward(self, input):        \n",
    "#         binary_weight = binarize(self.weight)        \n",
    "#         if self.bias is None:\n",
    "#             return self.do_slp_via_th(input,binary_weight)\n",
    "#                       \n",
    "#         else:   \n",
    "#             \n",
    "#             bias_one  = torch.ones(input.shape[0],1)            \n",
    "#             new_input = torch.cat((input, bias_one), -1)            \n",
    "#             bias = clipfunc(self.bias).unsqueeze(1)            \n",
    "#             new_weight = binary_weight            \n",
    "#             new_weight = torch.cat((new_weight,bias),-1)                        \n",
    "#             return self.do_slp_via_th(new_input,new_weight)\n",
    "#             \n",
    "#             \n",
    "#             torch.set_printoptions(edgeitems=64)\n",
    "#             # binary_bias = binarize(self.bias)/float(len(input[0].flatten())+1)\n",
    "#             binary_bias = binarize(self.bias)/float(len(input[0].flatten())+1)\n",
    "#             res = F.linear(input, binary_weight/float(len(input[0].flatten())+1), binary_bias)\n",
    "#             return res\n",
    "# \n",
    "#     def reset_parameters(self):\n",
    "#         # Glorot initialization\n",
    "#         in_features, out_features = self.weight.size()\n",
    "#         stdv = math.sqrt(1.5 / (in_features + out_features))\n",
    "#         self.weight.data.uniform_(-stdv, stdv)\n",
    "#         if self.bias is not None:\n",
    "#             self.bias.data.zero_()\n",
    "# \n",
    "#         self.weight.lr_scale = 1. / stdv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm(nn.Module):\n",
    "    def __init__(self, num_features, init_ang_inc = 10, momentum=0.1):        \n",
    "        super(QC_Norm, self).__init__()\n",
    "        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)        \n",
    "        self.ang_inc = Parameter(torch.ones(1)*init_ang_inc)\n",
    "        \n",
    "        self.momentum = momentum\n",
    "                \n",
    "        self.printed = False\n",
    "        self.x_mean_ancle=0\n",
    "        self.x_mean_rote = 0\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                print(\"self.ang_inc\",self.ang_inc)\n",
    "                self.printed = True\n",
    "                    \n",
    "            x = x.transpose(0,1)\n",
    "  \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)\n",
    "            x_1 = (x_final.cos()+1)/2\n",
    "                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            self.printed = False\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "            x_lack_sum = x_sum - x    \n",
    "            x_mean = x_lack_sum/x.shape[-1]\n",
    "            \n",
    "            \n",
    "                    \n",
    "            x_mean_ancle = (x_mean*2-1).acos()  \n",
    "            \n",
    "            ang_inc = self.ang_inc.unsqueeze(-1).expand(x_mean_ancle.shape) \n",
    "            # ang_inc = np.pi/2/(x.max(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) -x.min(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) )\n",
    "            \n",
    "            if given_ang!=-1:\n",
    "                x_mean_rote = (np.pi/2 - x_mean_ancle)*given_ang\n",
    "            else:\n",
    "                x_mean_rote = (np.pi/2 - x_mean_ancle)*ang_inc\n",
    "            \n",
    "            x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot\n",
    "                                                \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+x_mean_rote  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "      \n",
    "        return x_1\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        self.ang_inc.data.zeros_()\n",
    "        \n",
    "def print_degree(x,name=\"x\"):\n",
    "    print(name,x/np.pi*180)\n",
    "    \n",
    "    \n",
    "class QC_Norm_Real(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Real, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.x_max = 0\n",
    "        self.x_min = 0\n",
    "        # print(\"Using Normal without real\")\n",
    "        \n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            # x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_final = ((x_ancle-self.x_min)/(self.x_max-self.x_min))*np.pi\n",
    "            \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            x = x.transpose(0,1)        \n",
    "            x_ancle = (x*2-1).acos()     \n",
    "            x_rectify_ancle = (x_ancle.max(-1)[0]-x_ancle.min(-1)[0]).unsqueeze(-1).expand(x.shape)                                                                         \n",
    "            x_final = ((x_ancle-x_ancle.min(-1)[0].unsqueeze(-1))/(x_rectify_ancle))*np.pi\n",
    "            \n",
    "            x_moving_rot = x_final - x_ancle\n",
    "            \n",
    "            x_moving_rot_mean = x_moving_rot.sum(-1)/x.shape[-1] \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot_mean      \n",
    "            \n",
    "            self.x_max = self.momentum * x_ancle.max(-1)[0].unsqueeze(-1) + \\\n",
    "                                    (1 - self.momentum) * self.x_max\n",
    "            self.x_min = self.momentum * x_ancle.min(-1)[0].unsqueeze(-1) + \\\n",
    "                                    (1 - self.momentum) * self.x_min\n",
    "            \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "            \n",
    "        return x_1\n",
    "\n",
    "\n",
    "class QC_Norm_Real_Correction(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Real_Correction, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:            \n",
    "            \n",
    "            x = x.transpose(0,1)                    \n",
    "            x_ancle = (x*2-1).acos()                        \n",
    "            x_moving_rot = -1*(x_ancle.min(-1)[0])\n",
    "            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot                                                    \n",
    "            x_final = x_ancle+x_moving_rot.unsqueeze(-1)                                    \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        return x_1\n",
    "\n",
    "class QC_Norm_Correction(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Correction, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)                \n",
    "            x_mean = x_sum/x.shape[-1]\n",
    "                                \n",
    "            x_mean_ancle = (x_mean*2-1).acos()    \n",
    "            x_mean_rote = (np.pi/2 - x_mean_ancle) \n",
    "            \n",
    "            x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])\n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot                                        \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+x_mean_rote  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "        \n",
    "        return x_1\n",
    "\n",
    "## Define the NN architecture\n",
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc1 = BinaryLinear(img_size*img_size,num_f1,bias=False)\n",
    "        self.fc2 = BinaryLinear(num_f1,num_f2,bias=False)\n",
    "        # self.fc3 = BinaryLinear(num_f2,num_f3,bias=False)\n",
    "        # # \n",
    "        \n",
    "        if with_norm:\n",
    "            self.qc1 = QC_Norm(num_features=num_f1,init_ang_inc=10)\n",
    "            self.qc2 = QC_Norm(num_features=num_f2,init_ang_inc=40)\n",
    "            # self.qc3 = QC_Norm(num_features=num_f3)\n",
    "    \n",
    "            self.qc1a = QC_Norm_Correction(num_features=num_f1)\n",
    "            self.qc2a = QC_Norm_Correction(num_features=num_f2)\n",
    "            # self.qc3a = QC_Norm_Correction(num_features=num_f3)\n",
    "            \n",
    "        # \n",
    "        # self.qc1 = QC_Norm_Real(num_features=num_f1)\n",
    "        # self.qc2 = QC_Norm_Real(num_features=num_f2)\n",
    "        # self.qc3 = QC_Norm_Real(num_features=num_f3)\n",
    "\n",
    "\n",
    "        # self.qc1a = QC_Norm_Real_Correction(num_features=num_f1)\n",
    "        # self.qc2a = QC_Norm_Real_Correction(num_features=num_f2)\n",
    "        # self.qc3a = QC_Norm_Real_Correction(num_features=num_f3)\n",
    "        # \n",
    "    def forward(self, x, training=1):        \n",
    "        x = x.view(-1, img_size * img_size)\n",
    "        \n",
    "        if training == 1:\n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            # \n",
    "            \n",
    "\n",
    "            if with_norm:\n",
    "                x = self.qc1(self.qc1a(self.fc1(x)))        \n",
    "                x = self.qc2(self.qc2a(self.fc2(x)))\n",
    "            else:\n",
    "                x = self.fc1(x)        \n",
    "                x = self.fc2(x)                           \n",
    "\n",
    "            # x = self.qc3(self.qc3a(self.fc3(x)))\n",
    "            # \n",
    "            # x = self.qc1((self.fc1(x)))        \n",
    "            # x = self.qc2((self.fc2(x)))                           \n",
    "            # x = self.qc3((self.fc3(x)))\n",
    "            # \n",
    "        elif training == 2:\n",
    "            \n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            \n",
    "            print(\"=\"*10,\"layer 1\",\"=\"*10)\n",
    "            print(x)\n",
    "            torch.set_printoptions(profile=\"full\")            \n",
    "            print(binarize(self.fc1.weight))                                            \n",
    "            torch.set_printoptions(profile=\"default\")\n",
    "            x = self.fc1(x)            \n",
    "            \n",
    "            print(\"=\"*10,\"layer 2\",\"=\"*10)\n",
    "            print(x)\n",
    "            torch.set_printoptions(profile=\"full\")            \n",
    "            print(binarize(self.fc2.weight))                                            \n",
    "            torch.set_printoptions(profile=\"default\")\n",
    "            x = self.fc2(x)\n",
    "            \n",
    "            print(\"=\"*10,\"results\",\"=\"*10)\n",
    "            print(x)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # x = self.qc1(self.fc1(x),training=False)\n",
    "            #     x = self.qc2(self.fc2(x),training=False)\n",
    "            # x = self.qc3(self.fc3(x),training=False)\n",
    "            # \n",
    "            \n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            # \n",
    "            # x = self.qc3(self.qc3a(self.fc3(x),training=False),training=False)\n",
    "            \n",
    "            if with_norm:\n",
    "                x = self.qc1(self.qc1a(self.fc1(x),training=False),training=False)                \n",
    "                x = self.qc2(self.qc2a(self.fc2(x),training=False),training=False)                                        \n",
    "            else:\n",
    "                x = self.fc1(x)                    \n",
    "                x = self.fc2(x)\n",
    "            \n",
    "        if num_f2==1:            \n",
    "            x = torch.cat((x,1-x),-1)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    epoch_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target,new_target = modify_target(target)\n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        # \n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data,True)\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()    \n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "                \n",
    "        if batch_idx % 100 == 0:        \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}/{} ({:.2f}%)'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss, correct, (batch_idx+1) * len(data),\n",
    "                100. * float(correct) / float(((batch_idx+1) * len(data)) )))                \n",
    "    print(\"-\"*20,\"training done, loss\",\"-\"*20)\n",
    "    print(\"Training Set: Average loss: {}\".format(round(sum(epoch_loss)/len(epoch_loss),6)))\n",
    "    \n",
    "accur=[]\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target,new_target = modify_target(target)\n",
    "        \n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        \n",
    "        # print(\"=\"*100)        \n",
    "        # print(data)\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # print(\"Debug\")\n",
    "        # output = model(data,2)\n",
    "        # \n",
    "        \n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data,False)\n",
    "        # print(target,output)\n",
    "        # sys.exit(0)\n",
    "        test_loss += criterion(output, target) # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    a=100.*correct / len(test_loader.dataset)\n",
    "    accur.append(a)  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / float(len(test_loader.dataset))))\n",
    "    \n",
    "    return float(correct) / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Training\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "print(\"=\"*10,\"Model Info\",\"=\"*10)\n",
    "print(model)\n",
    "\n",
    "if with_norm and given_ang==-1:\n",
    "    optimizer = torch.optim.Adam([\n",
    "                    {'params': model.fc1.parameters()},\n",
    "                    {'params': model.fc2.parameters()},\n",
    "                    # {'params': model.fc3.parameters()},\n",
    "                    {'params': model.qc1.parameters(), 'lr': init_qc_lr},\n",
    "                    {'params': model.qc2.parameters(), 'lr': init_qc_lr},\n",
    "                    # {'params': model.qc3.parameters(), 'lr': 1},\n",
    "                ], lr=init_lr)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# optimizer = torch.optim.SGD([\n",
    "#                 {'params': model.fc1.parameters()},\n",
    "#                 {'params': model.fc2.parameters()},\n",
    "#                 {'params': model.fc3.parameters()},\n",
    "#                 {'params': model.qc1.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc2.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc3.parameters(), 'lr': 1},\n",
    "#             ], lr=0.1, momentum=0.9)\n",
    "# \n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \\\n",
    "#         base_lr=[1e-1,1e-1,1e-1,1,1,1], \\\n",
    "#         max_lr=[1e-3,1e-3,1e-3,1e-2,1e-2,1e-2], \\\n",
    "#         step_size_up=100\n",
    "#         )\n",
    "\n",
    "milestones = [3, 7, 9]\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)\n",
    "\n",
    "# \n",
    "# \n",
    "# test()\n",
    "# \n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "==================== 0 epoch ====================\n",
      "Epoch Start at: 04/23/2020 21:48:06\n",
      "-------------------- learning rates --------------------\n",
      "0.1,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/23/2020 21:48:06\n",
      "Train Epoch: 0 [0/18791 (0%)]\tLoss: 1.110373\tAccuracy: 4/16 (25.00%)\n",
      "Train Epoch: 0 [1600/18791 (9%)]\tLoss: 1.031002\tAccuracy: 818/1616 (50.62%)\n",
      "Train Epoch: 0 [3200/18791 (17%)]\tLoss: 1.054934\tAccuracy: 1660/3216 (51.62%)\n",
      "Train Epoch: 0 [4800/18791 (26%)]\tLoss: 0.985863\tAccuracy: 2541/4816 (52.76%)\n",
      "Train Epoch: 0 [6400/18791 (34%)]\tLoss: 1.029826\tAccuracy: 3360/6416 (52.37%)\n",
      "Train Epoch: 0 [8000/18791 (43%)]\tLoss: 0.998938\tAccuracy: 4213/8016 (52.56%)\n",
      "Train Epoch: 0 [9600/18791 (51%)]\tLoss: 1.020914\tAccuracy: 5059/9616 (52.61%)\n",
      "Train Epoch: 0 [11200/18791 (60%)]\tLoss: 0.991041\tAccuracy: 5881/11216 (52.43%)\n",
      "Train Epoch: 0 [12800/18791 (68%)]\tLoss: 1.027000\tAccuracy: 6713/12816 (52.38%)\n",
      "Train Epoch: 0 [14400/18791 (77%)]\tLoss: 1.024554\tAccuracy: 7568/14416 (52.50%)\n",
      "Train Epoch: 0 [16000/18791 (85%)]\tLoss: 1.072004\tAccuracy: 8436/16016 (52.67%)\n",
      "Train Epoch: 0 [17600/18791 (94%)]\tLoss: 1.098805\tAccuracy: 9290/17616 (52.74%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 1.028983\n",
      "Trainign End at: 04/23/2020 21:48:31\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/23/2020 21:48:31\n",
      "Test set: Average loss: 0.0637, Accuracy: 1657/3103 (53.40%)\n",
      "Testing End at: 04/23/2020 21:48:33\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.5339993554624557; Current accuracy 0.5339993554624557. Checkpointing\n",
      "Epoch End at: 04/23/2020 21:48:33\n",
      "============================================================\n",
      "\n",
      "==================== 1 epoch ====================\n",
      "Epoch Start at: 04/23/2020 21:48:33\n",
      "-------------------- learning rates --------------------\n",
      "0.1,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/23/2020 21:48:33\n",
      "Train Epoch: 1 [0/18791 (0%)]\tLoss: 1.055044\tAccuracy: 7/16 (43.75%)\n",
      "Train Epoch: 1 [1600/18791 (9%)]\tLoss: 1.086529\tAccuracy: 863/1616 (53.40%)\n",
      "Train Epoch: 1 [3200/18791 (17%)]\tLoss: 1.062122\tAccuracy: 1713/3216 (53.26%)\n",
      "Train Epoch: 1 [4800/18791 (26%)]\tLoss: 1.044090\tAccuracy: 2592/4816 (53.82%)\n",
      "Train Epoch: 1 [6400/18791 (34%)]\tLoss: 0.976404\tAccuracy: 3446/6416 (53.71%)\n",
      "Train Epoch: 1 [8000/18791 (43%)]\tLoss: 1.002584\tAccuracy: 4318/8016 (53.87%)\n",
      "Train Epoch: 1 [9600/18791 (51%)]\tLoss: 1.019111\tAccuracy: 5175/9616 (53.82%)\n",
      "Train Epoch: 1 [11200/18791 (60%)]\tLoss: 1.039698\tAccuracy: 6024/11216 (53.71%)\n",
      "Train Epoch: 1 [12800/18791 (68%)]\tLoss: 0.945522\tAccuracy: 6869/12816 (53.60%)\n",
      "Train Epoch: 1 [14400/18791 (77%)]\tLoss: 1.028711\tAccuracy: 7697/14416 (53.39%)\n",
      "Train Epoch: 1 [16000/18791 (85%)]\tLoss: 1.023690\tAccuracy: 8539/16016 (53.32%)\n",
      "Train Epoch: 1 [17600/18791 (94%)]\tLoss: 1.024440\tAccuracy: 9344/17616 (53.04%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 1.027987\n",
      "Trainign End at: 04/23/2020 21:48:58\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/23/2020 21:48:58\n",
      "Test set: Average loss: 0.0637, Accuracy: 1654/3103 (53.30%)\n",
      "Testing End at: 04/23/2020 21:49:00\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.5339993554624557; Current accuracy 0.5330325491459877. Checkpointing\n",
      "Epoch End at: 04/23/2020 21:49:00\n",
      "============================================================\n",
      "\n",
      "==================== 2 epoch ====================\n",
      "Epoch Start at: 04/23/2020 21:49:00\n",
      "-------------------- learning rates --------------------\n",
      "0.1,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/23/2020 21:49:00\n",
      "Train Epoch: 2 [0/18791 (0%)]\tLoss: 1.101203\tAccuracy: 5/16 (31.25%)\n",
      "Train Epoch: 2 [1600/18791 (9%)]\tLoss: 1.057045\tAccuracy: 884/1616 (54.70%)\n",
      "Train Epoch: 2 [3200/18791 (17%)]\tLoss: 1.091519\tAccuracy: 1710/3216 (53.17%)\n",
      "Train Epoch: 2 [4800/18791 (26%)]\tLoss: 1.015478\tAccuracy: 2557/4816 (53.09%)\n",
      "Train Epoch: 2 [6400/18791 (34%)]\tLoss: 0.984060\tAccuracy: 3391/6416 (52.85%)\n",
      "Train Epoch: 2 [8000/18791 (43%)]\tLoss: 1.076815\tAccuracy: 4248/8016 (52.99%)\n",
      "Train Epoch: 2 [9600/18791 (51%)]\tLoss: 1.013948\tAccuracy: 5096/9616 (53.00%)\n",
      "Train Epoch: 2 [11200/18791 (60%)]\tLoss: 1.008224\tAccuracy: 5943/11216 (52.99%)\n",
      "Train Epoch: 2 [12800/18791 (68%)]\tLoss: 1.018138\tAccuracy: 6809/12816 (53.13%)\n",
      "Train Epoch: 2 [14400/18791 (77%)]\tLoss: 0.987785\tAccuracy: 7669/14416 (53.20%)\n",
      "Train Epoch: 2 [16000/18791 (85%)]\tLoss: 1.033179\tAccuracy: 8541/16016 (53.33%)\n",
      "Train Epoch: 2 [17600/18791 (94%)]\tLoss: 1.028330\tAccuracy: 9355/17616 (53.11%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 1.027997\n",
      "Trainign End at: 04/23/2020 21:49:25\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/23/2020 21:49:25\n",
      "Test set: Average loss: 0.0637, Accuracy: 1656/3103 (53.37%)\n",
      "Testing End at: 04/23/2020 21:49:27\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.5339993554624557; Current accuracy 0.5336770866902997. Checkpointing\n",
      "Epoch End at: 04/23/2020 21:49:27\n",
      "============================================================\n",
      "\n",
      "==================== 3 epoch ====================\n",
      "Epoch Start at: 04/23/2020 21:49:27\n",
      "-------------------- learning rates --------------------\n",
      "0.010000000000000002,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/23/2020 21:49:27\n",
      "Train Epoch: 3 [0/18791 (0%)]\tLoss: 1.038544\tAccuracy: 8/16 (50.00%)\n",
      "Train Epoch: 3 [1600/18791 (9%)]\tLoss: 1.011662\tAccuracy: 829/1616 (51.30%)\n",
      "Train Epoch: 3 [3200/18791 (17%)]\tLoss: 1.029290\tAccuracy: 1699/3216 (52.83%)\n",
      "Train Epoch: 3 [4800/18791 (26%)]\tLoss: 0.956168\tAccuracy: 2558/4816 (53.11%)\n",
      "Train Epoch: 3 [6400/18791 (34%)]\tLoss: 1.053592\tAccuracy: 3416/6416 (53.24%)\n",
      "Train Epoch: 3 [8000/18791 (43%)]\tLoss: 0.958317\tAccuracy: 4280/8016 (53.39%)\n",
      "Train Epoch: 3 [9600/18791 (51%)]\tLoss: 1.005646\tAccuracy: 5115/9616 (53.19%)\n",
      "Train Epoch: 3 [11200/18791 (60%)]\tLoss: 1.043239\tAccuracy: 5957/11216 (53.11%)\n",
      "Train Epoch: 3 [12800/18791 (68%)]\tLoss: 1.088490\tAccuracy: 6792/12816 (53.00%)\n",
      "Train Epoch: 3 [14400/18791 (77%)]\tLoss: 0.956577\tAccuracy: 7619/14416 (52.85%)\n",
      "Train Epoch: 3 [16000/18791 (85%)]\tLoss: 1.039305\tAccuracy: 8467/16016 (52.87%)\n",
      "Train Epoch: 3 [17600/18791 (94%)]\tLoss: 1.015762\tAccuracy: 9320/17616 (52.91%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 1.028001\n",
      "Trainign End at: 04/23/2020 21:49:52\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/23/2020 21:49:52\n",
      "Test set: Average loss: 0.0637, Accuracy: 1656/3103 (53.37%)\n",
      "Testing End at: 04/23/2020 21:49:54\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.5339993554624557; Current accuracy 0.5336770866902997. Checkpointing\n",
      "Epoch End at: 04/23/2020 21:49:54\n",
      "============================================================\n",
      "\n",
      "==================== 4 epoch ====================\n",
      "Epoch Start at: 04/23/2020 21:49:54\n",
      "-------------------- learning rates --------------------\n",
      "0.010000000000000002,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/23/2020 21:49:54\n",
      "Train Epoch: 4 [0/18791 (0%)]\tLoss: 1.027905\tAccuracy: 9/16 (56.25%)\n",
      "Train Epoch: 4 [1600/18791 (9%)]\tLoss: 1.016537\tAccuracy: 883/1616 (54.64%)\n",
      "Train Epoch: 4 [3200/18791 (17%)]\tLoss: 0.978851\tAccuracy: 1688/3216 (52.49%)\n",
      "Train Epoch: 4 [4800/18791 (26%)]\tLoss: 1.035316\tAccuracy: 2530/4816 (52.53%)\n",
      "Train Epoch: 4 [6400/18791 (34%)]\tLoss: 0.976737\tAccuracy: 3387/6416 (52.79%)\n",
      "Train Epoch: 4 [8000/18791 (43%)]\tLoss: 1.023695\tAccuracy: 4226/8016 (52.72%)\n",
      "Train Epoch: 4 [9600/18791 (51%)]\tLoss: 1.024539\tAccuracy: 5079/9616 (52.82%)\n",
      "Train Epoch: 4 [11200/18791 (60%)]\tLoss: 1.117200\tAccuracy: 5959/11216 (53.13%)\n",
      "Train Epoch: 4 [12800/18791 (68%)]\tLoss: 1.033736\tAccuracy: 6811/12816 (53.14%)\n",
      "Train Epoch: 4 [14400/18791 (77%)]\tLoss: 0.981261\tAccuracy: 7676/14416 (53.25%)\n",
      "Train Epoch: 4 [16000/18791 (85%)]\tLoss: 1.005609\tAccuracy: 8506/16016 (53.11%)\n",
      "Train Epoch: 4 [17600/18791 (94%)]\tLoss: 1.032310\tAccuracy: 9367/17616 (53.17%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 1.028016\n",
      "Trainign End at: 04/23/2020 21:50:21\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/23/2020 21:50:21\n",
      "Test set: Average loss: 0.0637, Accuracy: 1658/3103 (53.43%)\n",
      "Testing End at: 04/23/2020 21:50:23\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.5343216242346117; Current accuracy 0.5343216242346117. Checkpointing\n",
      "Epoch End at: 04/23/2020 21:50:23\n",
      "============================================================\n",
      "\n",
      "==================== 5 epoch ====================\n",
      "Epoch Start at: 04/23/2020 21:50:23\n",
      "-------------------- learning rates --------------------\n",
      "0.010000000000000002,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/23/2020 21:50:23\n",
      "Train Epoch: 5 [0/18791 (0%)]\tLoss: 1.066314\tAccuracy: 6/16 (37.50%)\n",
      "Train Epoch: 5 [1600/18791 (9%)]\tLoss: 1.012344\tAccuracy: 835/1616 (51.67%)\n",
      "Train Epoch: 5 [3200/18791 (17%)]\tLoss: 0.997289\tAccuracy: 1658/3216 (51.55%)\n",
      "Train Epoch: 5 [4800/18791 (26%)]\tLoss: 1.017927\tAccuracy: 2539/4816 (52.72%)\n",
      "Train Epoch: 5 [6400/18791 (34%)]\tLoss: 1.046492\tAccuracy: 3395/6416 (52.91%)\n",
      "Train Epoch: 5 [8000/18791 (43%)]\tLoss: 1.087558\tAccuracy: 4228/8016 (52.74%)\n",
      "Train Epoch: 5 [9600/18791 (51%)]\tLoss: 0.997686\tAccuracy: 5076/9616 (52.79%)\n",
      "Train Epoch: 5 [11200/18791 (60%)]\tLoss: 1.073763\tAccuracy: 5953/11216 (53.08%)\n",
      "Train Epoch: 5 [12800/18791 (68%)]\tLoss: 0.982035\tAccuracy: 6804/12816 (53.09%)\n",
      "Train Epoch: 5 [14400/18791 (77%)]\tLoss: 1.008047\tAccuracy: 7611/14416 (52.80%)\n",
      "Train Epoch: 5 [16000/18791 (85%)]\tLoss: 1.058949\tAccuracy: 8456/16016 (52.80%)\n",
      "Train Epoch: 5 [17600/18791 (94%)]\tLoss: 1.069260\tAccuracy: 9316/17616 (52.88%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 1.028014\n",
      "Trainign End at: 04/23/2020 21:50:47\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/23/2020 21:50:47\n",
      "Test set: Average loss: 0.0637, Accuracy: 1658/3103 (53.43%)\n",
      "Testing End at: 04/23/2020 21:50:49\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.5343216242346117; Current accuracy 0.5343216242346117. Checkpointing\n",
      "Epoch End at: 04/23/2020 21:50:49\n",
      "============================================================\n",
      "\n",
      "==================== 6 epoch ====================\n",
      "Epoch Start at: 04/23/2020 21:50:49\n",
      "-------------------- learning rates --------------------\n",
      "0.010000000000000002,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/23/2020 21:50:49\n",
      "Train Epoch: 6 [0/18791 (0%)]\tLoss: 1.039140\tAccuracy: 8/16 (50.00%)\n",
      "Train Epoch: 6 [1600/18791 (9%)]\tLoss: 0.984556\tAccuracy: 856/1616 (52.97%)\n",
      "Train Epoch: 6 [3200/18791 (17%)]\tLoss: 1.015203\tAccuracy: 1711/3216 (53.20%)\n",
      "Train Epoch: 6 [4800/18791 (26%)]\tLoss: 1.074358\tAccuracy: 2552/4816 (52.99%)\n",
      "Train Epoch: 6 [6400/18791 (34%)]\tLoss: 0.982712\tAccuracy: 3425/6416 (53.38%)\n",
      "Train Epoch: 6 [8000/18791 (43%)]\tLoss: 0.912054\tAccuracy: 4239/8016 (52.88%)\n",
      "Train Epoch: 6 [9600/18791 (51%)]\tLoss: 1.033727\tAccuracy: 5082/9616 (52.85%)\n",
      "Train Epoch: 6 [11200/18791 (60%)]\tLoss: 1.029307\tAccuracy: 5960/11216 (53.14%)\n",
      "Train Epoch: 6 [12800/18791 (68%)]\tLoss: 1.016629\tAccuracy: 6772/12816 (52.84%)\n",
      "Train Epoch: 6 [14400/18791 (77%)]\tLoss: 1.041663\tAccuracy: 7642/14416 (53.01%)\n",
      "Train Epoch: 6 [16000/18791 (85%)]\tLoss: 1.068655\tAccuracy: 8497/16016 (53.05%)\n",
      "Train Epoch: 6 [17600/18791 (94%)]\tLoss: 1.052829\tAccuracy: 9332/17616 (52.97%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 1.027974\n",
      "Trainign End at: 04/23/2020 21:51:13\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/23/2020 21:51:13\n",
      "Test set: Average loss: 0.0637, Accuracy: 1658/3103 (53.43%)\n",
      "Testing End at: 04/23/2020 21:51:15\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.5343216242346117; Current accuracy 0.5343216242346117. Checkpointing\n",
      "Epoch End at: 04/23/2020 21:51:15\n",
      "============================================================\n",
      "\n",
      "==================== 7 epoch ====================\n",
      "Epoch Start at: 04/23/2020 21:51:15\n",
      "-------------------- learning rates --------------------\n",
      "0.0010000000000000002,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/23/2020 21:51:15\n",
      "Train Epoch: 7 [0/18791 (0%)]\tLoss: 1.061208\tAccuracy: 7/16 (43.75%)\n",
      "Train Epoch: 7 [1600/18791 (9%)]\tLoss: 1.094900\tAccuracy: 858/1616 (53.09%)\n",
      "Train Epoch: 7 [3200/18791 (17%)]\tLoss: 1.007961\tAccuracy: 1688/3216 (52.49%)\n",
      "Train Epoch: 7 [4800/18791 (26%)]\tLoss: 1.050659\tAccuracy: 2560/4816 (53.16%)\n",
      "Train Epoch: 7 [6400/18791 (34%)]\tLoss: 0.995897\tAccuracy: 3383/6416 (52.73%)\n",
      "Train Epoch: 7 [8000/18791 (43%)]\tLoss: 0.977922\tAccuracy: 4242/8016 (52.92%)\n",
      "Train Epoch: 7 [9600/18791 (51%)]\tLoss: 1.056195\tAccuracy: 5141/9616 (53.46%)\n",
      "Train Epoch: 7 [11200/18791 (60%)]\tLoss: 1.032933\tAccuracy: 6024/11216 (53.71%)\n",
      "Train Epoch: 7 [12800/18791 (68%)]\tLoss: 0.968111\tAccuracy: 6888/12816 (53.75%)\n",
      "Train Epoch: 7 [14400/18791 (77%)]\tLoss: 1.098509\tAccuracy: 7726/14416 (53.59%)\n",
      "Train Epoch: 7 [16000/18791 (85%)]\tLoss: 1.067746\tAccuracy: 8536/16016 (53.30%)\n",
      "Train Epoch: 7 [17600/18791 (94%)]\tLoss: 1.095753\tAccuracy: 9367/17616 (53.17%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 1.027962\n",
      "Trainign End at: 04/23/2020 21:51:40\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/23/2020 21:51:40\n",
      "Test set: Average loss: 0.0637, Accuracy: 1660/3103 (53.50%)\n",
      "Testing End at: 04/23/2020 21:51:42\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.5349661617789236; Current accuracy 0.5349661617789236. Checkpointing\n",
      "Epoch End at: 04/23/2020 21:51:42\n",
      "============================================================\n",
      "\n",
      "==================== 8 epoch ====================\n",
      "Epoch Start at: 04/23/2020 21:51:42\n",
      "-------------------- learning rates --------------------\n",
      "0.0010000000000000002,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/23/2020 21:51:43\n",
      "Train Epoch: 8 [0/18791 (0%)]\tLoss: 1.037167\tAccuracy: 8/16 (50.00%)\n",
      "Train Epoch: 8 [1600/18791 (9%)]\tLoss: 1.038695\tAccuracy: 851/1616 (52.66%)\n",
      "Train Epoch: 8 [3200/18791 (17%)]\tLoss: 1.021037\tAccuracy: 1697/3216 (52.77%)\n",
      "Train Epoch: 8 [4800/18791 (26%)]\tLoss: 1.009918\tAccuracy: 2548/4816 (52.91%)\n",
      "Train Epoch: 8 [6400/18791 (34%)]\tLoss: 1.033835\tAccuracy: 3389/6416 (52.82%)\n",
      "Train Epoch: 8 [8000/18791 (43%)]\tLoss: 1.043420\tAccuracy: 4217/8016 (52.61%)\n",
      "Train Epoch: 8 [9600/18791 (51%)]\tLoss: 1.052476\tAccuracy: 5071/9616 (52.74%)\n",
      "Train Epoch: 8 [11200/18791 (60%)]\tLoss: 1.041021\tAccuracy: 5947/11216 (53.02%)\n",
      "Train Epoch: 8 [12800/18791 (68%)]\tLoss: 0.981558\tAccuracy: 6798/12816 (53.04%)\n",
      "Train Epoch: 8 [14400/18791 (77%)]\tLoss: 0.979364\tAccuracy: 7621/14416 (52.86%)\n",
      "Train Epoch: 8 [16000/18791 (85%)]\tLoss: 0.950069\tAccuracy: 8510/16016 (53.13%)\n",
      "Train Epoch: 8 [17600/18791 (94%)]\tLoss: 1.011955\tAccuracy: 9344/17616 (53.04%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 1.027986\n",
      "Trainign End at: 04/23/2020 21:52:07\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/23/2020 21:52:07\n",
      "Test set: Average loss: 0.0637, Accuracy: 1655/3103 (53.34%)\n",
      "Testing End at: 04/23/2020 21:52:09\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.5349661617789236; Current accuracy 0.5333548179181438. Checkpointing\n",
      "Epoch End at: 04/23/2020 21:52:09\n",
      "============================================================\n",
      "\n",
      "==================== 9 epoch ====================\n",
      "Epoch Start at: 04/23/2020 21:52:09\n",
      "-------------------- learning rates --------------------\n",
      "0.00010000000000000003,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/23/2020 21:52:09\n",
      "Train Epoch: 9 [0/18791 (0%)]\tLoss: 0.980091\tAccuracy: 10/16 (62.50%)\n",
      "Train Epoch: 9 [1600/18791 (9%)]\tLoss: 1.005294\tAccuracy: 878/1616 (54.33%)\n",
      "Train Epoch: 9 [3200/18791 (17%)]\tLoss: 1.002883\tAccuracy: 1686/3216 (52.43%)\n",
      "Train Epoch: 9 [4800/18791 (26%)]\tLoss: 1.030398\tAccuracy: 2551/4816 (52.97%)\n",
      "Train Epoch: 9 [6400/18791 (34%)]\tLoss: 1.070121\tAccuracy: 3403/6416 (53.04%)\n",
      "Train Epoch: 9 [8000/18791 (43%)]\tLoss: 1.027844\tAccuracy: 4263/8016 (53.18%)\n",
      "Train Epoch: 9 [9600/18791 (51%)]\tLoss: 1.001244\tAccuracy: 5140/9616 (53.45%)\n",
      "Train Epoch: 9 [11200/18791 (60%)]\tLoss: 1.036236\tAccuracy: 5947/11216 (53.02%)\n",
      "Train Epoch: 9 [12800/18791 (68%)]\tLoss: 1.066277\tAccuracy: 6811/12816 (53.14%)\n",
      "Train Epoch: 9 [14400/18791 (77%)]\tLoss: 0.944480\tAccuracy: 7651/14416 (53.07%)\n",
      "Train Epoch: 9 [16000/18791 (85%)]\tLoss: 1.052933\tAccuracy: 8484/16016 (52.97%)\n",
      "Train Epoch: 9 [17600/18791 (94%)]\tLoss: 1.061130\tAccuracy: 9345/17616 (53.05%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 1.028003\n",
      "Trainign End at: 04/23/2020 21:52:34\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/23/2020 21:52:34\n",
      "Test set: Average loss: 0.0637, Accuracy: 1658/3103 (53.43%)\n",
      "Testing End at: 04/23/2020 21:52:37\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.5349661617789236; Current accuracy 0.5343216242346117. Checkpointing\n",
      "Epoch End at: 04/23/2020 21:52:37\n",
      "============================================================\n",
      "\n",
      "==================== 10 epoch ====================\n",
      "Epoch Start at: 04/23/2020 21:52:37\n",
      "-------------------- learning rates --------------------\n",
      "0.00010000000000000003,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/23/2020 21:52:37\n",
      "Train Epoch: 10 [0/18791 (0%)]\tLoss: 1.009203\tAccuracy: 9/16 (56.25%)\n",
      "Train Epoch: 10 [1600/18791 (9%)]\tLoss: 1.001380\tAccuracy: 841/1616 (52.04%)\n",
      "Train Epoch: 10 [3200/18791 (17%)]\tLoss: 1.075786\tAccuracy: 1678/3216 (52.18%)\n",
      "Train Epoch: 10 [4800/18791 (26%)]\tLoss: 1.059234\tAccuracy: 2531/4816 (52.55%)\n",
      "Train Epoch: 10 [6400/18791 (34%)]\tLoss: 1.013015\tAccuracy: 3387/6416 (52.79%)\n",
      "Train Epoch: 10 [8000/18791 (43%)]\tLoss: 1.040153\tAccuracy: 4232/8016 (52.79%)\n",
      "Train Epoch: 10 [9600/18791 (51%)]\tLoss: 1.011110\tAccuracy: 5091/9616 (52.94%)\n",
      "Train Epoch: 10 [11200/18791 (60%)]\tLoss: 0.989700\tAccuracy: 5936/11216 (52.92%)\n",
      "Train Epoch: 10 [12800/18791 (68%)]\tLoss: 1.005821\tAccuracy: 6802/12816 (53.07%)\n",
      "Train Epoch: 10 [14400/18791 (77%)]\tLoss: 1.010659\tAccuracy: 7642/14416 (53.01%)\n",
      "Train Epoch: 10 [16000/18791 (85%)]\tLoss: 1.029313\tAccuracy: 8500/16016 (53.07%)\n",
      "Train Epoch: 10 [17600/18791 (94%)]\tLoss: 0.989185\tAccuracy: 9367/17616 (53.17%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 1.027995\n",
      "Trainign End at: 04/23/2020 21:53:02\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/23/2020 21:53:02\n",
      "Test set: Average loss: 0.0637, Accuracy: 1658/3103 (53.43%)\n",
      "Testing End at: 04/23/2020 21:53:04\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.5349661617789236; Current accuracy 0.5343216242346117. Checkpointing\n",
      "Epoch End at: 04/23/2020 21:53:04\n",
      "============================================================\n",
      "\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ],
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error"
    },
    {
     "name": "stderr",
     "text": [
      "/home/weiwen/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3327: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile(resume_path):\n",
    "    print(\"=> loading checkpoint from '{}'<=\".format(resume_path))\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    epoch_init,acc = checkpoint[\"epoch\"],checkpoint[\"acc\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])    \n",
    "    scheduler.milestones = Counter(milestones)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "else:\n",
    "    epoch_init,acc = 0,0\n",
    "\n",
    "\n",
    "\n",
    "if training:\n",
    "    for epoch in range(epoch_init, max_epoch + 1):\n",
    "        print(\"=\"*20,epoch,\"epoch\",\"=\"*20)  \n",
    "        print(\"Epoch Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))        \n",
    "\n",
    "        print(\"-\"*20,\"learning rates\",\"-\"*20)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(param_group['lr'],end=\",\")\n",
    "        print()    \n",
    "        \n",
    "        print(\"-\"*20,\"training\",\"-\"*20)\n",
    "        print(\"Trainign Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        train(epoch)\n",
    "        print(\"Trainign End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"-\"*20,\"testing\",\"-\"*20)\n",
    "        print(\"Testing Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))        \n",
    "        cur_acc = test()\n",
    "        print(\"Testing End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"-\"*60)\n",
    "        print()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        is_best = False\n",
    "        if cur_acc > acc:\n",
    "            is_best = True\n",
    "            acc=cur_acc\n",
    "        \n",
    "        print(\"Best accuracy: {}; Current accuracy {}. Checkpointing\".format(acc,cur_acc))\n",
    "        if save_chkp:\n",
    "            save_checkpoint({\n",
    "              'epoch': epoch + 1,\n",
    "              'acc': acc, \n",
    "              'state_dict': model.state_dict(),      \n",
    "              'optimizer' : optimizer.state_dict(),\n",
    "               'scheduler': scheduler.state_dict(),\n",
    "            }, is_best, save_path, 'checkpoint_{}_{}.pth.tar'.format(epoch,round(cur_acc,4)))\n",
    "        print(\"Epoch End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"=\"*60)\n",
    "        print()        \n",
    "else:    \n",
    "    print(\"=\"*20,epoch,\"Testing\",\"=\"*20)  \n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, \n",
    "        num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "    test()\n",
    "    \n",
    "\n",
    "sys.exit(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../interfae\")\n",
    "from qiskit_simulator import *\n",
    "\n",
    "\n",
    "\n",
    "accur=[]\n",
    "def test_debug():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target,new_target = modify_target(target)\n",
    "        \n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # print(\"Debug\",data,target)\n",
    "        # output = model(data,2)\n",
    "        # \n",
    "        # run_simulator(model,data)\n",
    "        \n",
    "        output = model(data,False)\n",
    "        print(data)\n",
    "        print(output)\n",
    "        print(target)\n",
    "        sys.exit(0)\n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "        # output = model(data,False)\n",
    "        test_loss += criterion(output, target) # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    a=100.*correct / len(test_loader.dataset)\n",
    "    accur.append(a)  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / float(len(test_loader.dataset))))\n",
    "    \n",
    "    return float(correct) / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile(resume_path):\n",
    "    print(\"=> loading checkpoint from '{}'<=\".format(resume_path))\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    epoch_init,acc = checkpoint[\"epoch\"],checkpoint[\"acc\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])    \n",
    "    scheduler.milestones = Counter(milestones)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "else:\n",
    "    epoch_init,acc = 0,0\n",
    "\n",
    "print(\"HERE\")\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "        num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "test_debug()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for name, para in model.named_parameters():\n",
    "#     print(name,para)\n",
    "print(\"HERE\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8213722",
   "language": "python",
   "display_name": "PyCharm (qiskit_practice)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}