{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "====================================================================================================\n",
      "Training procedure for Quantum Computer:\n",
      "\tStart at: 04/22/2020 02:42:57\n",
      "\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\n",
      "\tEnjoy and Good Luck!\n",
      "====================================================================================================\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import functools\n",
    "from collections import Counter\n",
    "print = functools.partial(print, flush=True)\n",
    "\n",
    "# interest_num = [0,1,2,3,4,5,6,7,8,9]\n",
    "interest_num = [3,6]\n",
    "img_size = 4\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 32\n",
    "inference_batch_size = 1\n",
    "num_f1 = 4\n",
    "# num_f2 = len(interest_num)\n",
    "num_f2 = 2\n",
    "init_lr = 0.01\n",
    "with_norm = False\n",
    "save_chkp = False\n",
    "\n",
    "\n",
    "save_to_file = False\n",
    "if save_to_file:\n",
    "    sys.stdout = open(save_path+\"/log\", 'w')\n",
    "save_path = \"./model/\"+os.path.basename(sys.argv[0])+\"_\"+time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# resume_path = \"./model/ipykernel_launcher.py_2020_04_22-00_17_32/checkpoint_8_0.9568.pth.tar\"\n",
    "\n",
    "resume_path = \"\"\n",
    "training = True\n",
    "max_epoch = 10\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\"*100)\n",
    "print(\"Training procedure for Quantum Computer:\")\n",
    "print(\"\\tStart at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "print(\"\\tProblems and issues, please contact Dr. Weiwen Jiang (wjiang2@nd.edu)\")\n",
    "print(\"\\tEnjoy and Good Luck!\")\n",
    "print(\"=\"*100)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "def modify_target(target):\n",
    "    for j in range(len(target)):\n",
    "        for idx in range(len(interest_num)):\n",
    "            if target[j] == interest_num[idx]:\n",
    "                target[j] = idx\n",
    "                break\n",
    "    \n",
    "    new_target = torch.zeros(target.shape[0],2)\n",
    "        \n",
    "    for i in range(target.shape[0]):        \n",
    "        if target[i].item() == 0:            \n",
    "            new_target[i] = torch.tensor([1,0]).clone()     \n",
    "        else:\n",
    "            new_target[i] = torch.tensor([0,1]).clone()\n",
    "               \n",
    "    return target,new_target\n",
    "\n",
    "def select_num(dataset,interest_num):\n",
    "    labels = dataset.targets #get labels\n",
    "    labels = labels.numpy()\n",
    "    idx = {}\n",
    "    for num in interest_num:\n",
    "        idx[num] = np.where(labels == num)\n",
    "        \n",
    "    fin_idx = idx[interest_num[0]]\n",
    "    for i in range(1,len(interest_num)):           \n",
    "        \n",
    "        fin_idx = (np.concatenate((fin_idx[0],idx[interest_num[i]][0])),)\n",
    "    \n",
    "    fin_idx = fin_idx[0]    \n",
    "    \n",
    "    dataset.targets = labels[fin_idx]\n",
    "    dataset.data = dataset.data[fin_idx]\n",
    "    \n",
    "    # print(dataset.targets.shape)\n",
    "    \n",
    "    dataset.targets,_ = modify_target(dataset.targets)\n",
    "    # print(dataset.targets.shape)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.Compose([transforms.Resize((img_size,img_size)),transforms.ToTensor()])\n",
    "# transform = transforms.Compose([transforms.Resize((img_size,img_size)),transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.MNIST(root='data', train=True,\n",
    "                                   download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='data', train=False,\n",
    "                                  download=True, transform=transform)\n",
    "\n",
    "train_data = select_num(train_data,interest_num)\n",
    "test_data =  select_num(test_data,interest_num)\n",
    "\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=inference_batch_size, \n",
    "    num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    filename = os.path.join(save_path, filename)\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        bestname = os.path.join(save_path, 'model_best.tar')\n",
    "        shutil.copyfile(filename, bestname)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "========== Model Info ==========\n",
      "Net(\n",
      "  (fc1): BinaryLinear(in_features=16, out_features=4, bias=False)\n",
      "  (fc2): BinaryLinear(in_features=4, out_features=2, bias=False)\n",
      "  (qc1): QC_Norm()\n",
      "  (qc2): QC_Norm()\n",
      "  (qc1a): QC_Norm_Correction()\n",
      "  (qc2a): QC_Norm_Correction()\n",
      ")\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from torch.nn.parameter import Parameter\n",
    " \n",
    "\n",
    "class BinarizeF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(cxt, input):\n",
    "        output = input.new(input.size())\n",
    "        output[input >= 0] = 1\n",
    "        output[input < 0] = -1\n",
    "        \n",
    "              \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(cxt, grad_output):\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input\n",
    "# aliases\n",
    "binarize = BinarizeF.apply\n",
    "\n",
    "\n",
    "class ClipF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        output = input.clone().detach()\n",
    "        # output = input.new(input.size())\n",
    "        output[input >= 1] = 1\n",
    "        output[input <= 0] = 0\n",
    "        ctx.save_for_backward(input)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input >= 1] = 0\n",
    "        grad_input[input <= 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "# aliases\n",
    "clipfunc = ClipF.apply\n",
    "\n",
    "\n",
    "class BinaryLinear(nn.Linear):\n",
    "    \n",
    "    \n",
    "    def do_slp_via_th(self,input_ori,w_ori):\n",
    "        p = input_ori\n",
    "        d = 4*p*(1-p)\n",
    "        e = (2*p-1)\n",
    "        # e_sq = torch.tensor(1)\n",
    "        w = w_ori\n",
    "        \n",
    "        sum_of_sq = (d+e.pow(2)).sum(-1)\n",
    "        sum_of_sq = sum_of_sq.unsqueeze(-1)        \n",
    "        sum_of_sq = sum_of_sq.expand(p.shape[0], w.shape[0])\n",
    "                \n",
    "        diag_p = torch.diag_embed(e)        \n",
    "        \n",
    "        p_w = torch.matmul(w,diag_p)\n",
    "        \n",
    "        z_p_w = torch.zeros_like(p_w)        \n",
    "        shft_p_w = torch.cat((p_w, z_p_w), -1)\n",
    "        \n",
    "        sum_of_cross = torch.zeros_like(p_w)\n",
    "        length = p.shape[1]    \n",
    "        \n",
    "        for shft in range(1,length):    \n",
    "            sum_of_cross += shft_p_w[:,:,0:length]*shft_p_w[:,:,shft:length+shft]\n",
    "\n",
    "        sum_of_cross = sum_of_cross.sum(-1)\n",
    "                \n",
    "        return (sum_of_sq+2*sum_of_cross)/(length**2) \n",
    "    \n",
    "    def forward(self, input):        \n",
    "        binary_weight = binarize(self.weight)        \n",
    "        if self.bias is None:\n",
    "            return self.do_slp_via_th(input,binary_weight)\n",
    "                      \n",
    "        else:   \n",
    "            \n",
    "            bias_one  = torch.ones(input.shape[0],1)            \n",
    "            new_input = torch.cat((input, bias_one), -1)            \n",
    "            bias = clipfunc(self.bias).unsqueeze(1)            \n",
    "            new_weight = binary_weight            \n",
    "            new_weight = torch.cat((new_weight,bias),-1)                        \n",
    "            return self.do_slp_via_th(new_input,new_weight)\n",
    "            \n",
    "            \n",
    "            torch.set_printoptions(edgeitems=64)\n",
    "            # binary_bias = binarize(self.bias)/float(len(input[0].flatten())+1)\n",
    "            binary_bias = binarize(self.bias)/float(len(input[0].flatten())+1)\n",
    "            res = F.linear(input, binary_weight/float(len(input[0].flatten())+1), binary_bias)\n",
    "            return res\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Glorot initialization\n",
    "        in_features, out_features = self.weight.size()\n",
    "        stdv = math.sqrt(1.5 / (in_features + out_features))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.zero_()\n",
    "\n",
    "        self.weight.lr_scale = 1. / stdv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class QC_Norm(nn.Module):\n",
    "    def __init__(self, num_features, momentum=0.1):        \n",
    "        super(QC_Norm, self).__init__()\n",
    "        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)        \n",
    "        self.ang_inc = Parameter(torch.ones(1)*10)\n",
    "        \n",
    "        self.momentum = momentum\n",
    "                \n",
    "        self.printed = False\n",
    "        self.x_mean_ancle=0\n",
    "        self.x_mean_rote = 0\n",
    "        self.input = 0\n",
    "        self.output = 0\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            if not self.printed:\n",
    "                print(\"self.ang_inc\",self.ang_inc)\n",
    "                self.printed = True\n",
    "                    \n",
    "            x = x.transpose(0,1)\n",
    "  \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)\n",
    "            x_1 = (x_final.cos()+1)/2\n",
    "                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            self.printed = False\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)\n",
    "            x_lack_sum = x_sum - x    \n",
    "            x_mean = x_lack_sum/x.shape[-1]\n",
    "            \n",
    "            \n",
    "                    \n",
    "            x_mean_ancle = (x_mean*2-1).acos()  \n",
    "            \n",
    "            ang_inc = self.ang_inc.unsqueeze(-1).expand(x_mean_ancle.shape) \n",
    "            # ang_inc = np.pi/2/(x.max(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) -x.min(-1)[0].unsqueeze(-1).expand(x_mean_ancle.shape) )\n",
    "            x_mean_rote = (np.pi/2 - x_mean_ancle)*20 # ang_inc\n",
    "            \n",
    "            x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot\n",
    "                                                \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+x_mean_rote  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "      \n",
    "        return x_1\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.reset_running_stats()\n",
    "        self.ang_inc.data.zeros_()\n",
    "        \n",
    "def print_degree(x,name=\"x\"):\n",
    "    print(name,x/np.pi*180)\n",
    "    \n",
    "    \n",
    "class QC_Norm_Real(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Real, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.x_max = 0\n",
    "        self.x_min = 0\n",
    "        # print(\"Using Normal without real\")\n",
    "        \n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            # x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_final = ((x_ancle-self.x_min)/(self.x_max-self.x_min))*np.pi\n",
    "            \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            x = x.transpose(0,1)        \n",
    "            x_ancle = (x*2-1).acos()     \n",
    "            x_rectify_ancle = (x_ancle.max(-1)[0]-x_ancle.min(-1)[0]).unsqueeze(-1).expand(x.shape)                                                                         \n",
    "            x_final = ((x_ancle-x_ancle.min(-1)[0].unsqueeze(-1))/(x_rectify_ancle))*np.pi\n",
    "            \n",
    "            x_moving_rot = x_final - x_ancle\n",
    "            \n",
    "            x_moving_rot_mean = x_moving_rot.sum(-1)/x.shape[-1] \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot_mean      \n",
    "            \n",
    "            self.x_max = self.momentum * x_ancle.max(-1)[0].unsqueeze(-1) + \\\n",
    "                                    (1 - self.momentum) * self.x_max\n",
    "            self.x_min = self.momentum * x_ancle.min(-1)[0].unsqueeze(-1) + \\\n",
    "                                    (1 - self.momentum) * self.x_min\n",
    "            \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "            \n",
    "        return x_1\n",
    "\n",
    "\n",
    "class QC_Norm_Real_Correction(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Real_Correction, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:            \n",
    "            \n",
    "            x = x.transpose(0,1)                    \n",
    "            x_ancle = (x*2-1).acos()                        \n",
    "            x_moving_rot = -1*(x_ancle.min(-1)[0])\n",
    "            \n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot                                                    \n",
    "            x_final = x_ancle+x_moving_rot.unsqueeze(-1)                                    \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        return x_1\n",
    "\n",
    "class QC_Norm_Correction(nn.Module):\n",
    "    def __init__(self,num_features,momentum=0.1):        \n",
    "        super(QC_Norm_Correction, self).__init__()        \n",
    "        self.x_running_rot = Parameter(torch.zeros(num_features),requires_grad=False)\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    def forward(self,x,training=True):  \n",
    "        if not training:\n",
    "            x = x.transpose(0,1)\n",
    "            \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+self.x_running_rot.unsqueeze(-1)  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "            \n",
    "        else:\n",
    "            x = x.transpose(0,1)        \n",
    "            x_sum = x.sum(-1).unsqueeze(-1).expand(x.shape)                \n",
    "            x_mean = x_sum/x.shape[-1]\n",
    "                                \n",
    "            x_mean_ancle = (x_mean*2-1).acos()    \n",
    "            x_mean_rote = (np.pi/2 - x_mean_ancle) \n",
    "            \n",
    "            x_moving_rot = (x_mean_rote.sum(-1)/x.shape[-1])\n",
    "            self.x_running_rot[:] = self.momentum * self.x_running_rot + \\\n",
    "                                  (1 - self.momentum) * x_moving_rot                                        \n",
    "            x_ancle = (x*2-1).acos()\n",
    "            x_final = x_ancle+x_mean_rote  \n",
    "            x_1 = (x_final.cos()+1)/2                                \n",
    "            x_1 = x_1.transpose(0,1)\n",
    "        \n",
    "        return x_1\n",
    "\n",
    "## Define the NN architecture\n",
    "class Net(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc1 = BinaryLinear(img_size*img_size,num_f1,bias=False)\n",
    "        self.fc2 = BinaryLinear(num_f1,num_f2,bias=False)\n",
    "        # self.fc3 = BinaryLinear(num_f2,num_f3,bias=False)\n",
    "        # # \n",
    "        self.qc1 = QC_Norm(num_features=num_f1)\n",
    "        self.qc2 = QC_Norm(num_features=num_f2)\n",
    "        # self.qc3 = QC_Norm(num_features=num_f3)\n",
    "\n",
    "        self.qc1a = QC_Norm_Correction(num_features=num_f1)\n",
    "        self.qc2a = QC_Norm_Correction(num_features=num_f2)\n",
    "        # self.qc3a = QC_Norm_Correction(num_features=num_f3)\n",
    "        \n",
    "        # \n",
    "        # self.qc1 = QC_Norm_Real(num_features=num_f1)\n",
    "        # self.qc2 = QC_Norm_Real(num_features=num_f2)\n",
    "        # self.qc3 = QC_Norm_Real(num_features=num_f3)\n",
    "\n",
    "\n",
    "        # self.qc1a = QC_Norm_Real_Correction(num_features=num_f1)\n",
    "        # self.qc2a = QC_Norm_Real_Correction(num_features=num_f2)\n",
    "        # self.qc3a = QC_Norm_Real_Correction(num_features=num_f3)\n",
    "        # \n",
    "    def forward(self, x, training=1):        \n",
    "        x = x.view(-1, img_size * img_size)\n",
    "        \n",
    "        if training == 1:\n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            # \n",
    "            \n",
    "\n",
    "            if with_norm:\n",
    "                x = self.qc1(self.qc1a(self.fc1(x)))        \n",
    "                x = self.qc2(self.qc2a(self.fc2(x)))\n",
    "            else:\n",
    "                x = self.fc1(x)        \n",
    "                x = self.fc2(x)                           \n",
    "\n",
    "            # x = self.qc3(self.qc3a(self.fc3(x)))\n",
    "            # \n",
    "            # x = self.qc1((self.fc1(x)))        \n",
    "            # x = self.qc2((self.fc2(x)))                           \n",
    "            # x = self.qc3((self.fc3(x)))\n",
    "            # \n",
    "        elif training == 2:\n",
    "            \n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            \n",
    "            print(\"=\"*10,\"layer 1\",\"=\"*10)\n",
    "            print(x)\n",
    "            torch.set_printoptions(profile=\"full\")            \n",
    "            print(binarize(self.fc1.weight))                                            \n",
    "            torch.set_printoptions(profile=\"default\")\n",
    "            x = self.fc1(x)            \n",
    "            \n",
    "            print(\"=\"*10,\"layer 2\",\"=\"*10)\n",
    "            print(x)\n",
    "            torch.set_printoptions(profile=\"full\")            \n",
    "            print(binarize(self.fc2.weight))                                            \n",
    "            torch.set_printoptions(profile=\"default\")\n",
    "            x = self.fc2(x)\n",
    "            \n",
    "            print(\"=\"*10,\"results\",\"=\"*10)\n",
    "            print(x)\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # x = self.qc1(self.fc1(x),training=False)\n",
    "            #     x = self.qc2(self.fc2(x),training=False)\n",
    "            # x = self.qc3(self.fc3(x),training=False)\n",
    "            # \n",
    "            \n",
    "            # x = binarize(x-0.0001)\n",
    "            # x = (x+1)/2\n",
    "            # \n",
    "            # x = self.qc3(self.qc3a(self.fc3(x),training=False),training=False)\n",
    "            \n",
    "            if with_norm:\n",
    "                x = self.qc1(self.qc1a(self.fc1(x),training=False),training=False)                \n",
    "                x = self.qc2(self.qc2a(self.fc2(x),training=False),training=False)                                        \n",
    "            else:\n",
    "                x = self.fc1(x)                    \n",
    "                x = self.fc2(x)\n",
    "            \n",
    "        if num_f2==1:            \n",
    "            x = torch.cat((x,1-x),-1)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    epoch_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target,new_target = modify_target(target)\n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        # \n",
    "        \n",
    "        \n",
    "        \n",
    "        data, target = data.to(device), target.to(device)        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data,True)\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()    \n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        epoch_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "                \n",
    "        if batch_idx % 100 == 0:        \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy: {}/{} ({:.2f}%)'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss, correct, (batch_idx+1) * len(data),\n",
    "                100. * float(correct) / float(((batch_idx+1) * len(data)) )))                \n",
    "    print(\"-\"*20,\"training done, loss\",\"-\"*20)\n",
    "    print(\"Training Set: Average loss: {}\".format(round(sum(epoch_loss)/len(epoch_loss),6)))\n",
    "    \n",
    "accur=[]\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target,new_target = modify_target(target)\n",
    "        \n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # print(\"Debug\")\n",
    "        # output = model(data,2)\n",
    "        # \n",
    "        # sys.exit(0)\n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data,False)\n",
    "        test_loss += criterion(output, target) # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    a=100.*correct / len(test_loader.dataset)\n",
    "    accur.append(a)  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / float(len(test_loader.dataset))))\n",
    "    \n",
    "    return float(correct) / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Training\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "print(\"=\"*10,\"Model Info\",\"=\"*10)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "# optimizer = torch.optim.Adam([\n",
    "#                 {'params': model.fc1.parameters()},\n",
    "#                 {'params': model.fc2.parameters()},\n",
    "#                 {'params': model.fc3.parameters()},\n",
    "#                 {'params': model.qc1.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc2.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc3.parameters(), 'lr': 1},\n",
    "#             ], lr=0.1)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# optimizer = torch.optim.SGD([\n",
    "#                 {'params': model.fc1.parameters()},\n",
    "#                 {'params': model.fc2.parameters()},\n",
    "#                 {'params': model.fc3.parameters()},\n",
    "#                 {'params': model.qc1.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc2.parameters(), 'lr': 1},\n",
    "#                 {'params': model.qc3.parameters(), 'lr': 1},\n",
    "#             ], lr=0.1, momentum=0.9)\n",
    "# \n",
    "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \\\n",
    "#         base_lr=[1e-1,1e-1,1e-1,1,1,1], \\\n",
    "#         max_lr=[1e-3,1e-3,1e-3,1e-2,1e-2,1e-2], \\\n",
    "#         step_size_up=100\n",
    "#         )\n",
    "\n",
    "milestones = [3, 7, 9]\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)\n",
    "\n",
    "# \n",
    "# \n",
    "# test()\n",
    "# \n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "==================== 0 epoch ====================\n",
      "Epoch Start at: 04/22/2020 02:43:00\n",
      "-------------------- learning rates --------------------\n",
      "0.01,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 02:43:00\n",
      "Train Epoch: 0 [0/12049 (0%)]\tLoss: 0.680141\tAccuracy: 19/32 (59.38%)\n",
      "Train Epoch: 0 [3200/12049 (27%)]\tLoss: 0.648333\tAccuracy: 1669/3232 (51.64%)\n",
      "Train Epoch: 0 [6400/12049 (53%)]\tLoss: 0.649790\tAccuracy: 3795/6432 (59.00%)\n",
      "Train Epoch: 0 [9600/12049 (80%)]\tLoss: 0.656750\tAccuracy: 6506/9632 (67.55%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.659386\n",
      "Trainign End at: 04/22/2020 02:43:11\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 02:43:11\n",
      "Test set: Average loss: 0.6507, Accuracy: 1816/1968 (92.28%)\n",
      "Testing End at: 04/22/2020 02:43:18\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9227642276422764; Current accuracy 0.9227642276422764. Checkpointing\n",
      "Epoch End at: 04/22/2020 02:43:18\n",
      "============================================================\n",
      "\n",
      "==================== 1 epoch ====================\n",
      "Epoch Start at: 04/22/2020 02:43:18\n",
      "-------------------- learning rates --------------------\n",
      "0.01,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 02:43:18\n",
      "Train Epoch: 1 [0/12049 (0%)]\tLoss: 0.658487\tAccuracy: 29/32 (90.62%)\n",
      "Train Epoch: 1 [3200/12049 (27%)]\tLoss: 0.659541\tAccuracy: 2521/3232 (78.00%)\n",
      "Train Epoch: 1 [6400/12049 (53%)]\tLoss: 0.656005\tAccuracy: 4964/6432 (77.18%)\n",
      "Train Epoch: 1 [9600/12049 (80%)]\tLoss: 0.658622\tAccuracy: 7475/9632 (77.61%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.653496\n",
      "Trainign End at: 04/22/2020 02:43:28\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 02:43:28\n",
      "Test set: Average loss: 0.6507, Accuracy: 1816/1968 (92.28%)\n",
      "Testing End at: 04/22/2020 02:43:36\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9227642276422764; Current accuracy 0.9227642276422764. Checkpointing\n",
      "Epoch End at: 04/22/2020 02:43:36\n",
      "============================================================\n",
      "\n",
      "==================== 2 epoch ====================\n",
      "Epoch Start at: 04/22/2020 02:43:36\n",
      "-------------------- learning rates --------------------\n",
      "0.01,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 02:43:36\n",
      "Train Epoch: 2 [0/12049 (0%)]\tLoss: 0.656663\tAccuracy: 29/32 (90.62%)\n",
      "Train Epoch: 2 [3200/12049 (27%)]\tLoss: 0.656437\tAccuracy: 2669/3232 (82.58%)\n",
      "Train Epoch: 2 [6400/12049 (53%)]\tLoss: 0.658311\tAccuracy: 5194/6432 (80.75%)\n",
      "Train Epoch: 2 [9600/12049 (80%)]\tLoss: 0.666153\tAccuracy: 7886/9632 (81.87%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.653902\n",
      "Trainign End at: 04/22/2020 02:43:48\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 02:43:48\n",
      "Test set: Average loss: 0.6506, Accuracy: 1816/1968 (92.28%)\n",
      "Testing End at: 04/22/2020 02:43:55\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9227642276422764; Current accuracy 0.9227642276422764. Checkpointing\n",
      "Epoch End at: 04/22/2020 02:43:55\n",
      "============================================================\n",
      "\n",
      "==================== 3 epoch ====================\n",
      "Epoch Start at: 04/22/2020 02:43:55\n",
      "-------------------- learning rates --------------------\n",
      "0.001,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 02:43:55\n",
      "Train Epoch: 3 [0/12049 (0%)]\tLoss: 0.642648\tAccuracy: 31/32 (96.88%)\n",
      "Train Epoch: 3 [3200/12049 (27%)]\tLoss: 0.644852\tAccuracy: 2954/3232 (91.40%)\n",
      "Train Epoch: 3 [6400/12049 (53%)]\tLoss: 0.647590\tAccuracy: 5673/6432 (88.20%)\n",
      "Train Epoch: 3 [9600/12049 (80%)]\tLoss: 0.649441\tAccuracy: 8236/9632 (85.51%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.653391\n",
      "Trainign End at: 04/22/2020 02:44:06\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 02:44:06\n",
      "Test set: Average loss: 0.6522, Accuracy: 1400/1968 (71.14%)\n",
      "Testing End at: 04/22/2020 02:44:13\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9227642276422764; Current accuracy 0.7113821138211383. Checkpointing\n",
      "Epoch End at: 04/22/2020 02:44:14\n",
      "============================================================\n",
      "\n",
      "==================== 4 epoch ====================\n",
      "Epoch Start at: 04/22/2020 02:44:14\n",
      "-------------------- learning rates --------------------\n",
      "0.001,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 02:44:14\n",
      "Train Epoch: 4 [0/12049 (0%)]\tLoss: 0.652386\tAccuracy: 23/32 (71.88%)\n",
      "Train Epoch: 4 [3200/12049 (27%)]\tLoss: 0.639463\tAccuracy: 2538/3232 (78.53%)\n",
      "Train Epoch: 4 [6400/12049 (53%)]\tLoss: 0.650265\tAccuracy: 5147/6432 (80.02%)\n",
      "Train Epoch: 4 [9600/12049 (80%)]\tLoss: 0.658502\tAccuracy: 7837/9632 (81.36%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.653606\n",
      "Trainign End at: 04/22/2020 02:44:24\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 02:44:24\n",
      "Test set: Average loss: 0.6506, Accuracy: 1816/1968 (92.28%)\n",
      "Testing End at: 04/22/2020 02:44:31\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9227642276422764; Current accuracy 0.9227642276422764. Checkpointing\n",
      "Epoch End at: 04/22/2020 02:44:31\n",
      "============================================================\n",
      "\n",
      "==================== 5 epoch ====================\n",
      "Epoch Start at: 04/22/2020 02:44:31\n",
      "-------------------- learning rates --------------------\n",
      "0.001,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 02:44:31\n",
      "Train Epoch: 5 [0/12049 (0%)]\tLoss: 0.656552\tAccuracy: 30/32 (93.75%)\n",
      "Train Epoch: 5 [3200/12049 (27%)]\tLoss: 0.655248\tAccuracy: 2585/3232 (79.98%)\n",
      "Train Epoch: 5 [6400/12049 (53%)]\tLoss: 0.647213\tAccuracy: 5193/6432 (80.74%)\n",
      "Train Epoch: 5 [9600/12049 (80%)]\tLoss: 0.647772\tAccuracy: 7702/9632 (79.96%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.653806\n",
      "Trainign End at: 04/22/2020 02:44:41\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 02:44:41\n",
      "Test set: Average loss: 0.6506, Accuracy: 1816/1968 (92.28%)\n",
      "Testing End at: 04/22/2020 02:44:48\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9227642276422764; Current accuracy 0.9227642276422764. Checkpointing\n",
      "Epoch End at: 04/22/2020 02:44:48\n",
      "============================================================\n",
      "\n",
      "==================== 6 epoch ====================\n",
      "Epoch Start at: 04/22/2020 02:44:48\n",
      "-------------------- learning rates --------------------\n",
      "0.001,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 02:44:48\n",
      "Train Epoch: 6 [0/12049 (0%)]\tLoss: 0.667624\tAccuracy: 24/32 (75.00%)\n",
      "Train Epoch: 6 [3200/12049 (27%)]\tLoss: 0.658581\tAccuracy: 2504/3232 (77.48%)\n",
      "Train Epoch: 6 [6400/12049 (53%)]\tLoss: 0.647868\tAccuracy: 5211/6432 (81.02%)\n",
      "Train Epoch: 6 [9600/12049 (80%)]\tLoss: 0.651519\tAccuracy: 7923/9632 (82.26%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.653667\n",
      "Trainign End at: 04/22/2020 02:44:58\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 02:44:58\n",
      "Test set: Average loss: 0.6522, Accuracy: 1401/1968 (71.19%)\n",
      "Testing End at: 04/22/2020 02:45:05\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9227642276422764; Current accuracy 0.711890243902439. Checkpointing\n",
      "Epoch End at: 04/22/2020 02:45:05\n",
      "============================================================\n",
      "\n",
      "==================== 7 epoch ====================\n",
      "Epoch Start at: 04/22/2020 02:45:05\n",
      "-------------------- learning rates --------------------\n",
      "0.00010000000000000002,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 02:45:05\n",
      "Train Epoch: 7 [0/12049 (0%)]\tLoss: 0.653075\tAccuracy: 23/32 (71.88%)\n",
      "Train Epoch: 7 [3200/12049 (27%)]\tLoss: 0.654377\tAccuracy: 2359/3232 (72.99%)\n",
      "Train Epoch: 7 [6400/12049 (53%)]\tLoss: 0.645784\tAccuracy: 4692/6432 (72.95%)\n",
      "Train Epoch: 7 [9600/12049 (80%)]\tLoss: 0.659723\tAccuracy: 7055/9632 (73.25%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.653482\n",
      "Trainign End at: 04/22/2020 02:45:15\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 02:45:15\n",
      "Test set: Average loss: 0.6522, Accuracy: 1401/1968 (71.19%)\n",
      "Testing End at: 04/22/2020 02:45:22\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9227642276422764; Current accuracy 0.711890243902439. Checkpointing\n",
      "Epoch End at: 04/22/2020 02:45:22\n",
      "============================================================\n",
      "\n",
      "==================== 8 epoch ====================\n",
      "Epoch Start at: 04/22/2020 02:45:22\n",
      "-------------------- learning rates --------------------\n",
      "0.00010000000000000002,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 02:45:22\n",
      "Train Epoch: 8 [0/12049 (0%)]\tLoss: 0.664764\tAccuracy: 22/32 (68.75%)\n",
      "Train Epoch: 8 [3200/12049 (27%)]\tLoss: 0.645576\tAccuracy: 2451/3232 (75.84%)\n",
      "Train Epoch: 8 [6400/12049 (53%)]\tLoss: 0.658935\tAccuracy: 5312/6432 (82.59%)\n",
      "Train Epoch: 8 [9600/12049 (80%)]\tLoss: 0.661668\tAccuracy: 7687/9632 (79.81%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.653538\n",
      "Trainign End at: 04/22/2020 02:45:33\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 02:45:33\n",
      "Test set: Average loss: 0.6506, Accuracy: 1816/1968 (92.28%)\n",
      "Testing End at: 04/22/2020 02:45:40\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9227642276422764; Current accuracy 0.9227642276422764. Checkpointing\n",
      "Epoch End at: 04/22/2020 02:45:40\n",
      "============================================================\n",
      "\n",
      "==================== 9 epoch ====================\n",
      "Epoch Start at: 04/22/2020 02:45:40\n",
      "-------------------- learning rates --------------------\n",
      "1.0000000000000003e-05,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 02:45:40\n",
      "Train Epoch: 9 [0/12049 (0%)]\tLoss: 0.662121\tAccuracy: 29/32 (90.62%)\n",
      "Train Epoch: 9 [3200/12049 (27%)]\tLoss: 0.661872\tAccuracy: 2595/3232 (80.29%)\n",
      "Train Epoch: 9 [6400/12049 (53%)]\tLoss: 0.654373\tAccuracy: 5398/6432 (83.92%)\n",
      "Train Epoch: 9 [9600/12049 (80%)]\tLoss: 0.662046\tAccuracy: 8030/9632 (83.37%)\n",
      "-------------------- training done, loss --------------------\n",
      "Training Set: Average loss: 0.653451\n",
      "Trainign End at: 04/22/2020 02:45:51\n",
      "------------------------------------------------------------\n",
      "\n",
      "-------------------- testing --------------------\n",
      "Testing Start at: 04/22/2020 02:45:51\n",
      "Test set: Average loss: 0.6522, Accuracy: 1401/1968 (71.19%)\n",
      "Testing End at: 04/22/2020 02:45:58\n",
      "------------------------------------------------------------\n",
      "\n",
      "Best accuracy: 0.9227642276422764; Current accuracy 0.711890243902439. Checkpointing\n",
      "Epoch End at: 04/22/2020 02:45:58\n",
      "============================================================\n",
      "\n",
      "==================== 10 epoch ====================\n",
      "Epoch Start at: 04/22/2020 02:45:58\n",
      "-------------------- learning rates --------------------\n",
      "1.0000000000000003e-05,\n",
      "-------------------- training --------------------\n",
      "Trainign Start at: 04/22/2020 02:45:58\n",
      "Train Epoch: 10 [0/12049 (0%)]\tLoss: 0.664717\tAccuracy: 22/32 (68.75%)\n",
      "Train Epoch: 10 [3200/12049 (27%)]\tLoss: 0.658130\tAccuracy: 2650/3232 (81.99%)\n",
      "Train Epoch: 10 [6400/12049 (53%)]\tLoss: 0.654278\tAccuracy: 5191/6432 (80.71%)\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-388643ceff95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainign Start at:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%m/%d/%Y %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainign End at:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%m/%d/%Y %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-c6c273a98d04>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "if os.path.isfile(resume_path):\n",
    "    print(\"=> loading checkpoint from '{}'<=\".format(resume_path))\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    epoch_init,acc = checkpoint[\"epoch\"],checkpoint[\"acc\"]\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])    \n",
    "    scheduler.milestones = Counter(milestones)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "else:\n",
    "    epoch_init,acc = 0,0\n",
    "\n",
    "\n",
    "\n",
    "if training:\n",
    "    for epoch in range(epoch_init, max_epoch + 1):\n",
    "        print(\"=\"*20,epoch,\"epoch\",\"=\"*20)  \n",
    "        print(\"Epoch Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))        \n",
    "\n",
    "        print(\"-\"*20,\"learning rates\",\"-\"*20)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(param_group['lr'],end=\",\")\n",
    "        print()    \n",
    "        \n",
    "        print(\"-\"*20,\"training\",\"-\"*20)\n",
    "        print(\"Trainign Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        train(epoch)\n",
    "        print(\"Trainign End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"-\"*20,\"testing\",\"-\"*20)\n",
    "        print(\"Testing Start at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))        \n",
    "        cur_acc = test()\n",
    "        print(\"Testing End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"-\"*60)\n",
    "        print()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        is_best = False\n",
    "        if cur_acc > acc:\n",
    "            is_best = True\n",
    "            acc=cur_acc\n",
    "        \n",
    "        print(\"Best accuracy: {}; Current accuracy {}. Checkpointing\".format(acc,cur_acc))\n",
    "        if save_chkp:\n",
    "            save_checkpoint({\n",
    "              'epoch': epoch + 1,\n",
    "              'acc': acc, \n",
    "              'state_dict': model.state_dict(),      \n",
    "              'optimizer' : optimizer.state_dict(),\n",
    "               'scheduler': scheduler.state_dict(),\n",
    "            }, is_best, save_path, 'checkpoint_{}_{}.pth.tar'.format(epoch,round(cur_acc,4)))\n",
    "        print(\"Epoch End at:\",time.strftime(\"%m/%d/%Y %H:%M:%S\"))\n",
    "        print(\"=\"*60)\n",
    "        print()        \n",
    "else:    \n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, \n",
    "        num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "    test()\n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "accur=[]\n",
    "def test_debug():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        target,new_target = modify_target(target)\n",
    "        \n",
    "        # \n",
    "        # data = (data-data.min())/(data.max()-data.min())\n",
    "        # data = (binarize(data-0.5)+1)/2\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        print(\"Debug\",data,target)\n",
    "        output = model(data,2)\n",
    "            \n",
    "        \n",
    "        sys.exit(0)\n",
    "        # data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data,False)\n",
    "        test_loss += criterion(output, target) # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    a=100.*correct / len(test_loader.dataset)\n",
    "    accur.append(a)  \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * float(correct) / float(len(test_loader.dataset))))\n",
    "    \n",
    "    return float(correct) / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "# \n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, \n",
    "#         num_workers=num_workers, shuffle=True, drop_last=True)\n",
    "# test_debug()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for name, para in model.named_parameters():\n",
    "#     print(name,para)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-8213722",
   "language": "python",
   "display_name": "PyCharm (qiskit_practice)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}